
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.7">
    
    
      
        <title>Deep learning (Neural Network) - LSHTM Omics</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.8608ea7d.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../stylesheets/extra.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#deep-learning-neural-network" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="LSHTM Omics" class="md-header__button md-logo" aria-label="LSHTM Omics" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            LSHTM Omics
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Deep learning (Neural Network)
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href=".." class="md-tabs__link">
          
  
  Home

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../introduction/intro-to-linux/" class="md-tabs__link">
          
  
  Introduction

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../advanced/phylogenetics/" class="md-tabs__link">
          
  
  Advanced

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../other-omics/microbiome/" class="md-tabs__link">
          
  
  Other omics

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../additional-info/conda/" class="md-tabs__link">
          
  
  Additional info

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../presentations/presentations/" class="md-tabs__link">
          
  
  Presentations

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="LSHTM Omics" class="md-nav__button md-logo" aria-label="LSHTM Omics" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    LSHTM Omics
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1" >
        
          
          <label class="md-nav__link" for="__nav_1" id="__nav_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1">
            <span class="md-nav__icon md-icon"></span>
            Home
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Getting Started
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../connecting-github/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Getting connected
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Introduction
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../introduction/intro-to-linux/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Linux
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../introduction/mapping/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Mapping
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../introduction/variant-detection/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Variant detection
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../introduction/assembly/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Assembly
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../introduction/task/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Task
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Advanced
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Advanced
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../advanced/phylogenetics/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Phylogenetics
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../advanced/third-generation-sequencing/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Third Generation Sequencing
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../advanced/gwas/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Genome Wide Association Studies (GWAS)
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Other omics
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Other omics
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../other-omics/microbiome/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Microbiomes Practical
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../other-omics/transcriptomics/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Transcriptomics
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../other-omics/methylation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Methylation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../other-omics/ml/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Deep learning (Neural Network)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../other-omics/tb-resistance/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TB resistance prediction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../other-omics/eqtl/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    EQTL
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Additional info
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Additional info
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../additional-info/conda/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Conda Environments: Why and How to Use Them
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../additional-info/software/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Software
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../additional-info/SettingUpVirtualLinux/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Virtual Box: Setting Up Linux
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../additional-info/faq/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Frequently asked questions
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Presentations
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Presentations
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../presentations/presentations/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Presentations
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="deep-learning-neural-network">Deep learning (Neural Network)</h1>
<p>Next-generation sequencing data is being produced at an ever-increasing rate. The raw data is not meaningful by itself and needs to be processed using various bioinformatic software. This practical will focus on genomic resequencing data where the raw data is aligned to a reference genome.</p>
<h2 id="introduction">Introduction</h2>
<p>Deep learning neural networks have revolutionized the field of predictive modelling, especially in bioinformatics and genomics. By utilizing multiple layers of artificial neurons to extract and process data, deep learning networks have shown impressive performance in tasks such as image classification, natural language processing, and speech recognition. The same principles can be applied to infectious disease genomic DNA data for drug resistance prediction. By leveraging the MINIST complexity and high-dimensional nature of genomic data, deep learning networks can learn to identify subtle patterns and relationships that may be missed by traditional statistical methods. Additionally, deep learning networks can be optimized to handle missing data, noisy data, and varying data types, which are common challenges in genomic data analysis.</p>
<p><img alt="mapping_1" src="/img/ml_1.png" /></p>
<div class="admonition knowledge">
<p class="admonition-title">Knowledge</p>
<p>A neural network is a type of machine learning model that consists of layers of interconnected nodes, also known as neurons. Each neuron takes in input values, multiplies them by weights, and applies an activation function to produce an output. The output from one layer becomes the input to the next layer until the final layer produces the model's prediction. During training, the model adjusts the weights to minimize the difference between its prediction and the actual output. This process is repeated multiple times until the model's predictions become accurate enough for the desired task.</p>
</div>
<!-- 

To predict drug resistance from infectious disease genomic DNA data, deep learning networks are trained on large and diverse datasets containing information on various aspects of the DNA, such as nucleotide sequences, structural variants, and gene expression levels. By analyzing these features, deep learning networks can learn to predict drug resistance with high accuracy, potentially leading to the development of more effective drugs and treatment strategies. The process of training these networks involves splitting the data into training, validation, and testing sets, and optimizing the network architecture and parameters to minimize the prediction error. -->

<p>In summary, deep learning neural networks offer a promising approach for predicting drug resistance from infectious disease genomic DNA data. By leveraging the complex and high-dimensional nature of genomic data, these networks can identify subtle patterns and relationships that are difficult to detect using traditional statistical methods. With the growing threat of drug-resistant infectious diseases.</p>
<p>However, In this practical, we will focus on training a model on the classic Mnist.</p>
<p>The MNIST dataset, short for Modified National Institute of Standards and Technology database, is a widely recognized and fundamental dataset in the field of computer vision and machine learning. It consists of a collection of 28x28 pixel grayscale images of handwritten digits, ranging from 0 to 9. MNIST serves as a benchmark for various classification tasks.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Before doing anything, we need to first activate the conda environment for this practical by typing the following: <code>conda activate deep_learning</code>. This environment contains most of the software we need for this practical. This command needs to be run each time we open up a new terminal or switch from a different environment. </p>
</div>
<h2 id="exercise-1-running-the-neural-network-for-predicting-isoniazid-resistance">Exercise 1: Running the neural network for predicting Isoniazid resistance</h2>
<p>Here we train a model using to recognise hand written digits using it's colorgradient</p>
<p>You can list all installed environments with <code>cond env list</code>.</p>
<p>In the terminal navigate to the ‘ml_workshop/MINIST_model’ directory, by typing:</p>
<div class="highlight"><pre><span></span><code>cd ~/ml_workshop/mnist_model
</code></pre></div>
<p>Now type the commands below to train the model with default parameters.</p>
<div class="highlight"><pre><span></span><code>python MINIST_model.py
</code></pre></div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>In the command, it is important to specify </strong> python </strong> before the script to complie (run the script) using python.</p>
</div>
<p>The model will run for 10 iterations, which will take about 2 minutes to complete, feel free to read forward, go through the addition info at the bottom of the page and play with the model on the webpage <a href=https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.58442&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false>Neural network Playground</a></p>
<p>Two graphs will be are produced:</p>
<div class="highlight"><pre><span></span><code>INH-model_LR:0.001-DR:0.2-ACC.png
INH-model_LR:0.001-DR:0.2-LOSS.png
</code></pre></div>
<p>You can open the current folder using command <code>open .</code>.
Double click on the picture files to view them.</p>
<p><code>MNISTmodel_LR:0.001-DR:0.2-LOSS.png</code></p>
<h3 id="acc-accuracy">ACC-accuracy</h3>
<p>Accuracy is a common performance metric used in deep learning to <strong>evaluate the effectiveness of a model</strong> at predicting the correct output. It measures the proportion of correct predictions made by the model out of the total number of predictions. </p>
<div class="admonition intuition">
<p class="admonition-title">Intuition</p>
<p>Imagine you are playing a game of darts and aiming for a bullseye. Your accuracy is determined by the number of times you hit the bullseye compared to the total number of attempts. </p>
</div>
<p>Similarly, in deep learning, accuracy measures <strong>how often the model's predictions match the true labels</strong> for a given set of inputs. </p>
<p>For example, if a model predicts that an image contains a cat and the true label is also cat, then that prediction is counted as correct. </p>
<p><strong>The higher the accuracy, the better the model</strong> is at making correct predictions. However, accuracy can be influenced by factors such as class imbalance or the types of errors the model makes. Therefore, it is important to consider other performance metrics in addition to accuracy when evaluating the performance of a deep learning model.</p>
<h3 id="loss-loss">LOSS-loss</h3>
<p>In deep learning, the goal of the model is to accurately predict the outcome of a given task, such as image recognition or natural language processing. </p>
<p><strong>Loss</strong> is a term used to quantify the difference between the predicted output and the actual output. </p>
<div class="admonition intuition">
<p class="admonition-title">Intuition</p>
<p>Think of it like a student taking a test - the score they receive is a measure of how well they performed relative to the expected outcome. </p>
</div>
<p>Similarly, the loss function in deep learning measures how well the model is performing by comparing its predictions to the actual results. The lower the loss, the better the model is at predicting the outcome. </p>
<p>The goal of training a deep learning model is to <strong>minimize the loss</strong> over the course of multiple iterations, or epochs, of training. <strong>Different types of loss functions</strong> can be used depending on the task and the type of output being predicted. Ultimately, the goal is to choose a loss function that encourages the model to learn the desired features and make accurate predictions.</p>
<h3 id="epoch">Epoch</h3>
<p>In deep learning, an epoch refers to a complete pass through the entire training dataset during the model training process. </p>
<div class="admonition intuition">
<p class="admonition-title">Intuition</p>
<p>Think of it like a chef preparing a recipe - each time they go through the entire recipe from start to finish, that's one epoch. </p>
</div>
<p>During each epoch, the model is shown a batch of input data and the corresponding output labels, and it updates its parameters based on the difference between the predicted output and the actual output. </p>
<p>The number of epochs that a model is trained for is an important hyperparameter that can impact the performance of the model. <strong>Training for too few epochs</strong> may result in a model that <strong>underfits</strong> or fails to learn the underlying patterns in the data. On the other hand, training for <strong>too many epochs</strong> may result in a model that <strong>overfits</strong> or becomes too specialized to the training data and fails to generalize well to new data. Therefore, the number of epochs should be chosen carefully to balance between underfitting and overfitting and achieve the best performance for the task at hand.</p>
<h2 id="exercise-2-try-different-values-of-dropout-and-learning-rate">Exercise 2 :Try different values of dropout and learning rate</h2>
<p>Type the commands below to train the model with different parameters.</p>
<p>Default set up
<div class="highlight"><pre><span></span><code>python MINIST_model.py -lr 0.001 -dr 0.2 
</code></pre></div>
<strong>High</strong> learning rate, <strong>high</strong> dropout rate
<div class="highlight"><pre><span></span><code>python MINIST_model.py -lr 0.01 -dr 0.6 
</code></pre></div>
<strong>High</strong> learning rate, <strong>low</strong> dropout rate
<div class="highlight"><pre><span></span><code>python MINIST_model.py -lr 0.01 -dr 0.01 
</code></pre></div>
<strong>Low</strong> learning rate, <strong>high</strong> dropout rate
<div class="highlight"><pre><span></span><code>python MINIST_model.py -lr 0.0002 -dr 0.6
</code></pre></div>
<strong>Low</strong> learning rate, <strong>low</strong> dropout rate
<div class="highlight"><pre><span></span><code>python MINIST_model.py -lr 0.0002 -dr 0.01
</code></pre></div></p>
<p>Also feel feel free to try different values within the below ranges
-    learning rate: between 10e-6 and 1
-    Dropout rate:  between 0 and 1</p>
<p>When working with python script that required input, you can also View the explanation for each input parameter using <code>python full_model.py -h</code>.</p>
<div class="highlight"><pre><span></span><code>usage: MINIST_model.py [-h] [-lr LEARNING_RATE] [-dr DROPOUT_RATE]

Isoniazid prediction model using KatG sequences as input.

optional arguments:
  -h, --help            show this help message and exit
  -lr LEARNING_RATE, --learning_rate LEARNING_RATE
                        Learning rate for the model(between 10e-6 and 1) (default: 0.001)
  -dr DROPOUT_RATE, --dropout_rate DROPOUT_RATE
                        Dropout rate for hte model layers (between 0 and 1) (default: 0.2)
</code></pre></div>
<h3 id="lr-learning-rate">lr: learning rate</h3>
<p>Learning rate is a <strong>key parameter</strong> in many machine learning algorithms. </p>
<p>It is the step size that a model takes when trying to find the best set of weights to make accurate predictions. </p>
<div class="admonition intuition">
<p class="admonition-title">Intuition</p>
<p>Imagine you are climbing a hill to reach the top. Your learning rate is how big of a step you take with each stride. If your steps are too small, it will take a long time to reach the top, but if your steps are too big, you might overshoot and miss the peak. </p>
</div>
<p>Similarly, in machine learning, a <strong>small learning rate</strong> will cause the algorithm to take longer to converge, while a <strong>large learning rate</strong> can cause the algorithm to overshoot and make inaccurate predictions. Therefore, choosing the right learning rate is important to achieve optimal results in machine learning models.</p>
<h3 id="dr-dropout-rate">dr: dropout rate</h3>
<p>Dropout is a technique used in machine learning to <strong>prevent overfitting</strong>, which is when a model becomes too specialized to the training data and fails to generalize well to new data. </p>
<p>Dropout works by <strong>randomly dropping out</strong>, or "turning off", some of the neurons in a neural network during training. </p>
<div class="admonition intuition">
<p class="admonition-title">Intuition</p>
<p>Again the chef intuition - if they only rely on a few key ingredients, the dish may taste great in the kitchen but won't necessarily appeal to a wider audience. </p>
</div>
<p>By randomly "turning off" some of the neurons, the <strong>model is forced to learn more robust and diverse features</strong>, leading to better generalization to new data. The dropout rate is the percentage of neurons that are randomly dropped out during each training epoch, and it is another important hyperparameter that can be tuned to optimize the performance of the model.</p>
<p><img alt="mapping_1" src="/img/ml_2.png" /></p>
<p>These are some rough guidance, take it with a grain of salt as these parameters need to be talored to the datasets.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Learning Rate</th>
<th style="text-align: center;">Dropout Rate</th>
<th style="text-align: center;">Consequences</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">High</td>
<td style="text-align: center;">High</td>
<td style="text-align: center;">Fast learning, high regularization, risk of underfitting</td>
</tr>
<tr>
<td style="text-align: center;">High</td>
<td style="text-align: center;">Low</td>
<td style="text-align: center;">Fast learning, low regularization, risk of overfitting</td>
</tr>
<tr>
<td style="text-align: center;">Low</td>
<td style="text-align: center;">High</td>
<td style="text-align: center;">Slow learning, high regularization, good generalization</td>
</tr>
<tr>
<td style="text-align: center;">Low</td>
<td style="text-align: center;">Low</td>
<td style="text-align: center;">Slow learning, low regularization, risk of overfitting</td>
</tr>
</tbody>
</table>
<p><img alt="mapping_1" src="/img/ml_3.png" /></p>
<p>A <strong>high learning rate</strong> means the model will make larger updates to its weights during training, while a <strong>low learning rate</strong> means the updates will be smaller. </p>
<p>A <strong>high dropout rate</strong> means more neurons will be dropped out during training, while a<strong> low dropout rate</strong> means fewer neurons will be dropped out.</p>
<p>If <strong>both the learning rate and dropout rate are high</strong>, the model will learn quickly but may not generalize well to new data due to excessive regularization. If <strong>both the learning rate and dropout rate are low</strong>, the model will learn slowly and may overfit to the training data. </p>
<p>If the learning rate is low and the dropout rate is high, the model will learn slowly but regularize effectively, leading to good generalization. Finally, if the learning rate is high and the dropout rate is low, the model will learn quickly but may overfit due to insufficient regularization.</p>
<h2 id="exercise-3-converting-fastq-to-onehot-encoding">Exercise 3: Converting Fastq to onehot encoding</h2>
<p>In machine learning, one-hot encoding is a way to represent categorical data in a numerical format that can be easily understood by algorithms.</p>
<p>In the case of DNA data, each nucleotide (<strong>A, T, C, G</strong>) can be considered as a category, and one-hot encoding is used to represent each nucleotide as a unique binary value. </p>
<p>This is necessary because <strong>machine learning algorithms require numerical inputs to make predictions</strong>, and simply representing DNA sequences as strings of characters would not be suitable for most machine learning tasks. </p>
<p>By using <strong>one-hot encoding</strong>, we can represent each DNA sequence as a series of binary values that capture the presence or absence of each nucleotide at each position in the sequence. This allows us to use a wide range of machine learning algorithms to analyse and make predictions based on DNA data, such as predicting the likelihood of a genetic disorder or identifying regions of the genome that are associated with certain traits or diseases.</p>
<p>First change the directory to the script folder using hte below command:</p>
<div class="highlight"><pre><span></span><code>cd ../fastq2oh
</code></pre></div>
<p>Now type the commands below to view the first few lines of the original fastq file:</p>
<div class="highlight"><pre><span></span><code>less ERR6634978_1.fastq | head
</code></pre></div>
<div class="admonition reminder">
<p class="admonition-title">Reminder</p>
<p>FASTQ is a text-based file format commonly used in bioinformatics to store and exchange sequences and their corresponding quality scores. It consists of four lines per sequence: the first line starts with "@" followed by a unique identifier for the sequence, the second line contains the actual nucleotide sequence, the third line starts with "+" followed by the same unique identifier as in the first line, and the fourth line contains the quality scores corresponding to each nucleotide in the sequence. The quality scores represent the confidence level of each nucleotide call and are represented as ASCII characters. The FASTQ format is widely used in sequencing technologies such as Illumina, Ion Torrent, and PacBio.</p>
</div>
<div class="admonition output">
<p class="admonition-title">Output</p>
<div class="highlight"><pre><span></span><code>@A00386:50:HGY3TDRXY:1:2101:11496:1016 1:N:0:ATTACTCG+TAAGATTA
GNCGTTGGCGATGCGCACGGTGTTGGAGAGCGTGCCACCCGTGACGGTGCCGTCCGAGATCGTCCGGCTGCAAGAGCAGCTGGCCCAGGTGGCAAAGGGTGAGGCTTTCCTGCTGCAGGGCGGCGACTGCGCTGAGACATTCATGGACAAC
+
F#FFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFF,F
@A00386:50:HGY3TDRXY:1:2101:12382:1016 1:N:0:ATTACTCG+TAAGATTA
GNCGGACGTGTCGAACTTGGGGCCTACGACGCCGAACATGACCTGATCCTGGAGAACGACCGCGGCTTCGTGCAGGTCGCCGGTGTCAACCAGGTCGGGGTGCTGCTCGC
+
F#FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF
@A00386:50:HGY3TDRXY:1:2101:21386:1016 1:N:0:ATTACTCG+TAAGATTA
ATTCCACCGCCTCGGCGACCACGACCAGCACGATCAATGTCCGGGCACTATCCCCGGCGCTGGTGGTGACATAGATCGGGTAATACCCCGACGGCACCGATCGAGCTACGGTGATCGCGACC
</code></pre></div>
</div>
<div class="admonition reminder">
<p class="admonition-title">Reminder</p>
<p><code>less</code> show a scrollable content of the file, <code>head</code> allow the terminal to only show the first few lines from less output.</p>
</div>
<p>As a little revision from the alignment lecture, let's go through mapping to get the consensus sequence in a one hot encoded format, starting from a fastq file: <code>ERR6634978.fq.gz</code></p>
<pre><code>Get the environment for file conversion
```
conda activate fastq2oh
```

Run Alignment and index the bam file
```
bwa mem MTB-h37rv_asm19595v2-eg18.fa ERR6634978.fq.gz | samtools sort -o ERR6634978.bam -
samtools index ERR6634978.bam
```

Run the conversion script to generate onehot encoded nucleotide sequence from ERR6634978.bam file
```
python fastq2oh.py -r gene.csv -o `ERR6634978`_oh -b ERR6634978.bam
```

-    r-gene regions
-    o-output file name
-    b-input bam file

Now type the commands below to view generate onehot encoded sequences:
```
</code></pre>
<p>less  ERR6634978_oh | head
```</p>
<div class="admonition output">
<p class="admonition-title">Output</p>
<p><code>A,C,G,T
1,0,0,0
0,1,0,0
1,0,0,0
0,1,0,0
0,0,1,0
0,0,0,1
0,1,0,0
0,0,1,0
1,0,0,0</code>
</p>
</div>
<h2 id="exercise-4-running-full-model-that-predicts-for-all-13-drug-resistances">Exercise 4: Running full model that predicts for all 13 drug resistances</h2>
<p>Now try to use a fully trained model to predict all 13 different types of drug resistance.

First change the directory to the script folder using hte below command:

<code>cd ../full_model</code>

Now type the below commands to train the model with different parameters, show output in terminal and saving output into <code>drug_predictions.txt</code> file:

<code>python full_model.py -i ../fastq2oh/ERR6634978_oh -v -o drug_predictions.txt</code></p>
<ul>
<li>i-one hot encoded input file</li>
<li>o-output file name</li>
<li>v-verbose (show output in the terminal)</li>
</ul>
<p>You can also view the output file using <code>less drug_predictions.txt</code>. Feel free to scroll around and press <code>Q</code> to exit.</p>
<h2 id="additional-information">Additional information</h2>
<p>In case you'd like a more hands on and visualised example of how a neural networks functions.
<a href=https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.58442&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false>Neural network Playground</a></p>
<h3 id="what-models-are-we-actually-using">What models are we actually using</h3>
<h4 id="convolutional-neural-network-cnn">Convolutional Neural Network (CNN)</h4>
<p>A Convolutional Neural Network (CNN) is a class of deep learning models designed primarily for processing structured grid data, such as images and videos. It's a specialized type of artificial neural network that excels at recognizing patterns and features in multi-dimensional data. At its core, a CNN employs a series of convolutional layers that perform convolutions, which are mathematical operations involving the element-wise multiplication of small, learnable filters with overlapping regions of the input data. These filters act as feature detectors, capable of identifying distinctive patterns like edges, corners, or textures within the data. By stacking multiple convolutional layers, a CNN can progressively learn complex hierarchical features, allowing it to recognize higher-level patterns, shapes, and objects in the input. Additionally, CNNs often include pooling layers to reduce spatial dimensions and fully connected layers for classification or regression tasks. The ability to automatically extract relevant features from raw data makes CNNs exceptionally powerful for image classification, object detection, facial recognition, and a wide range of computer vision tasks. Their success is attributed to their capacity to capture spatial hierarchies and translational invariance in data, enabling them to excel in various visual recognition applications.
<img alt="mapping_1" src="/img/ml_5.png" /></p>
<h3 id="other-neural-networks">Other neural networks</h3>
<h4 id="recurrent-neural-network-rnn">Recurrent Neural Network (RNN)</h4>
<p>Well, what else is out there, many! graphical networks, Generative adversarial network, Autoencoder, diffusion model... But one other very OG model type is RNN. </p>
<p>A Recurrent Neural Network (RNN) is like a smart detective that looks at a series of clues one by one to solve a mystery. Each clue represents a piece of information in a sequence, like words in a sentence. The RNN doesn't just look at one clue at a time; it remembers what it saw before.</p>
<p>Imagine reading a story one word at a time. As you read, you understand the story because you remember the words that came before. RNNs work in a similar way. They have a memory that keeps track of the words they've seen. So, when they see a new word, they use it along with their memory to make sense of the story.</p>
<p>This memory helps RNNs with tasks like predicting the next word in a sentence or understanding the sentiment of a text. However, sometimes RNNs can forget important things if the story is too long. That's why there are improved versions of RNNs, like LSTM and GRU, which have better memory and can understand longer stories.</p>
<p>In a nutshell, RNNs are like detectives for sequences of data. They remember what they've seen before to make sense of what comes next, which makes them great for tasks involving sequences, like understanding language or predicting future values in time series data.</p>
<h4 id="more-on-regularisation">More on regularisation</h4>
<p>Regularization is a technique used in deep learning to prevent overfitting and improve the generalization performance of a model. There are several ways to regularize a deep learning model, including:</p>
<ol>
<li>L1 and L2 Regularization: These are the most common types of regularization used in deep learning. L1 regularization adds a penalty term to the loss function that is proportional to the absolute values of the model parameters, while L2 regularization adds a penalty term that is proportional to the squared values of the model parameters. Both types of regularization encourage the model to learn simpler, more interpretable features by shrinking the magnitude of the parameters.</li>
<li>Dropout: Dropout is a technique that randomly drops out (i.e., sets to zero) a proportion of the neurons in a layer during training. This helps prevent overfitting by forcing the network to learn more robust features that do not rely on the activation of specific neurons.</li>
<li>Data Augmentation: Data augmentation is a technique that artificially increases the size of the training dataset by creating new examples from the existing ones. This can be done by applying transformations such as rotations, flips, and crops to the original images, or by adding noise to the input data.</li>
<li>Early Stopping: Early stopping is a technique that stops the training process before the model starts to overfit. This is done by monitoring the validation error during training and stopping the training process when the validation error stops improving.</li>
<li>Batch Normalization: Batch normalization is a technique that normalizes the activations of a layer by subtracting the mean and dividing by the standard deviation of the activations in a batch of data. This helps to reduce the internal covariate shift, which can improve the training speed and stability of the model.</li>
<li>Max-Norm Regularization: Max-Norm regularization constrains the magnitude of the weight vector for each neuron to a fixed value. This helps to prevent large weight updates during training, which can lead to overfitting</li>
<li>Label Smoothing: In label smoothing, instead of assigning a one-hot vector to the target labels, a smoothed label distribution is used. This helps prevent overfitting by introducing a small amount of noise into the training targets, which can encourage the model to learn more robust decision boundaries.</li>
<li>Cutout: Cutout is a form of data augmentation that randomly masks out square regions of the input images during training. This helps prevent overfitting by forcing the model to learn more robust features and by increasing the amount of training data.</li>
<li>Mixup: Mixup is a form of data augmentation that involves linearly interpolating pairs of training examples and their corresponding labels. This creates new training examples and encourages the model to learn more generalizable features.</li>
<li>Shake-Shake Regularization: Shake-Shake regularization is a form of regularization for residual networks that introduces stochastic depth into the network. This helps prevent overfitting by randomly dropping out entire residual blocks during training.</li>
<li>Stochastic Depth: Stochastic Depth is a variant of the Shake-Shake regularization that drops out entire residual blocks with a certain probability. This technique helps prevent overfitting by randomly removing some parts of the network during training.</li>
<li>Focal Loss: Focal loss is a variant of cross-entropy loss that gives more weight to hard-to-classify examples. This can help prevent overfitting by reducing the impact of easy-to-classify examples on the training process.</li>
</ol>
<h4 id="all-parameters-that-affects-the-model">All parameters that affects the model</h4>
<p>The learning rate and dropout rate and two of the most important hyperparameters that affect the model's learning.
-   Learning Rate: The learning rate determines the step size of the optimization algorithm during training. A high learning rate can lead to unstable training, while a low learning rate can lead to slow convergence.</p>
<ul>
<li>
<p>Batch Size: The batch size determines the number of samples that are used to compute each update during training. A larger batch size can lead to faster training, but it can also require more memory and may result in lower generalization performance.</p>
</li>
<li>
<p>Dropout Rate: Dropout is a regularization technique that randomly drops out some neurons during training to prevent overfitting. The dropout rate determines the fraction of neurons that are dropped out during training.</p>
</li>
<li>
<p>Weight Initialization: The initial values of the weights in the neural network can affect the model's ability to learn and generalize to new data. Common weight initialization techniques include random initialization and Xavier initialization.</p>
</li>
<li>
<p>Optimizer: The optimizer is the algorithm used to update the weights of the neural network during training. Common optimizers include stochastic gradient descent (SGD), Adam, and RMSprop. The choice of optimizer can affect the speed and stability of training, as well as the generalization performance of the model.</p>
</li>
<li>
<p>Number of Hidden Layers: This parameter determines the depth of the neural network, and it can greatly affect the complexity and capacity of the model. Adding more layers can allow the model to learn more complex features, but it can also increase the risk of overfitting.</p>
</li>
<li>
<p>Number of Neurons per Layer: This parameter determines the width of the neural network, and it can also affect the model's capacity. Increasing the number of neurons per layer can increase the model's ability to capture complex relationships in the data, but it can also increase the risk of overfitting.</p>
</li>
<li>
<p>Activation Function: The activation function is applied to each neuron in the neural network, and it determines the output of the neuron. Common activation functions include sigmoid, ReLU, and tanh. The choice of activation function can affect the model's ability to learn and generalize to new data.</p>
</li>
</ul>
<h4 id="full-model-structure">Full model structure</h4>
<p>In case if you are curious of how the structure of the full model looks like
<img alt="mapping_1" src="/img/ml_4.png" /></p>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": ["content.tooltips", "navigation.instant", "navigation.tabs", "toc.integrate", "content.tabs.link"], "search": "../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.c8b220af.min.js"></script>
      
    
  </body>
</html>