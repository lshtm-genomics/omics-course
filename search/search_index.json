{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting Started","text":"<p>Welcome to the Omics course. Use the tabs to select the right practical session.</p>"},{"location":"#how-to-use-the-tutorials","title":"How to use the tutorials","text":"<p>During the course you will be following the practical sessions on this website. To enable you to make full use of these sessions we will give you a quick introduction on how these session will be structured. The practicals will be a combination of reading the theory and performing bioinformatic analysis on real data.</p> <p>Sections will be labelled according to its intended purpose and are described below:</p>"},{"location":"#general-text","title":"General text","text":"<p>Text which does not have any special formatting and looks (plain) like this will guide you through the practicals, providing background and explaining what anlyses we are performing, and why.</p>"},{"location":"#information-and-tips","title":"Information and tips","text":"<p>Info</p> <p>Text which appears in boxes of this colour aims to inform you of important information. </p>"},{"location":"#code","title":"Code","text":"<pre><code>Text which appears in boxes of this colour will tell that you are looking at a terminal command.\nYou can copy and paste from here straight to the terminal but before you do take a moment to understand what the command is actually doing.\nSeveral command lines may be present, with each new line representing a single command. \n</code></pre>"},{"location":"#terminal-output","title":"Terminal output","text":"<p>Terminal output</p> <pre><code>Text appearing in these boxes represents output you might expect to see in the terminal in response to a command.\nCheck to see if you get a similar output!\n</code></pre>"},{"location":"#questions","title":"Questions","text":"<p>Question</p> <p>Text in these boxes will usually ask an open ended question. If you cannot think of an answer or you want to check you have the right one, do not hesitate to ask one of the demonstrators for help! </p> <p>Question</p> Question <p>Sometimes a question has a specific answer. Click on the answer tab to see if you were right.</p> Answer <p>The answer will be here</p>"},{"location":"#thats-it","title":"Thats it!","text":"<p>You are now ready to progress to the tutorials. If you have any questions don't hesitate to ask one of the demonstrators for help. Good luck! </p>"},{"location":"connecting-github/","title":"Getting connected","text":""},{"location":"connecting-github/#introduction","title":"Introduction","text":"<p>There are three comonents you will need to understand before we get connected.</p>"},{"location":"connecting-github/#github-codespaces","title":"GitHub Codespaces","text":"<p>GitHub Codespaces is a cloud-based development environment that allows users to run and edit code directly in a virtual machine (VM) hosted by GitHub. It provides a fully configured workspace, pre-installed with necessary tools like Git, Python, and Docker, eliminating the need for users to set up a local environment. Every GitHub user has a certain number of free Codespace hours per month (120 for a free account), making it an ideal solution for doing some occasional bioinformatics. </p>"},{"location":"connecting-github/#visual-studio-code-vs-code","title":"Visual Studio Code (VS Code)","text":"<p>VS Code is a lightweight yet powerful code editor that supports multiple programming languages. It also allows users to connect to GitHub Codespaces, allowing users to work on cloud-based VMs as if they were local files. Users will launch GitHub Codespaces via VS Code, enabling them to interact with their virtual machine through a familiar coding interface.</p>"},{"location":"connecting-github/#tigervnc","title":"TigerVNC","text":"<p>TigerVNC (Virtual Network Computing) is a tool that allows users to remotely access a graphical desktop environment of their virtual machine. Since GitHub Codespaces primarily provides a terminal interface, TigerVNC will let users interact with a full virtual desktop, running graphical applications as if they were using a local computer.</p>"},{"location":"connecting-github/#how-these-work-together","title":"How These Work Together:","text":"<ol> <li>Users launch GitHub Codespaces to create a cloud-based virtual machine.</li> <li>They connect to the VM using VS Code, accessing the terminal and code files.</li> <li>If a graphical interface is needed, they use TigerVNC to access the full desktop environment.</li> </ol> <p>This setup allows users to work on cloud-hosted projects with both command-line and GUI access, making it ideal for bioinformatics and data analysis workflows.</p>"},{"location":"connecting-github/#getting-set-up","title":"Getting set up","text":""},{"location":"connecting-github/#1-set-up-a-github-account","title":"1. Set up a GitHub account","text":"<p>Head over to https://github.com/ and sign up for an account. You can skip this step if you already have an account.</p>"},{"location":"connecting-github/#2-download-visual-studio-code","title":"2. Download Visual Studio Code","text":"<p>Get the latest version of vscode from https://code.visualstudio.com/ and follow install instructions from the website.</p>"},{"location":"connecting-github/#3-download-tigervnc","title":"3. Download TigerVNC","text":"<p>Download TigerVNC from https://sourceforge.net/projects/tigervnc/files/stable/1.15.0/ and follow install instructions from the website. Based on your operating system, you can download the appropriate version.</p> <ul> <li>Windows: tigervnc64-1.15.0.exe</li> <li>Mac: TigerVNC-1.15.0.dmg</li> </ul> <p>Follow the instructions provided to install TigerVNC on your system.</p>"},{"location":"connecting-github/#4-get-connected","title":"4. Get connected","text":"<p>Once you have installed TigerVNC, you can connect to your GitHub Codespace using the following steps:</p> <ol> <li>Open Visual Studio Code.</li> <li>Install the GitHub Codespaces extension by clicking on the following link and following instructions: GitHub Codespaces.</li> <li>Open the command palette by pressing <code>Ctrl+Shift+P</code> (Windows/Linux) or <code>Cmd+Shift+P</code> (Mac).</li> <li>Type <code>Create new Codespace</code> and select the option.</li> <li>Enter <code>jodyphelan/teaching-codespaces</code> in the repository field.</li> <li>Select the <code>mapping</code> template.</li> <li>Select <code>2 cores, 8GB RAM, 32 GB storage</code> option.</li> </ol> <p>It should take a few minutes to create the Codespace. Once it's ready, you will be able to access the terminal and files directly from VS Code.</p> <p>If you need a graphical interface, you can use TigerVNC to connect to the Codespace. Open TigerVNC and enter the following address: <code>localhost:5901</code>. Once you've entered the password, you should see the full desktop environment of your Codespace.</p> <p>Important</p> <p>When you are finished working, make sure to stop the Codespace to using up your free hours. You can do this by opening up the command palette (<code>Cmd+Shift+P</code> or <code>Ctrl+Shift+P</code>) and typing <code>Codespaces: Stop Current Codespace</code>. You will be able to restart that Codespace at any time by selecting <code>Codespaces: Connect to Codespace</code> from the command palette.</p> <p>That's it! You're now connected to your GitHub Codespace and ready to start your bioinformatics journey. </p>"},{"location":"connecting-linode/","title":"Getting connected","text":""},{"location":"connecting-linode/#1-download-and-install-tigervnc","title":"1. Download and install TigerVNC.","text":"<p>Click the following link to download the appropriate installer depending on your OS: </p> <ul> <li>MacOS</li> <li>Windows</li> </ul> <p>Install using instructions provided.</p>"},{"location":"connecting-linode/#2-start-tigervnc","title":"2. Start TigerVNC","text":"<p>Start TigerVNC and enter the IP address followed by :5901 that you have been provided and click connect. For example if you IP address is 178.79.189.111 then enter such as in the screenshot below</p> <p></p>"},{"location":"connecting-linode/#3-enter-you-password","title":"3. Enter you password","text":"<p>Enter the password genomics and click OK</p> <p></p> <p>You should now see your virtual desktop and can start using it. </p> <p></p>"},{"location":"genome-assembly/","title":"Genome Assembly","text":""},{"location":"genome-assembly/#introduction","title":"Introduction","text":"<p>The main challenge of sequencing a genome is determining how to arrange reads into chromosomes. We have already explored mapping, where the sequence data is aligned to a known reference genome. A complementary technique, where no reference is used or available, is called de novo assembly.</p> <p>De novo assembly is the process of reconstruction of the sample genome sequence without comparison to other genomes. It follows a bottom-up strategy by which reads are overlapped and grouped into contigs. Contigs are joined into scaffolds covering, ideally, the whole of each chromosome in the organism. However de novo assembly from next generation sequence (NGS) data faces several challenges. Read lengths are short and therefore detectable overlap between reads is lower and repeat regions harder to resolve. Longer read lengths will overcome these limitations but this is technology limited. These issues can also be overcome by increasing the coverage depth, i.e. the number of reads over each part of the genome. The higher the coverage then the greater the chance of observing overlaps among reads to create larger contigs and being able to span short repeat regions. In addition, the modern hardware commonly output paired-end reads. These are two reads that are separated by a gap of known size. They enable the resolution of repeats greater than the read length by employing their expected separation and orientation as location constraints. Sequencing errors add difficulty since algorithms must allow certain mismatches when overlapping reads and joining regions, possibly leading to discarding true overlaps and false positives (Miller, Koren, &amp; Sutton, 2010)</p> <p>Despite all mentioned limitations, the high coverage currently achieved, growing read lengths (e.g. 100 read depth, &gt;150bp \u2013 Illumina HiSeq2500) and paired-end information make it feasible to obtain assemblies from small genomes (e.g. bacterial) with relatively low fragmentation. Current assemblers employ graph theory to represent sequences and their overlaps as a set of nodes and edges, being classified into three main groups: greedy, Overlap/Layout/Consensus (OLC) and de Bruijn graph assemblers</p> <p>Graph theory is a branch of discrete mathematics that studies problems of graphs. Graphs are sets of points called \u2018vertices\u2019 or \u2018nodes\u2019 joined by lines called \u2018edges\u2019. In the graph to the left there are 6 nodes and 7 edges. The edges are unidirectional which means that they can only be traversed in the direction of the arrow. For example paths in this graph include 3-2-5-1, 3-4-6 and 3-2-1.</p> <p></p> <p>The greedy algorithms consist of progressively adding single reads into contigs by end-to-end overlapping, starting with those reads with the highest overlap score and ending once no more joins can be found. Such scores are normally measured as the overlap length or the percentage of identity between reads along their joining region. SSAKE (Warren, Sutton, Jones, &amp; Holt, 2007), the first short-read assembler, is based on this approach as well as its two descendants SHARCGS (Dohm, Lottaz, Borodina, &amp; Himmelbauer, 2007) and VCAKE (Jeck et al., 2007).</p> <p>OLC assemblers build an overlap graph, in which nodes represent the reads and edges the overlaps. It requires a very time-consuming first step, where all reads are compared against each other. Paths along the graph show likely contigs.</p> <p>These two algorithms (greedy and OLC) are more suited to fewer, longer reads than those generated with NGS technologies. De Bruijn graph assemblers are the state-of-the-art approach for data sets composed of many thousands of short reads.</p> <p></p> <p>De Bruijn graph assemblers start by splitting the set of reads into k-mers, a set of overlapping sub-reads, and then use the latter to build the graph. Each edge represents an observed k-mer and its adjacent nodes the prefix and suffix of the original k-mer. Therefore, groups of overlapping reads are not actually computed but rather represented as paths in the graph. In the figure above the sequence ATGGCGTGCA with 3-mers and overlap of 2 base pairs (bp).</p> <p>Since edges correspond to all k-mers existing in the sampled genome, the assembly is resolved by finding a path that visits every edge in the graph. This approach is especially suitable to handle the large number of reads because each k-mer is stored at most once regardless of how many times it occurs in the reads. Several programs implement de Bruijn graph algorithms, including Euler (Pevzner, Tang, &amp; Waterman, 2001), Velvet (Zerbino &amp; Birney, 2008), ABySS (Simpson et al., 2009), AllPaths (Butler et al., 2008) and SOAPdenovo (Li et al., 2010).</p> <p>In conclusion, the de Bruijn graph assemblers are more appropriate for large amounts of data from high-coverage sequencing and have become the programs of choice when processing short reads produced by Illumina and other established platforms (&gt;100bp).</p>"},{"location":"genome-assembly/#exercise-1-de-novo-assemly-using-spades","title":"Exercise 1: De novo Assemly using Spades","text":"<p>De novo assembly is one the most computationally demanding processes in bioinformatics. Large genomes require many hours or days of processing. A small bacterial genome may take up to several hours to assemble. Here we will assemble M. tuberculosis genomes using Spades (Bankevich et al, 2012). We will compute assembly statistics to check the quality, and review how resulting contigs can be aligned, ordered and orientated along the reference genome using Abacas (Assefa, Keane, Otto, Newbold, &amp; Berriman, 2009).</p> <p>Quick start and data checks</p> <p>Activate the relevant <code>conda</code> environment</p> <pre><code>conda activate assembly\n</code></pre> <p>Change to the data directory:</p> <pre><code>cd ~/data/tb\n</code></pre> <p>And list the files there:</p> <pre><code>ls\n</code></pre> <p>There are 6 fastq files in this directory; these are the input data for this practical. There are paired-end reads for three samples. To view the first 8 lines of a fastq file type:</p> <pre><code>zcat sample1_1.fastq.gz | head -8\n</code></pre> <p>This gives:</p> <p>Terminal output</p> <pre><code>@HS3_5961:1:2207:17088:52965#1/1\nGCCGGTTGTTCGGCTGGAAGGTCCTTTTGCCCTTGGTCACGGGCGTCTCCTCGCTATGTCTGGCAACATCACCAT\n+\nGGGGGGEGFGFGGGFGDBEBF:AFDEFG?CFFDD:FBEE?EFEDFDEFEAEBFB8CCC?A@CD;ADDD86CCC&gt;&lt;\n@HS3_5961:1:1208:19785:88067#1/1\nCTTAGGGTCGCCGTTAAGTTCGGAGACGACCGCGTTCCACACTGTGGTGAAGCCTGAACCGGGGTCATCGGTCAA\n+\nHHHHFFHBHHHHHHHEGDDDGGEGFHHHHHHGHHHDFFEFFFDD&gt;DB@CDAEFF?FBFGDDCCC.=?5=&lt;@@;DA\n</code></pre> <p>Each read consists of 4 lines, the first is the read name, the second are the bases read, the third line is ignored and the fourth is the encoded quality of each read (one character per base).</p> <p>The number of lines in each file is reported by:</p> <pre><code>wc -l *\n</code></pre> <p>and therefore the number of reads can be calculated by dividing by 4. The length of the first read can be calculated by running:</p> <pre><code>zcat sample1_1.fastq.gz | head -2 | tail -1 | wc -c\n</code></pre> <p>This gives the number of characters in the line including the new line character, so it gives a number one greater than the first read length.</p> <p>Running Spades</p> <p>Spades is implemented in a single program that performs several steps in the assembly pipeline with a single command. The options are explained by running the command with no parameters (spades.py):</p> <p>You will see that there are many options and the pipeline can be customised extensively. However, for most purposes we can run the tool with the default settings. You only need to supply the reads and an output directory.</p> <p>An example invocation that takes the fastq formatted paired end files (Mtb_sample1_1.fastq.gz and Mtb_sample1_2.fastq.gz) and performs assembly.</p> <p><pre><code>spades.py -1 ~/data/tb/sample1_1.fastq.gz -2  ~/data/tb/sample1_2.fastq.gz -o sample1_asm\n</code></pre> Employing paired-end reads (rather than single-end) increases the assembly quality. In general, libraries with smaller insert size produce more fragmented and shorter assemblies, whilst the genome coverage does not increase that much. The combination of libraries with different insert size always gives the best results.</p> <p>De novo assembly gives much better results if reads are aggressively pre-filtered before passing them to the assembler. This is the same as we have seen for mapping and variant detection earlier in the course.</p> <p>Although anecdotal recommendations for the k-mer length range from lower than half of the read length up to 80% of read length, the parameter is difficult to determine in advance since it depends on coverage, read length, error rates and the sample properties (repeat regions, etc). Therefore, it is advisable to perform several assemblies using different values of k-mer length to decide a value.</p> <p>Thankfully spades automatically chooses a range of k-mer lengths to test and chooses the best one. The pipeline also performs reads correction, which aims to correct random errors in reads which will have a knock on effect on the k-mers used in the assembly.</p> <p>Eventually, after spades finishes running there will be several files will be obtained in the directory the most important for our purposes are:</p> <ul> <li> <p>A fasta file (contigs.fasta) containing all assembled contigs</p> </li> <li> <p>A fasta file (scaffolds.fasta) containing all assembled scaffolds</p> </li> </ul> <p>Scaffolds are contigs that have been chained together into larger sequences using information such from mate pair libraries] and reference sequences. The scaffolds should be very similar to the contigs in this assembly because the insert sizes of our library are relatively small. For the rest of this practical we will only be analysing the contigs.</p> <p>Assembly Quality</p> <p>The outcome of de novo assembly is a set of contigs. In a perfect assembly each contig covers exactly one chromosome in its entirety. In practice many contigs are created and the assembly quality is measured by examining their length; individually and in combination. An assembly is considered to be of good quality if relatively few long contigs are obtained covering the majority of the original or expected genome. There are different metrics used to assess the quality of assemblies:</p> <ul> <li> <p>N50. The length of the contig that contains the middle nucleotide when the contigs are ordered by size. Note, N80 or N60 may also be used.</p> </li> <li> <p>Genome coverage. If a genome reference exists then the genome coverage can be computed as the percentage of bases in the reference covered by the assembled contigs.</p> </li> <li> <p>Maximum/median/average contig size. Usually computed after removing the smallest contigs (e.g. discarding all contigs shorter than 150 bp).</p> </li> </ul> <p>We can compute these statistics with the help of a tool called quast (made by the same developers as spades). To get the statistics first navigate to the assembly directory and run <code>quast</code></p> <pre><code>cd sample1_asm\nquast -r ../tb.fasta -o quast contigs.fasta \n</code></pre> <p>After it has finished you can examing the outputs. To view the report enter cat quast/report.txt.</p> <p>Question</p> <p>See if you can spot the statistics we mentioned above. Compare the size of the reference with the size of the assembly. Are they different?</p> <p>Question</p> Question 1Answer 1 <p>See if you can spot the statistics we mentioned above. Compare the size of the reference with the size of the assembly. Are they different?</p> <p>Quast report includes information on statistics N50, Genome fraction and Maximum contig size that were mentioned before as well as additional metrics worth exploring. Average contig size can be additionally calculated by dividing values from fields <code>Total length (&gt;= 0 bp)</code> and <code># contigs (&gt;= 0 bp)</code> or directly from FASTA file with external tools for example <code>seqmagick</code>. It is advised to look at a variety of metrics and supporting analyses to determine quality of assembly. The total size of reference genome (1000020 bp) and resulting genome assembly (1005736 bp) are comperable.</p> <p>High depth of coverage is essential to obtain high quality assembled genomes. It is always the case that contig sizes drop when coverage decreases under a certain value, generally considered to be 50x, although such threshold will depend on the size and repeat content of the sequenced genome. A coverage limit is reached above which no improvement on assembly metrics is observed. As shown to the left, coverage greater that 50x does not significantly improve N50 in an assembly of E. coli. Although such thresholds will not necessarily be the same for other genomes, the principles apply for all.</p> <p></p> <p>More data requires more memory too, as the lower graph demonstrates. Again the figures are genome specific but it is easy to see how large computers can quickly become necessary as coverage and genome size increase. (Illumina, 2009).</p>"},{"location":"genome-assembly/#exercise-2-contig-ordering","title":"Exercise 2: Contig Ordering","text":"<p>Ordering with ABACAS</p> <p>To align, order and orientate contigs along the reference genome we will be using ABACAS (Assefa et al., 2009). The output will be in the form of a pseudo molecule, where all of the contigs will be joined by N\u2019s. To display the complete list of possible parameters by typing:</p> <pre><code>abacas.pl \u2013h\n</code></pre> <p>The basic usage (for this exercise) is:</p> <pre><code>cp ~/data/tb/tb.fasta .\nabacas.pl -r tb.fasta -q contigs.fasta -p nucmer -b -d -a -m -N -g sample1 -o sample1_asm\n</code></pre> <p>This will create a pseudomolecule (into sample1_asm.fasta) from the assembled contigs (in contigs.fa). The program creates a number of files with the same base filename for the other requested processing output, including gap information (-g, sample1_asm.gaps, sample1_asm.gaps.tab), a comparison file (sample1_asm.crunch), no padding version (-n, sample1_asm.NoNs.fasta), etc.</p> <p>Visualisation with ACT</p> <p>Artemis Comparison Tools (ACT) (Carver et al., 2005) is a tool for displaying pair-wise comparison between two or more DNA sequences. It was developed to help identify and analyse regions of similarity and difference between genomes along their entire sequences and their annotation. ABACAS produces the required files to compare the original reference genome with the pseudo molecule resulting after ordering assembled contigs.</p> <p>Launch ACT by running:</p> <pre><code>act\n</code></pre> <p></p> <p>Click File &gt; Open. Select the reference fasta file (tb.fasta) and the pseudomoelcule (sample1_asm.fasta) as the two sequence files with the associated comparison file (sample1_asm.crunch) \u2013 you will have to navigate the directories. Then press Apply.</p> <p>A map between the two sequences will be displayed. To load annotation click on File &gt; tb.fasta &gt; Read An Entry... and select the tb.gff file from the <code>~/data/tb/</code> directory. To make stop codons disappear (represented as black vertical lines) right-click on the reference and the pseudo molecule panels and deselect Stop Codons option in the right-click menu.</p> <p>Right-click on one of the red ribbons linking the contigs and select View Selected Matches to find out the first and last position the contig maps to the reference and to the pseudo molecule. The percentage of identity between them is also shown.</p> <p></p>"},{"location":"genome-assembly/#exercise-3-mapping-back-to-the-reference","title":"Exercise 3: Mapping back to the Reference","text":"<p>Contigs aligning with large internal gaps or low confidence indicate either structural variants or misassemblies, and should be carefully monitored. If paired-end reads are available then mapping them back onto the contigs (pseudo molecule) is another source of information. Large deviations in the mapping distance or read pairs in which only one read maps against the contigs may hint at misassemblies rather than structural variation</p> <p>The following commands maps the raw reads against the pseudo molecules from the previous exercise. This is a similar process to those you have used in the previous classes.</p> <pre><code>cd ~/data/tb/sample1_asm\nbwa index -a is sample1_asm.fasta\nbwa mem -k 20 -c 100 -L 20 -U 20 -M -T 50 sample1_asm.fasta ~/data/tb/sample1_1.fastq.gz ~/data/tb/sample1_2.fastq.gz | samtools sort -o sample1_asm.bam\nsamtools index sample1_asm.bam\n</code></pre> <p>This process indexes, maps and stores the fastq paired-end reads against the ABACAS ordered pseudomolecule (sample1/sample1_asm.fasta) for sample1 created by spades. The output is a BAM file (sample1/sample1_asm.bam).</p> <p>This script can be modified to create a BAM file for the sequence data mapped against any reference genome, just as you have done in the previous class.</p> <p>These files can be visualised in ACT. File &gt; sample1_asm.fasta &gt; Read BAM/VCF... and choose the remapped.bam file (sample1_asm.bam). Do the same for tb.fasta and choose the originally mapped BAM file (sample1.bam). These BAM data are represented as pileup data \u2013 each read stacked up over the section of the reference that it has been mapped to. Extra information can be presented by right-clicking on the pileups and selecting Graph &gt; Coverage or View &gt; Inferred Size.</p> <p>The graph shown gives a good insight into the quality and depth of the assembly and is very useful for finding regions that have been misaligned (i.e. joined in error) as their coverage should be very low or very high.</p> <p></p> <p>In ACT inspect the region between 76,000 and 89,000</p> <p>Question</p> Question 2Answer 2Question 3Answer 3Question 4Answer 4 <p>How does the coverage vary across the pseudo molecule?</p> <p>Coverage across the pseudo molecule is sufficient across most regions. There are few visible fragmentations for which coverage is not existent that should be expected from the nature of the assembly.</p> <p>How does this compare to the coverage across the matching regions mapped against the reference?</p> <p>Overall coverage trends in comparison to original mapping assembly are similar. In the region of interest, we can notice pseudo molecule having uniform coverage whilst in mapping assembly the lack of supporting reads spanning between 795000-83000.</p> <p>Is there any kind of structural variant involved?</p> <p>A few regions based on the coverage distribution might indicate the existence of structural variants that should be investigated further.</p>"},{"location":"genome-assembly/#exercise-4-structural-variant-validation","title":"Exercise 4: Structural Variant Validation","text":"<p>Structural variant (SV) software aims to detect signatures left by changes in chromosome material and chromosome rearrangements. However, reads mapping to repetitive or highly polymorphic regions may mimic the same signatures (false positives).</p> <p>Here we describe a de novo assembly approach that we call targeted assembly of candidate SV regions. It consists of obtaining the reads from the predicted SV region, performing de novo assembly and mapping the resulting contig back to the reference. The nature of the contig alignment can reveal the SV.</p> <p>We have previously identified a deletion in sample1 in the region 79570-83050 using delly. This region has been investigated above, in the context of a large assembly, and interesting mapping and assembly results have been found.</p> <p>The reads are extracted from the BAM file using samtools. Extracting a wider range (by at least 400bp) than the candidate region enables spades to construct good contigs either side of the possible SV.</p> <pre><code>cd ~/data/tb\nsamtools view -b sample1.bam Chromosome:79000-84000  | samtools fastq - &gt; region.fastq\n</code></pre> <p>The next step is to perform de novo assembly of the extracted reads.</p> <pre><code>spades.py -s region.fastq -o region_assembly\n</code></pre> <p>We map the output contigs back to the reference with BLAST. This powerful alignment algorithm is available online or for download as a command line tool. Go to the web address http://blast.ncbi.nlm.nih.gov/Blast.cgi and navigate to the Nucleotide BLAST page. Check the \u201cAlign two or more sequences\u201d box. Upload the contigs file (~/data/tb/region_assembly/contigs.fasta) as the subject and the tb reference (~/data/tb/tb.fasta) as the query. Enter an appropriate region of the reference to blast against as there is little point in searching the entire genome when we are only interested in a small range (79100-83900).</p> <p></p> <p>Blast Alignment input screen. Choose the appropriate reference and sample sequences, enter the range of interest and click BLAST.</p> <p></p> <p>Once the server has blasted your data, it will create a web page for the results with graphical and textual information on how well the sequences aligned.</p> <p>Question</p> Question 5Answer 5 <p>Is it a deletion? Or was it a false positive? Using the output from delly in the variant detection practical have a look at other SVs as well.</p> <p>BLAST Graphic Summary of the resulting query contains a few rectangles (size correlates with alignment length) that represent regions of the sequence that maps to the reference. Rectangles joined by a thin line show that the contig sequence was split during alignment. It is indicating that the reads assembled into this single contig needed to be separated by a gap when aligning to the reference \u2013 therefore we can conclude a deletion compared to the reference. In order to validate in-depth deletion additional information could be extracted from long-read sequencing to prove or disprove the hypothesis. Shorter rectangles are matches coming from other smaller assembled contigs that may potentially reveal alternative areas of similarity or small repetitive sequences.</p>"},{"location":"genome-assembly/#advanced-topics","title":"Advanced Topics","text":"<p>Annotation Transfer</p> <p>Once the contigs ordered are against the reference, it is useful to determine the position and function of possible genes. It is possible to implement ab initio gene finding, but an alternative approach is to use the annotation of the reference and adapt it to the new assembly. A tool called RATT (Otto et al., 2011 \u201cRapid annotation transfer tools\u201d) has been developed to transfer the annotation from a reference to a new assembly. In the first step the similarity between the two sequences is determined and a synteny map is constructed. This map is used to align the annotation of the reference onto the new sequence. In a second step, the algorithm tries to correct gene models. One advantages of RATT is that the complete annotation is transferred, including descriptions. Thus careful manual annotation from the reference becomes available in the newly sequenced genome. Where no synteny exists, no transfer can be performed. An outputted combined \u201cembl\u201d format file with sequence and inferred annotation can be viewed in artemis to assess the quality of the transfer.</p> <p>Improving assemblies</p> <p>The quality of the assembly is determined by a number of factors, including the type of software used, the length of reads and the repetitive nature or complexity of the genome. For example, assembly of Plasmodium is more difficult than that of most bacteria (due to the high AT content and repeats). A good assembly of a bacterial genome will return 20-100 supercontigs. It is possible to manually / visually improve assemblies and annotation within Artemis. To improve the assembly in a more automated fashion, there is other software:</p> <ul> <li> <p>SSPACE: </p> <p>This tool can scaffold contigs. Although spades can also scaffold contigs, SSPACE generally performs better.</p> </li> <li> <p>Abacas:</p> <p>This tool has the option to design primers that can be used to generate a PCR product to span a possible gap. The resulting new sequence can then be included in the assembly. This process is called finishing.</p> </li> <li> <p>Image:</p> <p>This tool can close gaps in the assembly automatically. First the reads are mapped against the assembly. Using all reads that map close to a gap and with their mate pairs within the gap, it is possible to perform a local assembly. This process is repeated iteratively. This procedure can close up to 80% of the sequencing gaps (Tasi et al, 2010).</p> </li> <li> <p>iCORN:</p> <p>This tool can correct base errors in the sequence. Reads are mapped against the reference and differences are called. Those differences or variants that surpass a specified quality threshold are corrected. A correction is accepted if the amount of perfect mapping reads does not decrease. This algorithm also runs iteratively. Here, perfect mapping refers to the read and its mate pair mapping in the expected insert size without any difference to the iteratively derived reference.</p> </li> </ul> <p>All these programs are available through the PAGIT suite (post assembly genome improvement toolkit).</p> <p>References</p> <p>Assefa, S., Keane, T. M., Otto, T. D., Newbold, C., &amp; Berriman, M. (2009). ABACAS: algorithm-based automatic contiguation of assembled sequences. Bioinformatics (Oxford, England), 25(15), 1968-9. doi:10.1093/bioinformatics/btp347</p> <p>Butler, J., MacCallum, I., Kleber, M., Shlyakhter, I. A, Belmonte, M. K., Lander, E. S., Nusbaum, C., et al. (2008). ALLPATHS: de novo assembly of whole-genome shotgun microreads. Genome research, 18(5), 810-20. doi:10.1101/gr.7337908</p> <p>Carver, T. J., Rutherford, K. M., Berriman, M., Rajandream, M.-A., Barrell, B. G., &amp; Parkhill, J. (2005). ACT: the Artemis Comparison Tool. Bioinformatics (Oxford, England), 21(16), 3422-3. doi:10.1093/bioinformatics/bti553</p> <p>Compeau, P.E.C., Pevzner, P.A., Tesler, G. (2007) How to apply de Bruijn graphs to genome assembly. Nature Biotechnology 29(11) 987-991</p> <p>Dohm, J. C., Lottaz, C., Borodina, T., &amp; Himmelbauer, H. (2007). SHARCGS, a fast and highly accurate short-read assembly algorithm for de novo genomic sequencing. Genome research, 17(11), 1697-706. doi:10.1101/gr.6435207</p> <p>Illumina, I. (2009). De Novo Assembly Using Illumina Reads. Analyzer. Retrieved from http://scholar.google.com/scholar?hl=en&amp;btnG=Search&amp;q=intitle:De+Novo+Assembly+Using+Illumina+Reads#0</p> <p>Jeck, W. R., Reinhardt, J. a, Baltrus, D. a, Hickenbotham, M. T., Magrini, V., Mardis, E. R., Dangl, J. L., et al. (2007). Extending assembly of short DNA sequences to handle error. Bioinformatics (Oxford, England), 23(21), 2942-4. doi:10.1093/bioinformatics/btm451</p> <p>Li, R., Zhu, H., Ruan, J., Qian, W., Fang, X., Shi, Z., Li, Y., et al. (2010). De novo assembly of human genomes with massively parallel short read sequencing. Genome research, 20(2), 265-72. doi:10.1101/gr.097261.109</p> <p>Margulies, M., Egholm, M., Altman, W. E., Attiya, S., Bader, J. S., Bemben, L. a, Berka, J., et al. (2005). Genome sequencing in microfabricated high-density picolitre reactors. Nature, 437(7057), 376-80. doi:10.1038/nature03959</p> <p>Miller, J. R., Koren, S., &amp; Sutton, G. (2010). Assembly algorithms for next-generation sequencing data. Genomics, 95(6), 315-27. Elsevier Inc. doi:10.1016/j.ygeno.2010.03.001</p> <p>Pevzner, P. a, Tang, H., &amp; Waterman, M. S. (2001). An Eulerian path approach to DNA fragment assembly. Proceedings of the National Academy of Sciences of the United States of America, 98(17), 9748-53. doi:10.1073/pnas.171285098</p> <p>Simpson, J. T., Wong, K., Jackman, S. D., Schein, J. E., Jones, S. J. M., &amp; Birol, I. (2009). ABySS: a parallel assembler for short read sequence data. Genome research, 19(6), 1117-23. doi:10.1101/gr.089532.108</p> <p>Warren, R. L., Sutton, G. G., Jones, S. J. M., &amp; Holt, R. a. (2007). Assembling millions of short DNA sequences using SSAKE. Bioinformatics (Oxford, England), 23(4), 500-1. doi:10.1093/bioinformatics/btl629</p> <p>Zerbino, D. R., &amp; Birney, E. (2008). Velvet: algorithms for de novo short read assembly using de Bruijn graphs. Genome research, 18(5), 821-9. doi:10.1101/gr.074492.107</p> <p>Acknowledgements: Thomas Otto and Wellcome Trust.</p>"},{"location":"ml_cpu/","title":"Deep learning (Neural Network)","text":"<p>Next-generation sequencing data is being produced at an ever-increasing rate. The raw data is not meaningful by itself and needs to be processed using various bioinformatic software. This practical will focus on genomic resequencing data where the raw data is aligned to a reference genome.</p>"},{"location":"ml_cpu/#introduction","title":"Introduction","text":"<p>Deep learning neural networks have revolutionized the field of predictive modelling, especially in bioinformatics and genomics. By utilizing multiple layers of artificial neurons to extract and process data, deep learning networks have shown impressive performance in tasks such as image classification, natural language processing, and speech recognition. The same principles can be applied to infectious disease genomic DNA data for drug resistance prediction. By leveraging the MINIST complexity and high-dimensional nature of genomic data, deep learning networks can learn to identify subtle patterns and relationships that may be missed by traditional statistical methods. Additionally, deep learning networks can be optimized to handle missing data, noisy data, and varying data types, which are common challenges in genomic data analysis.</p> <p></p> <p>Knowledge</p> <p>A neural network is a type of machine learning model that consists of layers of interconnected nodes, also known as neurons. Each neuron takes in input values, multiplies them by weights, and applies an activation function to produce an output. The output from one layer becomes the input to the next layer until the final layer produces the model's prediction. During training, the model adjusts the weights to minimize the difference between its prediction and the actual output. This process is repeated multiple times until the model's predictions become accurate enough for the desired task.</p> <p>In summary, deep learning neural networks offer a promising approach for predicting drug resistance from infectious disease genomic DNA data. By leveraging the complex and high-dimensional nature of genomic data, these networks can identify subtle patterns and relationships that are difficult to detect using traditional statistical methods. With the growing threat of drug-resistant infectious diseases.</p> <p>However, In this practical, we will focus on training a model on the classic Mnist.</p> <p>The MNIST dataset, short for Modified National Institute of Standards and Technology database, is a widely recognized and fundamental dataset in the field of computer vision and machine learning. It consists of a collection of 28x28 pixel grayscale images of handwritten digits, ranging from 0 to 9. MNIST serves as a benchmark for various classification tasks.</p> <p>Important</p> <p>Before doing anything, we need to first activate the conda environment for this practical by typing the following: <code>conda activate deep_learning</code>. This environment contains most of the software we need for this practical. This command needs to be run each time we open up a new terminal or switch from a different environment. </p>"},{"location":"ml_cpu/#exercise-1-running-the-neural-network-for-predicting-isoniazid-resistance","title":"Exercise 1: Running the neural network for predicting Isoniazid resistance","text":"<p>Here we train a model using to recognise hand written digits using it's colorgradient</p> <p>You can list all installed environments with <code>cond env list</code>.</p> <p>In the terminal navigate to the \u2018ml_workshop/MINIST_model\u2019 directory, by typing:</p> <pre><code>cd ~/ml_workshop/mnist_model\n</code></pre> <p>Now type the commands below to train the model with default parameters.</p> <pre><code>python MINIST_model.py\n</code></pre> <p>Important</p> <p>In the command, it is important to specify  python  before the script to complie (run the script) using python.</p> <p>The model will run for 10 iterations, which will take about 2 minutes to complete, feel free to read forward, go through the addition info at the bottom of the page and play with the model on the webpage Neural network Playground</p> <p>Two graphs will be are produced:</p> <pre><code>INH-model_LR:0.001-DR:0.2-ACC.png\nINH-model_LR:0.001-DR:0.2-LOSS.png\n</code></pre> <p>You can open the current folder using command <code>open .</code>. Double click on the picture files to view them.</p> <p><code>MNISTmodel_LR:0.001-DR:0.2-LOSS.png</code></p>"},{"location":"ml_cpu/#acc-accuracy","title":"ACC-accuracy","text":"<p>Accuracy is a common performance metric used in deep learning to evaluate the effectiveness of a model at predicting the correct output. It measures the proportion of correct predictions made by the model out of the total number of predictions. </p> <p>Intuition</p> <p>Imagine you are playing a game of darts and aiming for a bullseye. Your accuracy is determined by the number of times you hit the bullseye compared to the total number of attempts. </p> <p>Similarly, in deep learning, accuracy measures how often the model's predictions match the true labels for a given set of inputs. </p> <p>For example, if a model predicts that an image contains a cat and the true label is also cat, then that prediction is counted as correct. </p> <p>The higher the accuracy, the better the model is at making correct predictions. However, accuracy can be influenced by factors such as class imbalance or the types of errors the model makes. Therefore, it is important to consider other performance metrics in addition to accuracy when evaluating the performance of a deep learning model.</p>"},{"location":"ml_cpu/#loss-loss","title":"LOSS-loss","text":"<p>In deep learning, the goal of the model is to accurately predict the outcome of a given task, such as image recognition or natural language processing. </p> <p>Loss is a term used to quantify the difference between the predicted output and the actual output. </p> <p>Intuition</p> <p>Think of it like a student taking a test - the score they receive is a measure of how well they performed relative to the expected outcome. </p> <p>Similarly, the loss function in deep learning measures how well the model is performing by comparing its predictions to the actual results. The lower the loss, the better the model is at predicting the outcome. </p> <p>The goal of training a deep learning model is to minimize the loss over the course of multiple iterations, or epochs, of training. Different types of loss functions can be used depending on the task and the type of output being predicted. Ultimately, the goal is to choose a loss function that encourages the model to learn the desired features and make accurate predictions.</p>"},{"location":"ml_cpu/#epoch","title":"Epoch","text":"<p>In deep learning, an epoch refers to a complete pass through the entire training dataset during the model training process. </p> <p>Intuition</p> <p>Think of it like a chef preparing a recipe - each time they go through the entire recipe from start to finish, that's one epoch. </p> <p>During each epoch, the model is shown a batch of input data and the corresponding output labels, and it updates its parameters based on the difference between the predicted output and the actual output. </p> <p>The number of epochs that a model is trained for is an important hyperparameter that can impact the performance of the model. Training for too few epochs may result in a model that underfits or fails to learn the underlying patterns in the data. On the other hand, training for too many epochs may result in a model that overfits or becomes too specialized to the training data and fails to generalize well to new data. Therefore, the number of epochs should be chosen carefully to balance between underfitting and overfitting and achieve the best performance for the task at hand.</p>"},{"location":"ml_cpu/#exercise-2-try-different-values-of-dropout-and-learning-rate","title":"Exercise 2 :Try different values of dropout and learning rate","text":"<p>Type the commands below to train the model with different parameters.</p> <p>Default set up <pre><code>python MINIST_model.py -lr 0.001 -dr 0.2 \n</code></pre> High learning rate, high dropout rate <pre><code>python MINIST_model.py -lr 0.01 -dr 0.6 \n</code></pre> High learning rate, low dropout rate <pre><code>python MINIST_model.py -lr 0.01 -dr 0.01 \n</code></pre> Low learning rate, high dropout rate <pre><code>python MINIST_model.py -lr 0.0002 -dr 0.6\n</code></pre> Low learning rate, low dropout rate <pre><code>python MINIST_model.py -lr 0.0002 -dr 0.01\n</code></pre></p> <p>Also feel feel free to try different values within the below ranges -    learning rate: between 10e-6 and 1 -    Dropout rate:  between 0 and 1</p> <p>When working with python script that required input, you can also View the explanation for each input parameter using <code>python full_model.py -h</code>.</p> <pre><code>usage: MINIST_model.py [-h] [-lr LEARNING_RATE] [-dr DROPOUT_RATE]\n\nIsoniazid prediction model using KatG sequences as input.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -lr LEARNING_RATE, --learning_rate LEARNING_RATE\n                        Learning rate for the model(between 10e-6 and 1) (default: 0.001)\n  -dr DROPOUT_RATE, --dropout_rate DROPOUT_RATE\n                        Dropout rate for hte model layers (between 0 and 1) (default: 0.2)\n</code></pre>"},{"location":"ml_cpu/#lr-learning-rate","title":"lr: learning rate","text":"<p>Learning rate is a key parameter in many machine learning algorithms. </p> <p>It is the step size that a model takes when trying to find the best set of weights to make accurate predictions. </p> <p>Intuition</p> <p>Imagine you are climbing a hill to reach the top. Your learning rate is how big of a step you take with each stride. If your steps are too small, it will take a long time to reach the top, but if your steps are too big, you might overshoot and miss the peak. </p> <p>Similarly, in machine learning, a small learning rate will cause the algorithm to take longer to converge, while a large learning rate can cause the algorithm to overshoot and make inaccurate predictions. Therefore, choosing the right learning rate is important to achieve optimal results in machine learning models.</p>"},{"location":"ml_cpu/#dr-dropout-rate","title":"dr: dropout rate","text":"<p>Dropout is a technique used in machine learning to prevent overfitting, which is when a model becomes too specialized to the training data and fails to generalize well to new data. </p> <p>Dropout works by randomly dropping out, or \"turning off\", some of the neurons in a neural network during training. </p> <p>Intuition</p> <p>Again the chef intuition - if they only rely on a few key ingredients, the dish may taste great in the kitchen but won't necessarily appeal to a wider audience. </p> <p>By randomly \"turning off\" some of the neurons, the model is forced to learn more robust and diverse features, leading to better generalization to new data. The dropout rate is the percentage of neurons that are randomly dropped out during each training epoch, and it is another important hyperparameter that can be tuned to optimize the performance of the model.</p> <p></p> <p>These are some rough guidance, take it with a grain of salt as these parameters need to be talored to the datasets.</p> Learning Rate Dropout Rate Consequences High High Fast learning, high regularization, risk of underfitting High Low Fast learning, low regularization, risk of overfitting Low High Slow learning, high regularization, good generalization Low Low Slow learning, low regularization, risk of overfitting <p></p> <p>A high learning rate means the model will make larger updates to its weights during training, while a low learning rate means the updates will be smaller. </p> <p>A high dropout rate means more neurons will be dropped out during training, while a low dropout rate means fewer neurons will be dropped out.</p> <p>If both the learning rate and dropout rate are high, the model will learn quickly but may not generalize well to new data due to excessive regularization. If both the learning rate and dropout rate are low, the model will learn slowly and may overfit to the training data. </p> <p>If the learning rate is low and the dropout rate is high, the model will learn slowly but regularize effectively, leading to good generalization. Finally, if the learning rate is high and the dropout rate is low, the model will learn quickly but may overfit due to insufficient regularization.</p>"},{"location":"ml_cpu/#exercise-3-converting-fastq-to-onehot-encoding","title":"Exercise 3: Converting Fastq to onehot encoding","text":"<p>In machine learning, one-hot encoding is a way to represent categorical data in a numerical format that can be easily understood by algorithms.</p> <p>In the case of DNA data, each nucleotide (A, T, C, G) can be considered as a category, and one-hot encoding is used to represent each nucleotide as a unique binary value. </p> <p>This is necessary because machine learning algorithms require numerical inputs to make predictions, and simply representing DNA sequences as strings of characters would not be suitable for most machine learning tasks. </p> <p>By using one-hot encoding, we can represent each DNA sequence as a series of binary values that capture the presence or absence of each nucleotide at each position in the sequence. This allows us to use a wide range of machine learning algorithms to analyse and make predictions based on DNA data, such as predicting the likelihood of a genetic disorder or identifying regions of the genome that are associated with certain traits or diseases.</p> <p>First change the directory to the script folder using hte below command:</p> <pre><code>cd ../fastq2oh\n</code></pre> <p>Now type the commands below to view the first few lines of the original fastq file:</p> <pre><code>less ERR6634978_1.fastq | head\n</code></pre> <p>Reminder</p> <p>FASTQ is a text-based file format commonly used in bioinformatics to store and exchange sequences and their corresponding quality scores. It consists of four lines per sequence: the first line starts with \"@\" followed by a unique identifier for the sequence, the second line contains the actual nucleotide sequence, the third line starts with \"+\" followed by the same unique identifier as in the first line, and the fourth line contains the quality scores corresponding to each nucleotide in the sequence. The quality scores represent the confidence level of each nucleotide call and are represented as ASCII characters. The FASTQ format is widely used in sequencing technologies such as Illumina, Ion Torrent, and PacBio.</p> <p>Output</p> <pre><code>@A00386:50:HGY3TDRXY:1:2101:11496:1016 1:N:0:ATTACTCG+TAAGATTA\nGNCGTTGGCGATGCGCACGGTGTTGGAGAGCGTGCCACCCGTGACGGTGCCGTCCGAGATCGTCCGGCTGCAAGAGCAGCTGGCCCAGGTGGCAAAGGGTGAGGCTTTCCTGCTGCAGGGCGGCGACTGCGCTGAGACATTCATGGACAAC\n+\nF#FFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFF,F\n@A00386:50:HGY3TDRXY:1:2101:12382:1016 1:N:0:ATTACTCG+TAAGATTA\nGNCGGACGTGTCGAACTTGGGGCCTACGACGCCGAACATGACCTGATCCTGGAGAACGACCGCGGCTTCGTGCAGGTCGCCGGTGTCAACCAGGTCGGGGTGCTGCTCGC\n+\nF#FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF\n@A00386:50:HGY3TDRXY:1:2101:21386:1016 1:N:0:ATTACTCG+TAAGATTA\nATTCCACCGCCTCGGCGACCACGACCAGCACGATCAATGTCCGGGCACTATCCCCGGCGCTGGTGGTGACATAGATCGGGTAATACCCCGACGGCACCGATCGAGCTACGGTGATCGCGACC\n</code></pre> <p>Reminder</p> <p><code>less</code> show a scrollable content of the file, <code>head</code> allow the terminal to only show the first few lines from less output.</p> <p>As a little revision from the alignment lecture, let's go through mapping to get the consensus sequence in a one hot encoded format, starting from a fastq file: <code>ERR6634978.fq.gz</code></p> <pre><code>Get the environment for file conversion\n```\nconda activate fastq2oh\n```\n\nRun Alignment and index the bam file\n```\nbwa mem MTB-h37rv_asm19595v2-eg18.fa ERR6634978.fq.gz | samtools sort -o ERR6634978.bam -\nsamtools index ERR6634978.bam\n```\n\nRun the conversion script to generate onehot encoded nucleotide sequence from ERR6634978.bam file\n```\npython fastq2oh.py -r gene.csv -o `ERR6634978`_oh -b ERR6634978.bam\n```\n\n-    r-gene regions\n-    o-output file name\n-    b-input bam file\n\nNow type the commands below to view generate onehot encoded sequences:\n```\n</code></pre> <p>less  ERR6634978_oh | head ```</p> <p>Output</p> <p><code>A,C,G,T 1,0,0,0 0,1,0,0 1,0,0,0 0,1,0,0 0,0,1,0 0,0,0,1 0,1,0,0 0,0,1,0 1,0,0,0</code> </p>"},{"location":"ml_cpu/#exercise-4-running-full-model-that-predicts-for-all-13-drug-resistances","title":"Exercise 4: Running full model that predicts for all 13 drug resistances","text":"<p>Now try to use a fully trained model to predict all 13 different types of drug resistance.  First change the directory to the script folder using hte below command:  <code>cd ../full_model</code>  Now type the below commands to train the model with different parameters, show output in terminal and saving output into <code>drug_predictions.txt</code> file:  <code>python full_model.py -i ../fastq2oh/ERR6634978_oh -v -o drug_predictions.txt</code></p> <ul> <li>i-one hot encoded input file</li> <li>o-output file name</li> <li>v-verbose (show output in the terminal)</li> </ul> <p>You can also view the output file using <code>less drug_predictions.txt</code>. Feel free to scroll around and press <code>Q</code> to exit.</p>"},{"location":"ml_cpu/#additional-information","title":"Additional information","text":"<p>In case you'd like a more hands on and visualised example of how a neural networks functions. Neural network Playground</p>"},{"location":"ml_cpu/#what-models-are-we-actually-using","title":"What models are we actually using","text":""},{"location":"ml_cpu/#convolutional-neural-network-cnn","title":"Convolutional Neural Network (CNN)","text":"<p>A Convolutional Neural Network (CNN) is a class of deep learning models designed primarily for processing structured grid data, such as images and videos. It's a specialized type of artificial neural network that excels at recognizing patterns and features in multi-dimensional data. At its core, a CNN employs a series of convolutional layers that perform convolutions, which are mathematical operations involving the element-wise multiplication of small, learnable filters with overlapping regions of the input data. These filters act as feature detectors, capable of identifying distinctive patterns like edges, corners, or textures within the data. By stacking multiple convolutional layers, a CNN can progressively learn complex hierarchical features, allowing it to recognize higher-level patterns, shapes, and objects in the input. Additionally, CNNs often include pooling layers to reduce spatial dimensions and fully connected layers for classification or regression tasks. The ability to automatically extract relevant features from raw data makes CNNs exceptionally powerful for image classification, object detection, facial recognition, and a wide range of computer vision tasks. Their success is attributed to their capacity to capture spatial hierarchies and translational invariance in data, enabling them to excel in various visual recognition applications. </p>"},{"location":"ml_cpu/#other-neural-networks","title":"Other neural networks","text":""},{"location":"ml_cpu/#recurrent-neural-network-rnn","title":"Recurrent Neural Network (RNN)","text":"<p>Well, what else is out there, many! graphical networks, Generative adversarial network, Autoencoder, diffusion model... But one other very OG model type is RNN. </p> <p>A Recurrent Neural Network (RNN) is like a smart detective that looks at a series of clues one by one to solve a mystery. Each clue represents a piece of information in a sequence, like words in a sentence. The RNN doesn't just look at one clue at a time; it remembers what it saw before.</p> <p>Imagine reading a story one word at a time. As you read, you understand the story because you remember the words that came before. RNNs work in a similar way. They have a memory that keeps track of the words they've seen. So, when they see a new word, they use it along with their memory to make sense of the story.</p> <p>This memory helps RNNs with tasks like predicting the next word in a sentence or understanding the sentiment of a text. However, sometimes RNNs can forget important things if the story is too long. That's why there are improved versions of RNNs, like LSTM and GRU, which have better memory and can understand longer stories.</p> <p>In a nutshell, RNNs are like detectives for sequences of data. They remember what they've seen before to make sense of what comes next, which makes them great for tasks involving sequences, like understanding language or predicting future values in time series data.</p>"},{"location":"ml_cpu/#more-on-regularisation","title":"More on regularisation","text":"<p>Regularization is a technique used in deep learning to prevent overfitting and improve the generalization performance of a model. There are several ways to regularize a deep learning model, including:</p> <ol> <li>L1 and L2 Regularization: These are the most common types of regularization used in deep learning. L1 regularization adds a penalty term to the loss function that is proportional to the absolute values of the model parameters, while L2 regularization adds a penalty term that is proportional to the squared values of the model parameters. Both types of regularization encourage the model to learn simpler, more interpretable features by shrinking the magnitude of the parameters.</li> <li>Dropout: Dropout is a technique that randomly drops out (i.e., sets to zero) a proportion of the neurons in a layer during training. This helps prevent overfitting by forcing the network to learn more robust features that do not rely on the activation of specific neurons.</li> <li>Data Augmentation: Data augmentation is a technique that artificially increases the size of the training dataset by creating new examples from the existing ones. This can be done by applying transformations such as rotations, flips, and crops to the original images, or by adding noise to the input data.</li> <li>Early Stopping: Early stopping is a technique that stops the training process before the model starts to overfit. This is done by monitoring the validation error during training and stopping the training process when the validation error stops improving.</li> <li>Batch Normalization: Batch normalization is a technique that normalizes the activations of a layer by subtracting the mean and dividing by the standard deviation of the activations in a batch of data. This helps to reduce the internal covariate shift, which can improve the training speed and stability of the model.</li> <li>Max-Norm Regularization: Max-Norm regularization constrains the magnitude of the weight vector for each neuron to a fixed value. This helps to prevent large weight updates during training, which can lead to overfitting</li> <li>Label Smoothing: In label smoothing, instead of assigning a one-hot vector to the target labels, a smoothed label distribution is used. This helps prevent overfitting by introducing a small amount of noise into the training targets, which can encourage the model to learn more robust decision boundaries.</li> <li>Cutout: Cutout is a form of data augmentation that randomly masks out square regions of the input images during training. This helps prevent overfitting by forcing the model to learn more robust features and by increasing the amount of training data.</li> <li>Mixup: Mixup is a form of data augmentation that involves linearly interpolating pairs of training examples and their corresponding labels. This creates new training examples and encourages the model to learn more generalizable features.</li> <li>Shake-Shake Regularization: Shake-Shake regularization is a form of regularization for residual networks that introduces stochastic depth into the network. This helps prevent overfitting by randomly dropping out entire residual blocks during training.</li> <li>Stochastic Depth: Stochastic Depth is a variant of the Shake-Shake regularization that drops out entire residual blocks with a certain probability. This technique helps prevent overfitting by randomly removing some parts of the network during training.</li> <li>Focal Loss: Focal loss is a variant of cross-entropy loss that gives more weight to hard-to-classify examples. This can help prevent overfitting by reducing the impact of easy-to-classify examples on the training process.</li> </ol>"},{"location":"ml_cpu/#all-parameters-that-affects-the-model","title":"All parameters that affects the model","text":"<p>The learning rate and dropout rate and two of the most important hyperparameters that affect the model's learning. -   Learning Rate: The learning rate determines the step size of the optimization algorithm during training. A high learning rate can lead to unstable training, while a low learning rate can lead to slow convergence.</p> <ul> <li> <p>Batch Size: The batch size determines the number of samples that are used to compute each update during training. A larger batch size can lead to faster training, but it can also require more memory and may result in lower generalization performance.</p> </li> <li> <p>Dropout Rate: Dropout is a regularization technique that randomly drops out some neurons during training to prevent overfitting. The dropout rate determines the fraction of neurons that are dropped out during training.</p> </li> <li> <p>Weight Initialization: The initial values of the weights in the neural network can affect the model's ability to learn and generalize to new data. Common weight initialization techniques include random initialization and Xavier initialization.</p> </li> <li> <p>Optimizer: The optimizer is the algorithm used to update the weights of the neural network during training. Common optimizers include stochastic gradient descent (SGD), Adam, and RMSprop. The choice of optimizer can affect the speed and stability of training, as well as the generalization performance of the model.</p> </li> <li> <p>Number of Hidden Layers: This parameter determines the depth of the neural network, and it can greatly affect the complexity and capacity of the model. Adding more layers can allow the model to learn more complex features, but it can also increase the risk of overfitting.</p> </li> <li> <p>Number of Neurons per Layer: This parameter determines the width of the neural network, and it can also affect the model's capacity. Increasing the number of neurons per layer can increase the model's ability to capture complex relationships in the data, but it can also increase the risk of overfitting.</p> </li> <li> <p>Activation Function: The activation function is applied to each neuron in the neural network, and it determines the output of the neuron. Common activation functions include sigmoid, ReLU, and tanh. The choice of activation function can affect the model's ability to learn and generalize to new data.</p> </li> </ul>"},{"location":"ml_cpu/#full-model-structure","title":"Full model structure","text":"<p>In case if you are curious of how the structure of the full model looks like </p>"},{"location":"phylogenetic/","title":"Phylogentics","text":"<p>edit</p>"},{"location":"phylogenetics/","title":"Pylogenetics","text":"<p>The study of evolutionary relationships among biological entities</p>"},{"location":"phylogenetics/#objectives","title":"Objectives","text":"<p>By the end of this practical you should:</p> <ul> <li>Understand the relationship between genetic relatedness and transmission events.</li> <li>Know how to reconstruct and interpret a phylogenetic tree</li> <li>Understand how time and spatial diffusion of an epidemic is inferred from a phylogeny</li> </ul>"},{"location":"phylogenetics/#introduction","title":"Introduction","text":"<p>In the lecture, we saw how epidemiological processes leave a measurable imprint on pathogen genomes sampled from infected individuals. These processes can be recovered from genetic data sampled at different times and places, using statistical inference methods that take into account the sequences\u2019 shared ancestry.</p> <p>In this practical session, we will reconstruct and interpret the Influenza A H1N1 2009 (H1N1/09) epidemic in England, based on a set of viral sequences isolated during the outbreak.</p> <p>We will identify transmission chains of H1N1/09, reconstruct their migration patterns, estimate the number of independent introductions in England, and infer the time and geographical location of these introduction events. In order to do so, we will apply the following procedure:</p> <ol> <li>We will reconstruct the phylogeny of influenza genomes sampled in England and other countries where H1N1 infections were diagnosed.</li> <li>We will apply a molecular clock model to the data in order to fit the phylogeny to real time scales.</li> <li>We will infer the migration patterns of H1N1 into and within England using Bayesian Markov chain Monte Carlo (MCMC) phylogeographic inference.</li> </ol>"},{"location":"phylogenetics/#section-1-phylogenetic-reconstruction","title":"Section 1 - Phylogenetic reconstruction","text":"<p>We will use the packages AliView and RAxML to view genetic sequence files and build a maximum-likelihood (ML) phylogenetic tree.</p> <p>The file \u2018H1N1.flu.2009.fas\u2019 is a multi-FASTA file containing 50 full-length influenza A H1N1/09 genomes. These nucleotide sequences correspond to influenza viruses sampled in Canada (n = 5), China (n = 3), England (n = 23), Mexico (n = 4), Peru (n = 1), and the United States of America (USA; n = 14) between April and June 2009. The sequences are 12,735 nucleotides long.</p> <p>We will use AliView to manually inspect the FASTA file:</p> <p>Activate the correct environment with <code>conda activate phylogenetics</code>. Open AliView by running the commmand <code>aliview</code>. Import the sequence file: File &gt; Open File, navigate to ~/data/phylogenetics and select the file H1N1.flu.2009.fas.</p> <p>Important</p> <p>To infer the phylogenetic relationship of a set of sequences, these need to be \u2018aligned\u2019. That is, nucleotide positions need to be arranged in columns to ensure that we compare homologous positions to one another. In evolutionary biology, homology means similarity due to descent from a common ancestor. Two genes are homologous if they descend from an ancestral gene. Likewise, two nucleotides in different sequences are homologous if they correspond to the same nucleotide position in the ancestral gene. Note that two objects can be \u2018similar\u2019 without being \u2018identical\u2019. The sequences shown here have been aligned but if your data are not, it will need to be aligned. This can be done in AliView, as well as in other programs including MUSCLE, ClustalW and MAFFT.</p> <p>The sequence alignment is displayed as a matrix, where rows correspond to viral samples and columns to nucleotide positions (see Figure 1). Cells are coloured by type of nucleotides (A, C, G or T) and missing information, or gaps, are indicated by a dash (\u201c-\u201c).</p> <p></p> <p>The unique identifier of the sequences is shown at the left hand side of the nucleotide matrix and contains the strain name (e.g. A/England/247/2009) followed by the date of sampling (e.g. d2009-05-31) in the format dYYYY-MM-DD.</p> <p>In the matrix, we can spot \u2018rare\u2019 nucleotide substitutions present in one or more sequences (scroll along the sequence alignment to spot some). These single nucleotide polymorphisms (SNPs) allow us to identify viruses that are genetically related (they share a common SNP) and infer epidemiological linkage between them. If viruses sampled from different individuals have the same SNPs, we can assume that they form a specific strain infecting these individuals. This is the property we use to reconstruct transmission chains from a phylogenetic tree.</p>"},{"location":"phylogenetics/#tree-reconstruction","title":"Tree reconstruction","text":"<p>RAxML [1] is a freely available software used to reconstruct phylogenetic relationships between individuals using a maximum-likelihood (ML) approach. This approach takes into account a substitution model to assess the probability of particular mutations. You can also perform bootstrapping, a method for validating the tree by repeating the analysis a specified number of times to produce pseudoreplicates. This acts as a support for the tree topology of the final tree that is produced by calculating the number of pseudoreplicates in which a given node of a tree is found.</p> <p>Open your command-line terminal and navigate to the folder containing the H1N1.flu.2009.fas sequence file. <pre><code>cd ~/data/phylogenetics/`\n</code></pre> Run the following command (This may take a few minutes to complete): <pre><code>raxmlHPC -m GTRGAMMA -s H1N1.flu.2009.fas -n H1N1.flu.2009.ML -p 11334 -k -f a -x 13243 -N 100\n</code></pre></p> <ul> <li>raxmlHPC: This will run RAxML on the command-line and uses the following parameters:</li> <li>m GTRGAMMA: This sets the substitution model to be used. We are using GTR (generalised time reversible) with a gamma distribution.</li> <li>s H1N1.flu.2009.fas: This is your input file.</li> <li>n H1N1.flu.2009.ML: This is the name that will appended to output files.</li> <li>p 11334: This is a random seed number.</li> <li>k: Trees are printed with branch lengths.</li> <li>f a: This tells RAxML to conduct a rapid Bootstrap analysis and search for the best-scoring ML tree in a single run. -x 13243: This is a random seed number for bootstrapping. -N 100: This specifies the number of bootstrap runs. Once the software has finished running, the file RAxML_bipartitions.H1N1.flu.2009.ML will contain the ML tree with bootstrap supports. Add the extension .tre to the file:</li> </ul> <p><pre><code>mv RAxML_bipartitions.H1N1.flu.2009.ML RAxML_bipartitions.H1N1.flu.2009.ML.tre\n</code></pre> And open the tree to view in the program FigTree, for this run the command: <pre><code>figtree\n</code></pre> FigTree is a tree editor with a graphic interface and is freely available at http://tree.bio.ed.ac.uk/software/figtree. It runs on all operating systems.</p> <p>When prompted at opening, name the bootstrap values \u201cbootstrap support\u201d. You can now view the ML tree with bootstrap support values. Click on the node labels section on the left of the viewer and select bootstrap support from the Display drop down box. This will display the bootstrap support for each of the nodes of the tree. Go to Trees &gt; Increasing Node Order to display the tree by increasing node order.</p> <p>Note</p> <p>Note. Many of the values shown are low (the scale is 0 [no support] to 100 [full support]). Typically values over 70% (in this case a bootstrap value of 70) are taken as showing strong support. Low values here may suggest that you need to change the substitution model or run the analysis with more bootstrap runs. For the purposes of time for this exercise, we will continue with the trees produced here.</p>"},{"location":"phylogenetics/#tree-interpretation","title":"Tree Interpretation","text":"<p>Take a few minutes to familiarize yourself with the tree.</p> <p>Reminder</p> <p>The components of a phylogenetic tree are shown in Figure 2. The sequences, or taxa (plural for taxon), are positioned at the end of the external branches. Related sequences are linked by a node (their most recent common ancestor). Two or more sequences descending from a node form a \u2018clade\u2019 (or cluster). The length of a branch represents the genetic distance between two nodes or between a node and a taxon, i.e., the number of mutations accumulated since divergence. The root corresponds to the common ancestor of all the taxa.</p> <p></p> <p>Since the sequences represent viruses sampled from different individuals, a clade in the tree corresponds to a transmission chain. The number of sequences in a clade reflects the number of infections sampled from that transmission chain. The root of the tree corresponds to the origin of the epidemic.</p> <p>After looking at your tree with <code>figtree</code>, can you answer these questions:</p> <p>Question</p> Q1Answer <p>How many times was H1N1/2009 introduced in England during the outbreak?</p> <p>Q1. The viruses sampled in England form 3 distinct clades, suggesting 3 introduction events (colored in red in the figure below). The first one involves 18 cases (top of the tree), the second 2 cases (A/England/247/2009 and A/England/251/2009), and the third one 3 cases (A/England/247/2009, A/England/312/2009, and A/England/415/2009).</p> <p></p> <p>Question</p> Q2Answer <p>What is/are the most likely geographical origins of the English strain(s)? (Take the sampling dates into account in your reasoning)</p> <p>Q2. Cluster 1 is closest to a Canadian clade made of 4 cases. The two clades share a recent common ancestor located at node 1. All Canadian isolates were sampled in April 2009, while the viruses in the English Cluster 1 were sampled in May or June the same year. We can, therefore, assume that the strain migrated from the Americas to Europe (as we know it did\u2026). We can also infer precedence of the Canadian outbreak by the fact that the Canadian sequences are genetically more similar to the common ancestor (node 1) than the English ones, as indicated by shorter branch lengths.</p> <p>Cluster 2 is closest to a Canadian isolate (A/Toronto/3141/2009). Their common ancestor is located at node 2. The Toronto isolate predates the English ones, suggesting an introduction from Canada.</p> <p>Cluster 3 is closest to a US isolate (A/New York/3532/2009). Their common ancestor is located at node 3. Again, we can conclude from the time of sampling and branch lengths that Clade 3 was imported from the US.</p>"},{"location":"phylogenetics/#dating-the-introductions-of-h1n109-in-england","title":"Dating the Introduction(s) of H1N1/09 in England","text":"<p>We will now estimate the time frame of these migration events. This time frame can be inferred from the rate at which mutations are accumulated in gene sequences. If, for instance, two sequences differ by 4 mutations and their rate of evolution is known to be around one substitution per nucleotide position per year, then 2 years have elapsed since they diverged from their common ancestor (i.e., 1 substitution x 2 years x 2 sequences = 4 mutations).</p> <p>The first step of this procedure involves testing whether the genomes in the tree have evolved at a constant rate over time or not. This is called testing the molecular clock hypothesis.</p> <p>If the molecular clock is constant or strict, the genetic distance between two sequences will be proportional to the time since these sequences last shared a common ancestor (as in the example given above). If the molecular clock is not constant, the correlation between genetic distance and time since divergence is weaker. The molecular clock is then said to be relaxed. Assuming a strict or relaxed molecular clock will have an impact on the dating of phylogenetic nodes. We will, therefore, test the molecular clock hypothesis before dating the tree.</p>"},{"location":"phylogenetics/#molecular-clock-testing","title":"Molecular Clock Testing","text":"<p>Open the software TempEst by running the following command:</p> <pre><code>tempest\n</code></pre> <p>TempEst [2] is a tool designed to investigate the \u2018temporal signal\u2019 of molecular phylogenies. It tests whether there is sufficient genetic change between sampling times to reconstruct a statistical relationship between genetic divergence and time, which is the essence of a molecular clock in your data. You can download the software for free from http://tree.bio.ed.ac.uk/software/tempest.</p> <p>How does it work? TempEst performs \u2018root-to-tip\u2019 linear regressions, which can be used as a simple diagnostic tool for molecular clock models. It implies plotting the genetic divergence of the sequences (i.e., the sum of the branch lengths from a sequence - the tip - to the root of the tree) against the sampling time of the sequences (Figure 3). A linear trend with few residuals indicates that evolution follows a strict molecular clock. The same trend with greater scatter from the regression line suggests that a relaxed molecular clock model may be most appropriate. No trend at all indicates that the data contains little temporal signal and is unsuitable for inference using phylogenetic molecular clock models.</p> <p></p> <p>Import the ML tree you saved earlier or, if you can\u2019t find it anymore, select the file named RAxML_bipartitions.H1N1.flu.2009.ML.tre in your folder. If you are not prompted for the tree when <code>TempEst</code> opens, import it using the drop-down menu: <code>File</code> &gt; <code>Open\u2026</code></p> <p>We now have to tag the sequences with their sampling date. A list of all the sequences in the tree will appear (in the default <code>Sample Dates</code> tab). In order to plot root-to-tip genetic distances against sampling time, each sequence has to be associated with its date of sampling. Sampling dates are indicated at the end of the sequences\u2019 name, preceded by the suffix \u2018_d\u2019, in the format YYYY-MM-DD. For instance, the sequence named \u2018A/Lima/WRAIR1687P/2009_d2009-06-27\u2019 was sampled on the 27th of June 2009.</p> <p>In the Sample Date tab, click on <code>Parse Dates</code>. In the <code>Parse Dates for Taxa</code> window, select the following options: - The date is given by a numerical field in the taxon label that is defined by a prefix and its order (Order: Last; Prefix: _d) - Parse as a calendar date (Date format: yyy-MM-dd)</p> <p>Click <code>OK</code>, and the time of sampling will appear in the Date column.</p> <p>Go to the <code>Root-to-tip</code> tab to see the root-to-tip linear regression plot. Check the box marked Best-fitting root. Summary statistics of the plot are shown in the left-hand side window:</p> <ul> <li>Date range: the maximum time interval between two sampled sequences.</li> <li>Slope: The slope of the regression line; corresponds to the rate of evolution, here the average number of nucleotide substitutions per unit of time (here, per year).</li> <li>X-intercept: the time at which the viral population had no genetic diversity, which corresponds to the time of the most recent common ancestor of the sampled population (tMRCA).</li> <li>Correlation Coefficient: A measure of the relationship between time and the number of accumulated mutations (range: -1, 1). A positive/negative value implies a positive/negative linear relationship between time and diversity (i.e., as time increases, so does genetic divergence). A value close to 0 suggests no relationship between time and genetic divergence.</li> <li>R squared: A measure of how close the data is to the regression line, i.e., what proportion of the variation in genetic divergence is explained by a strict molecular clock hypothesis.</li> </ul> <p>Question</p> Q3Answer <p>Can we assume that the rate of evolution of the viruses in the phylogeny is constant over time (i.e., that the molecular clock is strict)?</p> <p>The root-to-tip linear regression plot shows a positive correlation between time and divergence (correlation coefficient = 0.71; see the figure above). This suggests that the phylogeny has sufficient temporal signal to conduct a molecular clock analysis on this dataset.</p> <p>However, there is a certain degree of scatter from the regression line, and only 51% of the variation in genetic divergence is explained by a strict molecular clock hypothesis (R squared = 0.51). We will, therefore, assume significant variation from a strict molecular clock in this dataset and opt for a relaxed molecular clock (i.e., the rate of evolution varies over time and across lineages). This is despite a relatively short sampling interval (about 3 months) and moderate genetic diversity in the sampled population.</p> <p></p>"},{"location":"phylogenetics/#dating-migration-events","title":"Dating Migration Events","text":"<p>Now that the properties of the molecular clock in our dataset have been established, we will estimate the time(s) at which H1N1/09 was introduced in England.</p> <p>Reminder</p> <p>In a \u2018classic\u2019 phylogenetic tree, branch lengths reflect as the number of nucleotide substitutions per site. Rates of evolution are expressed as the number of substitutions per site per unit of time. By dividing the length of a branch by the rate of evolution, we end up with a tree where branch lengths represent time units. A branch therefore represents the time elapsed between two nodes. Or, in our case, the time elapsed between two transmission events.</p> <p>The reconstruction of dated phylogenetic trees is computationally intense and could not be achieved within the time imparted for this practical. A dated H1N1/09 phylogeny was therefore built prior to the session, under the appropriate molecular clock model, using the Bayesian MCMC approach implemented in the software BEAST v.1.8 [3]. The resulting dated tree is in your folder under the name H1N1.flu.2009.mol_clock.tre. A more detailed example of how to set up a BEAST v1.8. analysis, including producing the input files and analysing the output is shown in the Appendix.</p> <p>How does it work? Each tip of the tree has a known time, given by the sampling date of the sequence. Internal nodes are given arbitrary starting times consistent with their order in the tree (from the tips to the root). An additional parameter, the evolution rate, is used to scale these times into expected number of nucleotide substitutions per site. Markov chain Monte Carlo integration is then used to summarize the probability density function of a model tested against the data, providing a representative sample of parameter values of the chosen model. The model includes the tree topology, the times of internal nodes and the evolution rate.</p> <p>We will use the program FigTree to display the dated tree and its annotations.</p> <p>Open FigTree by running the command: <pre><code>figtree\n</code></pre></p> <p>Import the dated tree: <code>File</code>&gt; <code>Open\u2026</code> &gt; H1N1.flu.2009.mol_clock.2.tre</p> <p>A phylogeny will appear. Again, go to <code>Tree</code> &gt; <code>Increasing Node Order</code> to display the tree in the same way as the one you generated with RAxML. This will ease comparison. Note that the major clusters do not change, just the order in which the clusters are organised from top to bottom. The dated tree should be very similar to the ML tree you reconstructed in Session 1. However, in this tree, the branch lengths represent days rather than genetic distances. Notice the scale at the bottom of the tree.</p> <p>On the lefthand side toolbar, tick the <code>Node Labels</code> box. The age of the nodes will appear.</p> <p>The age of a node is expressed as the number of days prior to the most recent sampling date in the tree. Here, the most recent sample is A/Lima/WRAIR1687P/2009, sampled on the 27th of June 2009. If, for instance, a node age equals 21 days, it means that the date at this node is the 6th of June 2009.</p> <p>Question</p> Q4Answer <p>According to the molecular clock dating, what are the date(s) of introduction of H1N1/09 in England (rounded down)? Tip: If mental arithmetic is not your thing, you can use the 2009 calendar provided as Appendix to back-calculate the date of the nodes.</p> <p>The most recent viral sequence (A/Lima/WRAIR1687P/2009) was sampled on 27/06/2009. The ancestral node of Cluster 1 has an age of 63 days (rounded down) prior to the most recent sample. The estimated date of the first introduction of H1N1/09 in England is thus 25/04/2009.</p> <p>The ancestral node of Cluster 2 has an age of 57 days (rounded down) prior to the most recent sample. The estimated date of the second introduction of H1N1/09 in England is thus 01/05/2009.</p> <p>The ancestral node of Cluster 3 has an age of 40 days (rounded down) prior to the most recent sample. The estimated date of the third introduction of H1N1/09 in England is thus 18/05/2009.</p>"},{"location":"phylogenetics/#identifying-the-geographical-origins-of-the-h1n109-strains-imported-in-england","title":"Identifying the geographical origin(s) of the H1N1/09 strains imported in England","text":"<p>We will finally reconstruct the migration pathways of these H1N1/09 strains.</p> <p>Reminder</p> <p>When individuals are infected in one location and then move to another, or infect someone whilst travelling, this is apparent as a \u201cchange\u201d in the location ascribed to one branch of the tree. These changes in location along a phylogenetic tree can be inferred from the location values at the tips and the shape of the tree (see Figure 4). To do so, a model of location exchange process is fitted to the data and the most likely location of the viral strain positioned at the nodes of the tree, together with its probability, can be estimated using a MCMC sampling procedure similar to the one used in Section 2.</p> <p></p> <p>This approach was applied to the H1N1/09 phylogeny, using an asymmetric continuous-time Markov chain [5], as implemented in the program BEAST. The asymmetric model uses separate parameters for forward and reverse rates of movement between each pair of locations. The tree file we used for the molecular clock analysis (H1N1.flu.2009.mol_clock.2.tre) also contains the result of the inferred migration patterns.</p> <p>On the left hand side toolbar, pull down the <code>Node Labels</code> menu and under<code>Display</code>, select Location. The most likely location of the ancestral virus located at the nodes will appear.</p> <p>To<code>display</code> the posterior probability of the most likely node location, select Location.prob in the Display menu of the <code>Node Labels</code>.</p> <p>Question</p> Q5Answer <p>Which country is the most likely source of the H1N1/09 epidemic? What is the probability of that location?</p> <p>The most likely location state at the root of the tree is the US (posterior probability: 0.84).</p> <p>Question</p> Q6Answer <p>Where were the English strains of H1N1/09 imported from? How confident are we? How does your answer compare to that of Q4?**</p> <ul> <li>The location at the node from which Cluster 1 is branching off (node 1 in the figure shown with the answer to Q1) is the US (posterior probability: 0.55).</li> <li>The location at the node from which Cluster 2 is branching off (node 2 in the figure shown with the answer to Q1) is Canada (posterior probability: 0.48).</li> <li>The location state at the node from which Cluster 3 is branching off (node 3 in the figure shown with the answer to Q1) is the US (posterior probability: 0.87).</li> </ul> <p>The geographical origin of the founder viruses is consistent for Cluster 2 and 3. However, for Cluster 1, the tree topology suggests relatedness with a Canadian cluster while the phylogeographic reconstruction places the founder virus of that cluster in the US. The most likely explanation is that the English and Canadian viruses were both imported from the US.</p>"},{"location":"phylogenetics/#references","title":"References","text":"<ol> <li>A. Stamatakis: 'RAxML Version 8: A tool for Phylogenetic Analysis and Post-Analysis of Large Phylogenies'. Bioinformatics, 2014</li> <li>A. Rambaut, T. T. Lam, L. Max Carvalho, and O. G. Pybus, \u2018Exploring the temporal structure of heterochronous sequences using TempEst (formerly Path-O-Gen)\u2019, Virus Evol., vol. 2, no. 1, p. vew007, Jan. 2016.</li> <li>A. J. Drummond, M. A. Suchard, D. Xie, and A. Rambaut, \u2018Bayesian Phylogenetics with BEAUti and the BEAST 1.7\u2019, Mol. Biol. Evol., vol. 29, no. 8, pp. 1969\u20131973, Jan. 2012.</li> <li>N. R. Faria et al., \u2018The early spread and epidemic ignition of HIV-1 in human populations\u2019, Science, vol. 346, no. 6205, pp. 56\u201361, Mar. 2014.</li> <li>P. Lemey, A. Rambaut, A. J. Drummond, and M. A. Suchard, \u2018Bayesian phylogeography finds its roots\u2019, PLoS Comput. Biol., vol. 5, no. 9, p. e1000520, Sep. 2009.</li> </ol>"},{"location":"phylogenetics/#appendix","title":"Appendix","text":""},{"location":"phylogenetics/#phylogenetic-tree-building-with-beast","title":"Phylogenetic Tree Building with BEAST","text":"<p>(Note: This example is for initializing a BEAST v.1.8 analysis; BEAST2 has a different interface.)</p> <p>1. Creating a BEAST Input File Using BEAUti</p> <p>Launch BEAUti by typing <code>beauti</code> into the command line or opening it from the dashboard.</p> <p>1.1. Load the FASTA sequence file H1N1.flu.2009.fas by clicking the <code>+</code> button or going to <code>File</code> &gt; <code>Open\u2026</code>.</p> <p></p> <p>This first will give you information on the number of taxa (50), the number of sites (12735), and the data type (nucleotide).</p> <p>If you want to partition the data, i.e. if you have samples that you know are from different populations or lineages, you can go onto the \u2018Taxa\u2019 tab and set these partitions. This will allow you to specify whether these groups are monophyletic as well as divergence dates between groups. For this exercise, we will not assume any partitioning of samples.</p> <p>1.2. Attach a date to each sample.</p> <p>This can be done in the same way as was shown in TempEst: In the Tips tab, tick the box for Use tip dates, and then click on Guess Dates. In the Guess date values for all taxa window, select the following options:</p> <ul> <li>The date is given by a numerical field in the taxon label that is defined by a prefix and its order (Order: Last; Prefix: _d)</li> <li>Parse as a calendar date (Date format: yyyy-MM-dd) Alternatively, you can create a tab-delimited file (.txt) with two columns corresponding to the taxa names and sample dates, and use Import Dates.</li> </ul> <p></p> <p>1.3. Set the location for each sample</p> <p>To attach further traits to each sample, such as location, in the <code>Traits</code> tab, click on <code>Import Traits</code> and open a tab-delimited file (.txt) with two columns corresponding to the taxa names and location (or characteristic of choice).</p> <p>1.4. Set a site (substitution) model. </p> <p>This allows us to substitution and site heterogeneity models. The rate and likelihood of base substitution over the genome will be variable so we will set some parameters to describe the model of substitution. We will set the substitution model to GTR \u2013 Generalized Time Reversible model \u2013 as this is the most frequently used model and allows for variable base frequencies. It is also reversible (i.e. T-&gt;A same rate as A-&gt;T). We can also set the site heterogeneity model, i.e. the frequency of each site to a Gamma distribution with 4 categories.</p> <p>In the Sites tab, select GTR from the \u2018Substitution model\u2019 dropdown box. We will set the \u2018Site Heterogeneity model\u2019 as \u2018Gamma\u2019 with 4 categories.</p> <p></p> <p>1.5 Set a clock model. The \u2018Clocks\u2019 tab will allow us to set the molecular clock for mutations along each branch of the tree. If we set this to \u2018Strick clock\u2019 this specifies that the mutation rate along each branch will be invariable. As we have shown in the TempEst analysis that our data varies significantly from a strict molecular clock, we will select a relaxed clock to allow for rate variation across branches in our tree.</p> <p>In the Clocks tab, select \u2018Uncorrelated relaxed clock\u2019 from the \u2018Clock Type\u2019 dropdown box and keep the \u2018Relaxed Distribution\u2019 option as \u2018Lognormal\u2019.</p> <p></p> <p>1.6 Set a tree model.</p> <p>As the phylogenetic tree is built through a coalescent approach, backwards through time based on the genetic diversity at tips/samples, you can will need to set a model for the demographic history of the population that describes the effective population size through time. For example, \u2018Constant size\u2019 will state that the effective population size has remained constant through evolutionary time. For the purposes of this exercise, we will choose a constant population size.</p> <p>In the <code>Trees</code> tab, select \u2018Coalescent: Constant Size\u2019 from the \u2018Tree Prior\u2019 dropdown box.</p> <p> Note: You can also input a starting tree on which BEAST will base the topology of the Bayesian tree. This may be a neighbor-joining or parsimony tree and can speed up the analysis by approximating the topology as a prior. We will start with a random tree.</p> <p>The 'States', 'Priors\u2019 and 'Operators' tabs allow us to specify whether to reconstruct sequences, set specific priors for each parameter, and to specify which operators to include in the output log file. We will leave all these as default values.</p> <p>1.7 Define the length of the MCMC chain</p> <p>Finally, the \u2018MCMC\u2019 tab is where we will define the steps in the MCMC chain that is used to construct trees. Here we are interested in the length of the chain. This is the number of iterations (number of steps in the random walk) the program will go through to converge on the best tree topology. The higher this number, the more support there will be for the final tree and the more chance of convergence.</p> <p>Define the length of the chain by typing '10000000' in the 'Length of chain' box.</p> <p>The name of the output files can be specified here. This will determine the names of the files that will be created after the BEAST run, .log, .trees, and .ops. The output file from BEAUti will be a .xml file for BEAST input. We will call this H1N1.flu.2009, so type this in the \u2018File name stem\u2019 box, if it doesn\u2019t already appear there.</p> <p></p> <p>Then click on the <code>Generate BEAST file</code> button at the bottom right of the screen to generate the .xml file that will be used as the input for BEAST.</p> <ol> <li>Running the BEAST analysis BEAST can be run on the command line or through the desktop application. We will run BEAST on command line.</li> </ol> <p>To run BEAST, open up a terminal and make sure you are in the folder containing your .xml file from the BEAUti output and type: <pre><code>beast H1N1.flu.2009.xml\n</code></pre> This will create three files; .log, .trees, and .ops, with the \u201cH1N1.flu.2009.\u201d prefix.</p> <ol> <li>Evaluating your BEAST run Open the program TRACER by typing <code>tracer</code>. This will allow you to check the output from BEAST and determine how effectively the MCMC has mixed, i.e. how close it is to reaching a steady state (converging). This can be used to manually evaluate the performance of the BEAST run.</li> </ol> <p>Load in the \u2018H1N1.flu.2009.log\u2019 file to Tracer to see the output.</p> <p></p> <p></p> <p>Here we want to look at the ESS score. This is the Effective Sample Size, which is a measure of the number of effectively independent draws from the posterior distribution that the Markov chain is equivalent to. Low scores are highlighted in red, with scores &gt;200 considered sufficient to demonstrate good mixing.</p> <p>To increase ESS scores, you can run the analysis for longer (more MCMC iterations), or do multiple independent runs that can be combined with the program LogCombiner. Very low scores for many parameters may though suggest that the prior parameters are not a good fit and should be varied.</p> <p>In our data, we can see that most values are over 200, with only the prior and posterior values highlighted in red. Looking at the \u201cTrace\u201d of the MCMC though, we can see that it remaining reasonably steady around -19600 for the prior parameter, suggesting that this support will increase (ESS will increase) if the MCMC chain was longer.</p> <ol> <li>Producing a single phylogenetic tree</li> </ol> <p>The program TreeAnnotator summarises the sample of trees produced by BEAST (in the .tree file) onto a single maximum clade credibility tree. The posterior probabilities of the nodes in the target tree, the posterior estimates and HPD limits of the node heights and (in the case of a relaxed molecular clock model) the rates, are shown.</p> <p>Load in the \u2018H1N1.flu.2009.trees\u2019 file to TreeAnnotator through <code>Choose File\u2026</code> in <code>Input Tree File</code>. Select what to the call the output file, e.g. H1N1.flu.2009.tre</p> <p>You can also set the number of burnin states (The number of initial MCMC runs to discard).</p> <p>Then select <code>Run</code>to produce a single maximum clade credibility tree. </p>"},{"location":"phylogenetics/#calendar","title":"Calendar","text":""},{"location":"additional-info/SettingUpVirtualLinux/","title":"Virtual Box: Setting Up Linux","text":""},{"location":"additional-info/SettingUpVirtualLinux/#what-is-virtual-box","title":"What is Virtual Box","text":"<p>VirtualBox is a free tool that lets you use different operating systems on one computer. It works on Windows, macOS, Linux, and Solaris, and supports many types of operating systems inside it. It's easy to use, has useful features like saving your virtual machine's state, supports USB devices, and provides decent performance. VirtualBox is popular among users who need to run different operating systems without needing multiple physical computers, and it's supported by a helpful community.</p> <p> Virtual Box Website Link </p>"},{"location":"additional-info/SettingUpVirtualLinux/#step-by-step-setup","title":"Step-by-step Setup","text":""},{"location":"additional-info/SettingUpVirtualLinux/#step-1","title":"Step 1","text":"<p>Install Virtual Box from https://www.virtualbox.org/wiki/Downloads.      Remember to select the correct installation client depending on your      operating system (windows, macOS, etc). </p> <p></p>"},{"location":"additional-info/SettingUpVirtualLinux/#step-2","title":"Step 2","text":"<p>Download the latest version of Linux Ubuntu from https://ubuntu.com/download/desktop. This will be an ISO file (e.g. ubuntu-22.04.3-desktop-amd64.iso) which is approximately 5GB. </p> <p></p>"},{"location":"additional-info/SettingUpVirtualLinux/#step-3","title":"Step 3","text":"<p>Open Virtual Box on your computer and select to NEW to create new virtual machine.</p> <p></p>"},{"location":"additional-info/SettingUpVirtualLinux/#step-4","title":"Step 4","text":"<p>Name machine and select the ISO file (downloaded in step 2). Make sure to set  Type as Linux and select Version which matches you ISO file e.g. Ubuntu 22.04 LTS (Jammy Jellyfish)</p> <p></p>"},{"location":"additional-info/SettingUpVirtualLinux/#step-5","title":"Step 5","text":"<p>Set memory usage and storage space. Do not max this out! </p> <p></p>"},{"location":"additional-info/SettingUpVirtualLinux/#step-6","title":"Step 6","text":"<p>Select to create a Virtual Hard Disk</p> <p></p>"},{"location":"additional-info/SettingUpVirtualLinux/#step-7","title":"Step 7","text":"<p>Finish creating the instance and click START to launch our Virtual Machine.</p> <p></p>"},{"location":"additional-info/conda/","title":"Conda Environments: Why and How to Use Them","text":""},{"location":"additional-info/conda/#what-is-a-conda-environment","title":"What is a Conda Environment?","text":"<p>Conda is an open-source package manager and environment manager that allows users to install multiple versions of software packages and their dependencies in isolated environments. A conda environment is essentially a directory that contains a specific collection of conda packages.</p>"},{"location":"additional-info/conda/#why-use-conda-environments","title":"Why Use Conda Environments?","text":"<ol> <li>Isolation: Multiple projects may require different versions of libraries or even the Python interpreter itself. By using separate environments, you can ensure that each project has its own dependencies, isolated from each other.</li> <li>Avoid Conflicts: Dependencies of one project might conflict with another. Environments ensure that such conflicts are avoided by isolating dependencies.</li> <li>Reproducibility: Sharing a project with someone else (or even with your future self) becomes much simpler when you can provide a list of exact package versions used in your environment.</li> <li>Flexibility: You can easily switch between different versions of a package or Python itself, depending on the project's needs.</li> <li>Clean and Safe Experimentation: If you want to test out a new package or update an existing one, you can do so in a new environment. If something goes wrong, your main working environment remains unaffected.</li> </ol>"},{"location":"additional-info/conda/#how-does-it-work","title":"How Does It Work?","text":"<p>Conda environments work by creating isolated spaces, each with its own installation directories. This allows you to have different versions of a package (or even Python itself) in different environments. When you activate an environment, conda adjusts your <code>PATH</code> so that the selected environment's executables are used.</p>"},{"location":"additional-info/conda/#some-basic-conda-commands","title":"Some Basic Conda Commands:","text":"<ol> <li>Creating a New Environment: <code>conda create --name myenv python=3.7</code></li> <li>Activating an Environment:  </li> <li>On macOS and Linux: <code>source activate myenv</code> or <code>conda activate myenv</code></li> <li>Deactivating an Environment: <code>conda deactivate</code></li> <li>Listing All Environments: <code>conda env list</code> or <code>conda info --envs</code></li> <li>Installing a Package in an Active Environment: <code>conda install numpy</code></li> <li>Exporting an Environment to a YAML File: <code>conda env export &gt; environment.yml</code></li> <li>Creating an Environment from a YAML File: <code>conda env create -f environment.yml</code></li> <li>Removing an Environment: <code>conda env remove --name myenv</code></li> </ol>"},{"location":"additional-info/conda/#environments-for-the-course","title":"Environments for the course","text":"<p>Click here for links to environment file used for the practicals.</p>"},{"location":"additional-info/conda/#conclusion","title":"Conclusion","text":"<p>Conda environments are essential tools for data scientists and developers to manage dependencies and ensure project reproducibility. By understanding and utilizing them, you can maintain a more organized, conflict-free, and efficient development workflow.</p>"},{"location":"additional-info/faq/","title":"Frequently asked questions","text":""},{"location":"additional-info/faq/#i-dont-see-my-files-in-the-terminal-what-should-i-do","title":"I don't see my files in the terminal. What should I do?","text":"<p>There are a few things you can do to troubleshoot. </p> <ol> <li>Check that you are in the correct directory. You can do this by running <code>pwd</code> in the terminal. If you are not in the correct directory, you can change to the correct directory by running <code>cd /path/to/directory</code>. </li> <li>Did your command run correctly? If you are running a command that should produce output, make sure that the command ran correctly. If it did not, you may need to troubleshoot the command. You might see an error message at the end of the output that can help you identify the problem. For example if you see \"command not found\" you may need to activate the right conda environment.</li> <li>You can try to find the file using the <code>find</code> command. For example, if you are looking for a file called <code>sample1.bam</code> you can run <code>find ~/data/ -name sample1.bam</code>. This will search the entire filesystem for the file. If the file is found, the path to the file will be printed to the terminal.</li> </ol>"},{"location":"additional-info/software/","title":"Software","text":"<p>Here is a list of software that is used in the practicals</p>"},{"location":"additional-info/software/#mapping","title":"Mapping","text":"Software Notes bwa Used for aligning short read data (e.g. Illumina) minimap2 Used to align long read data (e.g. ONT)"},{"location":"additional-info/software/#variant-detection","title":"Variant detection","text":"Software Notes bcftools gatk Doesn't work on long read data freebayes"},{"location":"additional-info/software/#assembly","title":"Assembly","text":"Software Notes spades Only works on short read data flye Assembly for long read data"},{"location":"advanced/gwas/","title":"Genome Wide Association Studies (GWAS)","text":""},{"location":"advanced/gwas/#introduction","title":"Introduction","text":"<p>The objective of this tutorial is to get you familiar with the basic file format used for GWAS and common tools used for analysis and take you through data quality control (Crucial in any study!). </p> <p>Our dataset is based on a GWAS study for Meningococcal disease in a European population.</p> <p>The data required is genomic wide SNP data. It is already included in this practical. The scripts required to facilitate analysis are also included just like the other practicals.</p>"},{"location":"advanced/gwas/#software-you-will-need-for-analysis","title":"Software you will need for analysis:","text":"<ul> <li> <p>Computer workstation with Unix/Linux operating system (You will be using this already)</p> </li> <li> <p>PLINK software for genome-wide association analysis</p> </li> <li> <p>R (Statistical software for data analysis and graphing)</p> </li> <li> <p>Data analysis and graphing can be performed in a number of ways, and \"R\" is a example of a software that can be used for this. </p> </li> <li> <p>BCFtools</p> </li> </ul> <p>The data for this practical is in the <code>~/data/gwas</code> directory. Please navigate to this directory before running any commands. </p> <p>Before continuing also make sure to have loaded the \"gwas\" environment using the conda command.</p>"},{"location":"advanced/gwas/#1-create-bed-files-for-analysis","title":"1. Create BED files for analysis","text":"<p>Convert your plink genotype files to binary format - smaller file easier for manipulation of data. </p> <p>We have already provided you with plink formatted files we won't have to do this step.</p> <p>Info</p> <p>Plink allows for the conversion from many different formats to plink format. For example if you had a VCF file you could type: <pre><code>plink --vcf MD.vcf.gz --make-bed --out MD \n</code></pre></p> <p>Your data set:</p> <p>Plink binary formatted dataset consisting of 3004 individuals, 409 cases, 2595 controls, and 601089 variants.</p> <ul> <li>MD.bed \u2013 binary-coded information on individuals and variants</li> <li>MD.bim \u2013 variant information: \u201cChromosome\u201d, \u201cMarker name\u201d, \u201cGenetic Distance\u201d (or '0' as dummy variable), \u201cBase-pair coordinate\u201d, \u201cAllele 1\u201d, \u201cAllele 2\u201d. Each SNP must have two alleles.</li> <li>MD.fam \u2013 Individual Information. The first 6 columns are mandatory and in the order: \u201cFamily ID\u201d, \u201cIndividual ID\u201d, \u201cPaternal ID\u201d, \u201cMaternal ID\u201d, \u201cSex\u201d, \u201cPhenotype\u201d.</li> </ul> <p>Double-check the basic stats of your dataset (number of variants, individuals, controls, cases) by examining MD.bim and MD.fam with bash utilities like <code>awk</code> or <code>wc</code>. </p> <p>What other information is available?</p>"},{"location":"advanced/gwas/#2-sample-qc","title":"2. Sample QC","text":""},{"location":"advanced/gwas/#identification-of-individuals-with-discordant-sex-information","title":"Identification of Individuals with discordant sex information","text":"<p>Ideally, if X-chromosome data are available, we would calculate the mean homozygosity rate across X chromosome markers for each individual in the study and identify discordance with our reported Sex phenotype.</p> <p>As our data only contains autosomes we will skip this step.</p>"},{"location":"advanced/gwas/#identification-of-individuals-with-elevated-missing-data-rates-or-outlying-heterozygosity-rate","title":"Identification of individuals with elevated missing data rates or outlying heterozygosity rate","text":"<p>At the shell prompt type:  <pre><code>plink --bfile MD --missing --out MD\n</code></pre></p> <p>This creates the files MD.imiss (sample-based missing report) and MD.lmiss (variant-based missing report). The fourth column in the imiss file (N_MISS) gives the number of missing SNPs and the sixth column (F_MISS) gives the proportion of missing SNPs per individual. </p> <p>At the shell prompt type:</p> <pre><code>plink --bfile MD --het --out MD\n</code></pre> <p>This creates the file MD.het where the third column gives the observed number of homozygous genotypes [O(Hom)] and the fifth column gives the number of non-missing genotypes [N(NM)], per individual. </p> <p>Calculate the observed heterozygosity rate per individual using the formula (N(NM) - O(Hom))/N(NM) and create a graph where the proportion of missing SNPs per individual is plotted on the x-axis and the observed heterozygosity rate per individual is plotted on the y-axis. Type: </p> <pre><code>R CMD BATCH imiss-vs-het.Rscript\n</code></pre> <p>This creates the graph MD.imiss-vs-het.pdf (see below). </p> <p></p> <p>Info</p> <p>Examine the plot to decide reasonable thresholds at which to exclude individuals based on elevated missing or extreme heterozygosity. </p> <p>Here we will exclude all individuals with a genotype failure rate \u2265 0.0185 (vertical dashed line) and/or heterozygosity rate \u00b1 3 standard deviations from the mean (horizontal dashed lines). </p> <p>Add the family ID and individual ID of all the failing this QC using:</p> <pre><code>R CMD BATCH imiss_het_fail.Rscript\n</code></pre> <p>This produces a file named fail_imiss_het_qc.txt </p> <p>Question</p> <p>How many samples failed this QC stage? </p>"},{"location":"advanced/gwas/#identification-of-duplicated-or-related-individuals","title":"Identification of duplicated or related individuals","text":"<p>To identify duplicate &amp; related individuals, create an Identity-by-State (IBS) matrix \u2013 calculated for each pair of individuals based on the shared proportion of alleles. </p> <p>To reduce the computational complexity, first prune the dataset so that no pair of SNPs (within a given window e.g 200kb) has linkage disequilibrium (r\u00b2 &gt; 0.2). Type </p> <pre><code>plink --bfile MD --indep-pairwise 200 5 0.5 --out MD\n</code></pre> <p>This creates files with the extension .prune.in .prune.out &amp; *.log. Then, to extract pruned SNPs and generate pair-wise IBS, type: </p> <pre><code>plink --bfile MD --extract MD.prune.in --genome --out MD \n</code></pre> <p>This might take a few minutes and creates files with the extension .genome &amp; .log </p> <p>Info</p> <p>You can also create a BED file with the pruned SNPs: <code>nohup plink --bfile MD --extract MD.prune.in --make-bed --out MD.pruned &amp;</code>. Note: <code>nohup</code> and <code>&amp;</code> are used to allow the command to run in the background thus freeing up the terminal for further use. You can use top to see if the process is still running (remember to press q to exit). Alternatively, you could of course simply run the command in another terminal. </p> <p>Question</p> <p>How many SNPs are in your pruned list? HINT: Have a look at your log file (using the <code>less</code> command) or type <code>wc -l MD.prune.in</code></p> <p>To identify all pairs of individuals with an Idenity-by-descent (IBD) &gt; 0.185. Type: </p> <pre><code>perl run-IBD-QC.pl MD\n</code></pre> <p>Info</p> <p>The code also looks at the individual call rates stored in MD.imiss and outputs the ID of the individual with the lower call rate to fail_IBD-QC.txt for each pair of related individuals. </p> <p>To visualise the IBD rates, type: </p> <pre><code>R CMD BATCH  plot-IBD.Rscript\n</code></pre> <p>This generates MD.IBD-hist.pdf </p> <p></p> <p>Info</p> <p>Note: The expectation is that IBD = 1 for duplicates or monozygotic twins, IBD = 0.5 for 1st-degree relatives, IBD = 0.25 for 2nd-degree relatives and IBD = 0.125 for 3rd-degree relatives. Due to variation around these values it is typical to exclude one individual from each pair with an IBD &gt; 0.185, which is halfway between the expected IBD for 3rd- and 2nd-degree relatives. </p>"},{"location":"advanced/gwas/#identification-of-individuals-of-divergent-ancestry","title":"Identification of individuals of divergent ancestry","text":"<p>In the interest of time, this step has been mostly done for you.</p> <p>Principal components analysis (PCA) is performed with pruned bed file datasets generated before using the ./RUN_PCA.sh command. This generates the following output files: MD.pruned.pca.par, MD.pruned.pca.log, MD.pruned.pca.evec, MD.pruned.pca, MD.pruned.eval. The evec extension file is what you will need to view your PCs</p> <p>Create a scatter diagram of the first two principal components, including all individuals in the file MD.pruned.pca.evec (the first and second PCs are columns 2 and 3 respectively). Type: </p> <pre><code>R CMD BATCH plot-pca-results.Rscript\n</code></pre> <p>This outputs pca_plot.pdf</p> <p></p> <p>Info</p> <p>Data in column 4 is used to colour the points according to phenotype (i.e. case vs control). Here, we chose to exclude all individuals with a 2nd principal component score &gt;0.07. </p> <p>To write the FID and IID of the individuals to a file called fail_pca.txt type:</p> <pre><code>R CMD BATCH write_pca_fail.R\n</code></pre> <p>Question</p> <p>How many individuals failed the PCA threshold? </p>"},{"location":"advanced/gwas/#remove-all-individuals-failing-qc","title":"Remove all individuals failing QC","text":"<p>To concatenate all the files listing individuals failing the previous QC steps into single file, at the unix prompt type: </p> <pre><code>cat fail*txt | sort -k1 | uniq &gt; fail_qc_inds.txt\n</code></pre> <p>The file fail_qc_inds.txt should now contain a list of unique individuals failing the previous QC steps. </p> <p>To remove these from the dataset type: </p> <pre><code>plink --bfile MD --remove fail_qc_inds.txt --make-bed --out clean.MD\n</code></pre> <p>Question</p> <p>How many individuals in total will be excluded from further analysis?</p> <p>How many individuals in total do you have for further analysis? HINT: check your log file</p>"},{"location":"advanced/gwas/#marker-qc","title":"Marker QC","text":""},{"location":"advanced/gwas/#identify-all-markers-with-an-excessive-missing-data-rate","title":"Identify all markers with an excessive missing data rate","text":"<p>To calculate the missing genotype rate for each marker type: </p> <pre><code>plink --bfile clean.MD --missing --out clean.MD\n</code></pre> <p>The results of this analysis can be found in clean.MD.lmiss. </p> <p>Plot a histogram of the missing genotype rate to identify a threshold for extreme genotype failure rate. This can be done using the data in column five of the clean.MD.lmiss file. Type: </p> <pre><code>R CMD BATCH lmiss-hist.Rscript\n</code></pre> <p>This generates clean.MD.lmiss.pdf </p> <p></p> <p>We chose to a call-rate threshold of 5% (these SNPs will be removed later in the protocol). </p>"},{"location":"advanced/gwas/#test-markers-for-different-genotype-call-rates-between-cases-and-contols","title":"Test markers for different genotype call rates between cases and contols","text":"<p>To test all markers for differences in call rate between cases and controls, at the Unix prompt type: </p> <pre><code>plink --bfile clean.MD --test-missing --allow-no-sex --out clean.MD\n</code></pre> <p>The output of this test can be found in clean.MD.missing. </p> <p>To create a file called \u2018fail-diffmiss-qc.txt\u2019, which contains all SNPs with a significantly different (P&lt;0.00001) missing rate between cases and controls, type: </p> <pre><code>perl run-diffmiss-qc.pl clean.MD\n</code></pre> <p>Question</p> <p>How many variants have failed QC? </p>"},{"location":"advanced/gwas/#remove-all-markers-failing-qc","title":"Remove all markers failing QC","text":"<p>To remove poor SNPs from further analysis and create a new clean (QC\u2019D) MD data file, at the Unix prompt type:</p> <pre><code>plink --bfile clean.MD --exclude fail-diffmiss-qc.txt --maf 0.01 --geno 0.05 --hwe 0.00001 --make-bed --out clean.final.MD\n</code></pre> <p>In addition to markers failing previous QC steps, those with a MAF &lt; 0.01, missing rate &gt; 0.05 and a HWE P-value &lt; 0.00001 (in controls) are also removed. </p> <p>Question</p> <p>How many variants and individuals pass filters and QC for your GWAS? </p>"},{"location":"advanced/gwas/#perform-a-gwas-on-your-qcd-dataset","title":"Perform a GWAS on your QC\u2019d dataset","text":"<p>To run a basic case/control association test, at the unix prompt type:</p> <pre><code>plink --bfile clean.final.MD --assoc --ci 0.95 --adjust --allow-no-sex --out final.MD.assoc\n</code></pre> <p>Your association output file will contain 12 columns: </p> <ul> <li>CHR - Chromosome</li> <li>SNP - SNP ID</li> <li>BP - Physical position (base-pair)</li> <li>A1 - Minor allele name (based on whole sample)</li> <li>F_A - Frequency of this allele in cases</li> <li>F_U - Frequency of this allele in controls</li> <li>A2 - Major allele name</li> <li>CHISQ - Basic allelic test chi-square (1df)</li> <li>P - Asymptotic p-value for this test</li> <li>OR - Estimated odds ratio (for A1, i.e. A2 is reference)</li> <li>L95 - Lower bound of 95% confidence interval for odds ratio</li> <li>U95 - Upper bound of 95% confidence interval for odds ratio</li> </ul> <p>To visualise your data:</p> <p>Generate a Quantile-Quantile (QQ) plot of your p-values to look at the distribution of P-values and assess whether genomic inflation is present (lambda&gt;1) (this can also be found in your assoc. log file).</p> <p>Generate a manhattan plot to visualise where your association signals lie across the chromosomes. Type: </p> <pre><code>R CMD BATCH GWAS_plots.R\n</code></pre> <p>This generates both plots: final.MD.assoc_qq.png and final.MD.assoc_mhplot.png </p> <p></p> <p>Question</p> <p>What do the plots tell you? </p> <p>Let\u2019s zoom into a region of interest: the tower of SNPs on CHR1 (coloured in yellow). This the Complement Factor H (CFH) region known to be associated with Meningococcal disease. The previous Rscript in 8.2.2 above also generated the chr1_CFH_region.txt file. </p> <ul> <li>Open the locuszoom webpage: http://locuszoom.org/genform.php?type=yourdata</li> <li>Upload the text file (final.MD.assoc.assoc.adjusted)</li> <li>Set The P-Value column name to be \"GC\"</li> <li>Change the Column Delimiter to \"Whitespace\"</li> <li>Set the Marker column name to be \"SNP\"</li> <li>In the region section, enter the most associated snp (\u201crs1065489\u201d) with a flanking size of 500KB</li> <li>In the Genome Build/LD Population field select the appropriate hg19 european ref panel.</li> <li>Then press \"Plot Data\" to generate your plot.</li> </ul>"},{"location":"advanced/phylogenetics/","title":"Phylogenetics","text":"<p>The study of evolutionary relationships among biological entities</p>"},{"location":"advanced/phylogenetics/#objectives","title":"Objectives","text":"<p>By the end of this practical you should:</p> <ul> <li>Understand the relationship between genetic relatedness and transmission events.</li> <li>Know how to reconstruct and interpret a phylogenetic tree</li> <li>Understand how time and spatial diffusion of an epidemic is inferred from a phylogeny</li> </ul>"},{"location":"advanced/phylogenetics/#introduction","title":"Introduction","text":"<p>In the lecture, we saw how epidemiological processes leave a measurable imprint on pathogen genomes sampled from infected individuals. These processes can be recovered from genetic data sampled at different times and places, using statistical inference methods that take into account the sequences\u2019 shared ancestry.</p> <p>In this practical session, we will reconstruct and interpret the Influenza A H1N1 2009 (H1N1/09) epidemic in England, based on a set of viral sequences isolated during the outbreak.</p> <p>We will identify transmission chains of H1N1/09, reconstruct their migration patterns, estimate the number of independent introductions in England, and infer the time and geographical location of these introduction events. In order to do so, we will apply the following procedure:</p> <ol> <li>We will reconstruct the phylogeny of influenza genomes sampled in England and other countries where H1N1 infections were diagnosed.</li> <li>We will apply a molecular clock model to the data in order to fit the phylogeny to real time scales.</li> <li>We will infer the migration patterns of H1N1 into and within England using Bayesian Markov chain Monte Carlo (MCMC) phylogeographic inference.</li> </ol>"},{"location":"advanced/phylogenetics/#section-1-phylogenetic-reconstruction","title":"Section 1 - Phylogenetic reconstruction","text":"<p>We will use the packages AliView and RAxML to view genetic sequence files and build a maximum-likelihood (ML) phylogenetic tree.</p> <p>The file \u2018H1N1.flu.2009.fas\u2019 is a multi-FASTA file containing 50 full-length influenza A H1N1/09 genomes. These nucleotide sequences correspond to influenza viruses sampled in Canada (n = 5), China (n = 3), England (n = 23), Mexico (n = 4), Peru (n = 1), and the United States of America (USA; n = 14) between April and June 2009. The sequences are 12,735 nucleotides long.</p> <p>We will use AliView to manually inspect the FASTA file:</p> <p>Activate the correct environment with <code>conda activate phylo</code>. Open AliView by running the commmand <code>aliview</code>. Import the sequence file: File &gt; Open File, navigate to ~/data/phylogenetics and select the file H1N1.flu.2009.fas.</p> <p>Important</p> <p>To infer the phylogenetic relationship of a set of sequences, these need to be \u2018aligned\u2019. That is, nucleotide positions need to be arranged in columns to ensure that we compare homologous positions to one another. In evolutionary biology, homology means similarity due to descent from a common ancestor. Two genes are homologous if they descend from an ancestral gene. Likewise, two nucleotides in different sequences are homologous if they correspond to the same nucleotide position in the ancestral gene. Note that two objects can be \u2018similar\u2019 without being \u2018identical\u2019. The sequences shown here have been aligned but if your data are not, it will need to be aligned. This can be done in AliView, as well as in other programs including MUSCLE, ClustalW and MAFFT.</p> <p>The sequence alignment is displayed as a matrix, where rows correspond to viral samples and columns to nucleotide positions (see Figure 1). Cells are coloured by type of nucleotides (A, C, G or T) and missing information, or gaps, are indicated by a dash (\u201c-\u201c).</p> <p></p> <p>The unique identifier of the sequences is shown at the left hand side of the nucleotide matrix and contains the strain name (e.g. A/England/247/2009) followed by the date of sampling (e.g. d2009-05-31) in the format dYYYY-MM-DD.</p> <p>In the matrix, we can spot \u2018rare\u2019 nucleotide substitutions present in one or more sequences (scroll along the sequence alignment to spot some). These single nucleotide polymorphisms (SNPs) allow us to identify viruses that are genetically related (they share a common SNP) and infer epidemiological linkage between them. If viruses sampled from different individuals have the same SNPs, we can assume that they form a specific strain infecting these individuals. This is the property we use to reconstruct transmission chains from a phylogenetic tree.</p>"},{"location":"advanced/phylogenetics/#tree-reconstruction","title":"Tree reconstruction","text":"<p>RAxML [1] is a freely available software used to reconstruct phylogenetic relationships between individuals using a maximum-likelihood (ML) approach. This approach takes into account a substitution model to assess the probability of particular mutations. You can also perform bootstrapping, a method for validating the tree by repeating the analysis a specified number of times to produce pseudoreplicates. This acts as a support for the tree topology of the final tree that is produced by calculating the number of pseudoreplicates in which a given node of a tree is found.</p> <p>Open your command-line terminal and navigate to the folder containing the H1N1.flu.2009.fas sequence file. <pre><code>cd ~/data/phylogenetics/\n</code></pre> Run the following command (This may take a few minutes to complete): <pre><code>iqtree -m GTR+G -s H1N1.flu.2009.fas -bb 1000 \n</code></pre></p> <ul> <li>iqtree: This will run iqtree on the command-line and uses the following parameters:</li> <li>m GTR+G: This sets the substitution model to be used. We are using GTR (generalised time reversible) with a gamma distribution.</li> <li>s H1N1.flu.2009.fas: This is your input file. --bb 1000: This specifies the number of bootstrap runs.</li> </ul> <p>Once the software has finished running, the file <code>H1N1.flu.2009.fas.treefile</code> will contain the ML tree with bootstrap supports. Add the extension .tre to the file:</p> <p>And open the tree to view in the program FigTree, for this run the command: <pre><code>figtree\n</code></pre> FigTree is a tree editor with a graphic interface and is freely available at http://tree.bio.ed.ac.uk/software/figtree. It runs on all operating systems.</p> <p>When prompted at opening, name the bootstrap values \u201cbootstrap support\u201d. You can now view the ML tree with bootstrap support values. Click on the node labels section on the left of the viewer and select bootstrap support from the Display drop down box. This will display the bootstrap support for each of the nodes of the tree. Go to Trees &gt; Increasing Node Order to display the tree by increasing node order.</p> <p>Note</p> <p>Note. Many of the values shown are low (the scale is 0 [no support] to 100 [full support]). Typically values over 70% (in this case a bootstrap value of 70) are taken as showing strong support. Low values here may suggest that you need to change the substitution model or run the analysis with more bootstrap runs. For the purposes of time for this exercise, we will continue with the trees produced here.</p>"},{"location":"advanced/phylogenetics/#tree-interpretation","title":"Tree Interpretation","text":"<p>Take a few minutes to familiarize yourself with the tree.</p> <p>Reminder</p> <p>The components of a phylogenetic tree are shown in Figure 2. The sequences, or taxa (plural for taxon), are positioned at the end of the external branches. Related sequences are linked by a node (their most recent common ancestor). Two or more sequences descending from a node form a \u2018clade\u2019 (or cluster). The length of a branch represents the genetic distance between two nodes or between a node and a taxon, i.e., the number of mutations accumulated since divergence. The root corresponds to the common ancestor of all the taxa.</p> <p></p> <p>Since the sequences represent viruses sampled from different individuals, a clade in the tree corresponds to a transmission chain. The number of sequences in a clade reflects the number of infections sampled from that transmission chain. The root of the tree corresponds to the origin of the epidemic.</p> <p>After looking at your tree with <code>figtree</code>, can you answer these questions:</p> <p>Question</p> Q1Answer <p>How many times was H1N1/2009 introduced in England during the outbreak?</p> <p>Q1. The viruses sampled in England form 3 distinct clades, suggesting 3 introduction events (colored in red in the figure below). The first one involves 18 cases (top of the tree), the second 2 cases (A/England/247/2009 and A/England/251/2009), and the third one 3 cases (A/England/247/2009, A/England/312/2009, and A/England/415/2009).</p> <p></p> <p>Question</p> Q2Answer <p>What is/are the most likely geographical origins of the English strain(s)? (Take the sampling dates into account in your reasoning)</p> <p>Q2. Cluster 1 is closest to a Canadian clade made of 4 cases. The two clades share a recent common ancestor located at node 1. All Canadian isolates were sampled in April 2009, while the viruses in the English Cluster 1 were sampled in May or June the same year. We can, therefore, assume that the strain migrated from the Americas to Europe (as we know it did\u2026). We can also infer precedence of the Canadian outbreak by the fact that the Canadian sequences are genetically more similar to the common ancestor (node 1) than the English ones, as indicated by shorter branch lengths.</p> <p>Cluster 2 is closest to a Canadian isolate (A/Toronto/3141/2009). Their common ancestor is located at node 2. The Toronto isolate predates the English ones, suggesting an introduction from Canada.</p> <p>Cluster 3 is closest to a US isolate (A/New York/3532/2009). Their common ancestor is located at node 3. Again, we can conclude from the time of sampling and branch lengths that Clade 3 was imported from the US.</p>"},{"location":"advanced/phylogenetics/#dating-the-introductions-of-h1n109-in-england","title":"Dating the Introduction(s) of H1N1/09 in England","text":"<p>We will now estimate the time frame of these migration events. This time frame can be inferred from the rate at which mutations are accumulated in gene sequences. If, for instance, two sequences differ by 4 mutations and their rate of evolution is known to be around one substitution per nucleotide position per year, then 2 years have elapsed since they diverged from their common ancestor (i.e., 1 substitution x 2 years x 2 sequences = 4 mutations).</p> <p>The first step of this procedure involves testing whether the genomes in the tree have evolved at a constant rate over time or not. This is called testing the molecular clock hypothesis.</p> <p>If the molecular clock is constant or strict, the genetic distance between two sequences will be proportional to the time since these sequences last shared a common ancestor (as in the example given above). If the molecular clock is not constant, the correlation between genetic distance and time since divergence is weaker. The molecular clock is then said to be relaxed. Assuming a strict or relaxed molecular clock will have an impact on the dating of phylogenetic nodes. We will, therefore, test the molecular clock hypothesis before dating the tree.</p>"},{"location":"advanced/phylogenetics/#molecular-clock-testing","title":"Molecular Clock Testing","text":"<p>Open the software TempEst by running the following command:</p> <pre><code>tempest\n</code></pre> <p>TempEst [2] is a tool designed to investigate the \u2018temporal signal\u2019 of molecular phylogenies. It tests whether there is sufficient genetic change between sampling times to reconstruct a statistical relationship between genetic divergence and time, which is the essence of a molecular clock in your data. You can download the software for free from http://tree.bio.ed.ac.uk/software/tempest.</p> <p>How does it work? TempEst performs \u2018root-to-tip\u2019 linear regressions, which can be used as a simple diagnostic tool for molecular clock models. It implies plotting the genetic divergence of the sequences (i.e., the sum of the branch lengths from a sequence - the tip - to the root of the tree) against the sampling time of the sequences (Figure 3). A linear trend with few residuals indicates that evolution follows a strict molecular clock. The same trend with greater scatter from the regression line suggests that a relaxed molecular clock model may be most appropriate. No trend at all indicates that the data contains little temporal signal and is unsuitable for inference using phylogenetic molecular clock models.</p> <p></p> <p>Import the ML tree you saved earlier or, if you can\u2019t find it anymore, select the file named RAxML_bipartitions.H1N1.flu.2009.ML.tre in your folder. If you are not prompted for the tree when <code>TempEst</code> opens, import it using the drop-down menu: <code>File</code> &gt; <code>Open\u2026</code></p> <p>We now have to tag the sequences with their sampling date. A list of all the sequences in the tree will appear (in the default <code>Sample Dates</code> tab). In order to plot root-to-tip genetic distances against sampling time, each sequence has to be associated with its date of sampling. Sampling dates are indicated at the end of the sequences\u2019 name, preceded by the suffix \u2018_d\u2019, in the format YYYY-MM-DD. For instance, the sequence named \u2018A/Lima/WRAIR1687P/2009_d2009-06-27\u2019 was sampled on the 27th of June 2009.</p> <p>In the Sample Date tab, click on <code>Parse Dates</code>. In the <code>Parse Dates for Taxa</code> window, select the following options:</p> <ul> <li>The date is given by a numerical field in the taxon label that is defined by a prefix and its order (Order: Last; Prefix: _d)</li> <li>Parse as a calendar date (Date format: yyyy-MM-dd)</li> </ul> <p>Click <code>OK</code>, and the time of sampling will appear in the Date column.</p> <p>Go to the <code>Root-to-tip</code> tab to see the root-to-tip linear regression plot. Check the box marked Best-fitting root. Summary statistics of the plot are shown in the left-hand side window:</p> <ul> <li>Date range: the maximum time interval between two sampled sequences.</li> <li>Slope: The slope of the regression line; corresponds to the rate of evolution, here the average number of nucleotide substitutions per unit of time (here, per year).</li> <li>X-intercept: the time at which the viral population had no genetic diversity, which corresponds to the time of the most recent common ancestor of the sampled population (tMRCA).</li> <li>Correlation Coefficient: A measure of the relationship between time and the number of accumulated mutations (range: -1, 1). A positive/negative value implies a positive/negative linear relationship between time and diversity (i.e., as time increases, so does genetic divergence). A value close to 0 suggests no relationship between time and genetic divergence.</li> <li>R squared: A measure of how close the data is to the regression line, i.e., what proportion of the variation in genetic divergence is explained by a strict molecular clock hypothesis.</li> </ul> <p>Question</p> Q3Answer <p>Can we assume that the rate of evolution of the viruses in the phylogeny is constant over time (i.e., that the molecular clock is strict)?</p> <p>The root-to-tip linear regression plot shows a positive correlation between time and divergence (correlation coefficient = 0.71; see the figure above). This suggests that the phylogeny has sufficient temporal signal to conduct a molecular clock analysis on this dataset.</p> <p>However, there is a certain degree of scatter from the regression line, and only 51% of the variation in genetic divergence is explained by a strict molecular clock hypothesis (R squared = 0.51). We will, therefore, assume significant variation from a strict molecular clock in this dataset and opt for a relaxed molecular clock (i.e., the rate of evolution varies over time and across lineages). This is despite a relatively short sampling interval (about 3 months) and moderate genetic diversity in the sampled population.</p> <p></p>"},{"location":"advanced/phylogenetics/#dating-migration-events","title":"Dating Migration Events","text":"<p>Now that the properties of the molecular clock in our dataset have been established, we will estimate the time(s) at which H1N1/09 was introduced in England.</p> <p>Reminder</p> <p>In a \u2018classic\u2019 phylogenetic tree, branch lengths reflect as the number of nucleotide substitutions per site. Rates of evolution are expressed as the number of substitutions per site per unit of time. By dividing the length of a branch by the rate of evolution, we end up with a tree where branch lengths represent time units. A branch therefore represents the time elapsed between two nodes. Or, in our case, the time elapsed between two transmission events.</p> <p>The reconstruction of dated phylogenetic trees is computationally intense and could not be achieved within the time imparted for this practical. A dated H1N1/09 phylogeny was therefore built prior to the session, under the appropriate molecular clock model, using the Bayesian MCMC approach implemented in the software BEAST v.1.8 [3]. The resulting dated tree is in your folder under the name H1N1.flu.2009.mol_clock.tre. A more detailed example of how to set up a BEAST v1.8. analysis, including producing the input files and analysing the output is shown in the Appendix.</p> <p>How does it work? Each tip of the tree has a known time, given by the sampling date of the sequence. Internal nodes are given arbitrary starting times consistent with their order in the tree (from the tips to the root). An additional parameter, the evolution rate, is used to scale these times into expected number of nucleotide substitutions per site. Markov chain Monte Carlo integration is then used to summarize the probability density function of a model tested against the data, providing a representative sample of parameter values of the chosen model. The model includes the tree topology, the times of internal nodes and the evolution rate.</p> <p>We will use the program FigTree to display the dated tree and its annotations.</p> <p>Open FigTree by running the command: <pre><code>figtree\n</code></pre></p> <p>Import the dated tree: <code>File</code>&gt; <code>Open\u2026</code> &gt; H1N1.flu.2009.mol_clock.2.tre</p> <p>A phylogeny will appear. Again, go to <code>Tree</code> &gt; <code>Increasing Node Order</code> to display the tree in the same way as the one you generated with RAxML. This will ease comparison. Note that the major clusters do not change, just the order in which the clusters are organised from top to bottom. The dated tree should be very similar to the ML tree you reconstructed in Session 1. However, in this tree, the branch lengths represent days rather than genetic distances. Notice the scale at the bottom of the tree.</p> <p>On the lefthand side toolbar, tick the <code>Node Labels</code> box. The age of the nodes will appear.</p> <p>The age of a node is expressed as the number of days prior to the most recent sampling date in the tree. Here, the most recent sample is A/Lima/WRAIR1687P/2009, sampled on the 27th of June 2009. If, for instance, a node age equals 21 days, it means that the date at this node is the 6th of June 2009.</p> <p>Question</p> Q4Answer <p>According to the molecular clock dating, what are the date(s) of introduction of H1N1/09 in England (rounded down)? Tip: If mental arithmetic is not your thing, you can use the 2009 calendar provided as Appendix to back-calculate the date of the nodes.</p> <p>The most recent viral sequence (A/Lima/WRAIR1687P/2009) was sampled on 27/06/2009. The ancestral node of Cluster 1 has an age of 63 days (rounded down) prior to the most recent sample. The estimated date of the first introduction of H1N1/09 in England is thus 25/04/2009.</p> <p>The ancestral node of Cluster 2 has an age of 57 days (rounded down) prior to the most recent sample. The estimated date of the second introduction of H1N1/09 in England is thus 01/05/2009.</p> <p>The ancestral node of Cluster 3 has an age of 40 days (rounded down) prior to the most recent sample. The estimated date of the third introduction of H1N1/09 in England is thus 18/05/2009.</p>"},{"location":"advanced/phylogenetics/#identifying-the-geographical-origins-of-the-h1n109-strains-imported-in-england","title":"Identifying the geographical origin(s) of the H1N1/09 strains imported in England","text":"<p>We will finally reconstruct the migration pathways of these H1N1/09 strains.</p> <p>Reminder</p> <p>When individuals are infected in one location and then move to another, or infect someone whilst travelling, this is apparent as a \u201cchange\u201d in the location ascribed to one branch of the tree. These changes in location along a phylogenetic tree can be inferred from the location values at the tips and the shape of the tree (see Figure 4). To do so, a model of location exchange process is fitted to the data and the most likely location of the viral strain positioned at the nodes of the tree, together with its probability, can be estimated using a MCMC sampling procedure similar to the one used in Section 2.</p> <p></p> <p>This approach was applied to the H1N1/09 phylogeny, using an asymmetric continuous-time Markov chain [5], as implemented in the program BEAST. The asymmetric model uses separate parameters for forward and reverse rates of movement between each pair of locations. The tree file we used for the molecular clock analysis (H1N1.flu.2009.mol_clock.2.tre) also contains the result of the inferred migration patterns.</p> <p>On the left hand side toolbar, pull down the Node Labels menu and under Display, select Location. The most likely location of the ancestral virus located at the nodes will appear.</p> <p>To display the posterior probability of the most likely node location, select Location.prob in the Display menu of the Node Labels.</p> <p>Question</p> Q5Answer <p>Which country is the most likely source of the H1N1/09 epidemic? What is the probability of that location?</p> <p>The most likely location state at the root of the tree is the US (posterior probability: 0.84).</p> <p>Question</p> Q6Answer <p>Where were the English strains of H1N1/09 imported from? How confident are we? How does your answer compare to that of Q4?**</p> <ul> <li>The location at the node from which Cluster 1 is branching off (node 1 in the figure shown with the answer to Q1) is the US (posterior probability: 0.55).</li> <li>The location at the node from which Cluster 2 is branching off (node 2 in the figure shown with the answer to Q1) is Canada (posterior probability: 0.48).</li> <li>The location state at the node from which Cluster 3 is branching off (node 3 in the figure shown with the answer to Q1) is the US (posterior probability: 0.87).</li> </ul> <p>The geographical origin of the founder viruses is consistent for Cluster 2 and 3. However, for Cluster 1, the tree topology suggests relatedness with a Canadian cluster while the phylogeographic reconstruction places the founder virus of that cluster in the US. The most likely explanation is that the English and Canadian viruses were both imported from the US.</p>"},{"location":"advanced/phylogenetics/#references","title":"References","text":"<ol> <li>A. Stamatakis: 'RAxML Version 8: A tool for Phylogenetic Analysis and Post-Analysis of Large Phylogenies'. Bioinformatics, 2014</li> <li>A. Rambaut, T. T. Lam, L. Max Carvalho, and O. G. Pybus, \u2018Exploring the temporal structure of heterochronous sequences using TempEst (formerly Path-O-Gen)\u2019, Virus Evol., vol. 2, no. 1, p. vew007, Jan. 2016.</li> <li>A. J. Drummond, M. A. Suchard, D. Xie, and A. Rambaut, \u2018Bayesian Phylogenetics with BEAUti and the BEAST 1.7\u2019, Mol. Biol. Evol., vol. 29, no. 8, pp. 1969\u20131973, Jan. 2012.</li> <li>N. R. Faria et al., \u2018The early spread and epidemic ignition of HIV-1 in human populations\u2019, Science, vol. 346, no. 6205, pp. 56\u201361, Mar. 2014.</li> <li>P. Lemey, A. Rambaut, A. J. Drummond, and M. A. Suchard, \u2018Bayesian phylogeography finds its roots\u2019, PLoS Comput. Biol., vol. 5, no. 9, p. e1000520, Sep. 2009.</li> </ol>"},{"location":"advanced/phylogenetics/#appendix","title":"Appendix","text":""},{"location":"advanced/phylogenetics/#phylogenetic-tree-building-with-beast","title":"Phylogenetic Tree Building with BEAST","text":"<p>(Note: This example is for initializing a BEAST v.1.8 analysis; BEAST2 has a different interface.)</p> <p>1. Creating a BEAST Input File Using BEAUti</p> <p>Launch BEAUti by typing <code>beauti</code> into the command line or opening it from the dashboard.</p> <p>1.1. Load the FASTA sequence file H1N1.flu.2009.fas by clicking the <code>+</code> button or going to <code>File</code> &gt; <code>Open\u2026</code>.</p> <p></p> <p>This first will give you information on the number of taxa (50), the number of sites (12735), and the data type (nucleotide).</p> <p>If you want to partition the data, i.e. if you have samples that you know are from different populations or lineages, you can go onto the \u2018Taxa\u2019 tab and set these partitions. This will allow you to specify whether these groups are monophyletic as well as divergence dates between groups. For this exercise, we will not assume any partitioning of samples.</p> <p>1.2. Attach a date to each sample.</p> <p>This can be done in the same way as was shown in TempEst: In the Tips tab, tick the box for Use tip dates, and then click on Parse Dates. In the Parse Dates for Taxa window, select the following options:</p> <ul> <li>The date is given by a numerical field in the taxon label that is defined by a prefix and its order (Order: Last; Prefix: _d)</li> <li>Parse as a calendar date (Date format: yyyy-MM-dd)</li> </ul> <p>Alternatively, you can create a tab-delimited file (.txt) with two columns corresponding to the taxa names and sample dates, and use Import Dates.</p> <p></p> <p>1.3. Set the location for each sample</p> <p>To attach further traits to each sample, such as location, in the <code>Traits</code> tab, click on <code>Import Traits</code> and open a tab-delimited file (.txt) with two columns corresponding to the taxa names and location (or characteristic of choice).</p> <p>1.4. Set a site (substitution) model. </p> <p>This allows us to substitution and site heterogeneity models. The rate and likelihood of base substitution over the genome will be variable so we will set some parameters to describe the model of substitution. We will set the substitution model to GTR \u2013 Generalized Time Reversible model \u2013 as this is the most frequently used model and allows for variable base frequencies. It is also reversible (i.e. T-&gt;A same rate as A-&gt;T). We can also set the site heterogeneity model, i.e. the frequency of each site to a Gamma distribution with 4 categories.</p> <p>In the Sites tab, select GTR from the \u2018Substitution model\u2019 dropdown box. We will set the \u2018Site Heterogeneity model\u2019 as \u2018Gamma\u2019 with 4 categories.</p> <p></p> <p>1.5 Set a clock model. The \u2018Clocks\u2019 tab will allow us to set the molecular clock for mutations along each branch of the tree. If we set this to \u2018Strick clock\u2019 this specifies that the mutation rate along each branch will be invariable. As we have shown in the TempEst analysis that our data varies significantly from a strict molecular clock, we will select a relaxed clock to allow for rate variation across branches in our tree.</p> <p>In the Clocks tab, select \u2018Uncorrelated relaxed clock\u2019 from the \u2018Clock Type\u2019 dropdown box and keep the \u2018Relaxed Distribution\u2019 option as \u2018Lognormal\u2019.</p> <p></p> <p>1.6 Set a tree model.</p> <p>As the phylogenetic tree is built through a coalescent approach, backwards through time based on the genetic diversity at tips/samples, you can will need to set a model for the demographic history of the population that describes the effective population size through time. For example, \u2018Constant size\u2019 will state that the effective population size has remained constant through evolutionary time. For the purposes of this exercise, we will choose a constant population size.</p> <p>In the <code>Trees</code> tab, select \u2018Coalescent: Constant Size\u2019 from the \u2018Tree Prior\u2019 dropdown box.</p> <p> Note: You can also input a starting tree on which BEAST will base the topology of the Bayesian tree. This may be a neighbor-joining or parsimony tree and can speed up the analysis by approximating the topology as a prior. We will start with a random tree.</p> <p>The 'States', 'Priors\u2019 and 'Operators' tabs allow us to specify whether to reconstruct sequences, set specific priors for each parameter, and to specify which operators to include in the output log file. We will leave all these as default values.</p> <p>1.7 Define the length of the MCMC chain</p> <p>Finally, the \u2018MCMC\u2019 tab is where we will define the steps in the MCMC chain that is used to construct trees. Here we are interested in the length of the chain. This is the number of iterations (number of steps in the random walk) the program will go through to converge on the best tree topology. The higher this number, the more support there will be for the final tree and the more chance of convergence.</p> <p>Define the length of the chain by typing '10000000' in the 'Length of chain' box.</p> <p>The name of the output files can be specified here. This will determine the names of the files that will be created after the BEAST run, .log, .trees, and .ops. The output file from BEAUti will be a .xml file for BEAST input. We will call this H1N1.flu.2009, so type this in the \u2018File name stem\u2019 box, if it doesn\u2019t already appear there.</p> <p></p> <p>Then click on the <code>Generate BEAST file</code> button at the bottom right of the screen to generate the .xml file that will be used as the input for BEAST.</p> <ol> <li>Running the BEAST analysis BEAST can be run on the command line or through the desktop application. We will run BEAST on command line.</li> </ol> <p>To run BEAST, open up a terminal and make sure you are in the folder containing your .xml file from the BEAUti output and type: <pre><code>beast H1N1.flu.2009.xml\n</code></pre> This will create three files; .log, .trees, and .ops, with the \u201cH1N1.flu.2009.\u201d prefix.</p> <ol> <li>Evaluating your BEAST run Open the program TRACER by typing <code>tracer</code>. This will allow you to check the output from BEAST and determine how effectively the MCMC has mixed, i.e. how close it is to reaching a steady state (converging). This can be used to manually evaluate the performance of the BEAST run.</li> </ol> <p>Load in the \u2018H1N1.flu.2009.log\u2019 file to Tracer to see the output.</p> <p></p> <p></p> <p>Here we want to look at the ESS score. This is the Effective Sample Size, which is a measure of the number of effectively independent draws from the posterior distribution that the Markov chain is equivalent to. Low scores are highlighted in red, with scores &gt;200 considered sufficient to demonstrate good mixing.</p> <p>To increase ESS scores, you can run the analysis for longer (more MCMC iterations), or do multiple independent runs that can be combined with the program LogCombiner. Very low scores for many parameters may though suggest that the prior parameters are not a good fit and should be varied.</p> <p>In our data, we can see that most values are over 200, with only the prior and posterior values highlighted in red. Looking at the \u201cTrace\u201d of the MCMC though, we can see that it remaining reasonably steady around -19600 for the prior parameter, suggesting that this support will increase (ESS will increase) if the MCMC chain was longer.</p> <ol> <li>Producing a single phylogenetic tree</li> </ol> <p>The program TreeAnnotator summarises the sample of trees produced by BEAST (in the .tree file) onto a single maximum clade credibility tree. The posterior probabilities of the nodes in the target tree, the posterior estimates and HPD limits of the node heights and (in the case of a relaxed molecular clock model) the rates, are shown.</p> <p>Load in the \u2018H1N1.flu.2009.trees\u2019 file to TreeAnnotator through <code>Choose File\u2026</code> in <code>Input Tree File</code>. Select what to the call the output file, e.g. H1N1.flu.2009.tre</p> <p>You can also set the number of burnin states (The number of initial MCMC runs to discard).</p> <p>Then select <code>Run</code>to produce a single maximum clade credibility tree. </p>"},{"location":"advanced/phylogenetics/#calendar","title":"Calendar","text":""},{"location":"advanced/third-generation-sequencing/","title":"Third Generation Sequencing","text":""},{"location":"advanced/third-generation-sequencing/#introduction","title":"Introduction","text":"<p>In this session we are going to be looking at data generated by third-generation nanopore sequencing technology. Developed by Oxford Nanopore Technologies (ONT), these platforms, rather than the next-generation 'sequencing-by-synthesis approach', make use of an array of microscopic protein \u2018pores\u2019 set in in an electrically resistant membrane which guide strands of DNA or RNA through them. Each nanopore corresponds to its own electrode connected to a channel and sensor chip, which measures the electric current that flows through the nanopore. When a molecule passes through a nanopore, the current is disrupted to produce a characteristic \u2018squiggle\u2019. The squiggle is then decoded using basecalling algorithms to determine the DNA or RNA sequence in real time. Oxford Nanopore\u2019s most popular platform is the MinION which is capable of generating single reads of up to 2.3 Mb (2.3 million bases).</p> <p></p> <p>The MinION is one of 5 scalable platforms developed by ONT. High-throughput applications such as the GridION and PromethION use an array of nanopore flowcells to produce between 5 to 48 times more data than the MinION alone \u2013 outputting up to 48 TB of data in one run. More downscaled solutions such as The Flongle and SmidgION use a smaller, single flowcell to generate data. The MinION is a highly portable sequencing platform, about the size of a large USB flash drive. This technology enables researchers to perform sequencing analyses almost anywhere, providing they have the correct equipment to prepare the DNA libraries and analyse the output data.</p> <p></p> <p>A complete sequencing run on the MinION platform can generate upwards to of 1TB of raw data, and downstream analyses require a significant amount of computing power, multicore high performance processors and large amounts of RAM. This poses a significant logistical challenge for researchers who want to take advantage of the platform\u2019s portability aspect. Over recent years, the integration of GPUs (graphics processing units) has made it easier to analysis workflows.</p> <p></p>"},{"location":"advanced/third-generation-sequencing/#activity-briefing","title":"Activity Briefing","text":"<p>Today we will be working with data generated by the ZIBRA project Faria et al., 2017. The ZIBRA group used real-time portable sequencing during the 2015 South American Zika virus (ZIKV) outbreak to provide a surveillance framework for tracking ZIKV spread into other geographic regions. With data generated using the MinION during the outbreak, we will follow a typical nanopore data analysis pipeline to produce a whole-genome Zika virus sequence to explore the capabilities of nanopore sequencing and the kind of data it can produce. We will then use maximum-likelihood (ML) techniques for phylogeographic reconstruction, to try and find out where our isolate came from and place it in the context of the wider South American Zika.</p>"},{"location":"advanced/third-generation-sequencing/#basecalling","title":"Basecalling","text":"<p>To convert the raw data output produced by the MinION sequencing run in to a usable form we need to perform a process called basecalling. This converts the raw electronic signal which is collected as the DNA passes through the pore, in to base reads \u2013 A, C, T or G. To do this we will use a program called Guppy \u2013 a software package designed by ONT which uses recurrent neural nets (RNN) to interpret the raw signal, which comes in a proprietary '.fast5' format file produced by the sequencer software and convert it in to the standard .fastq format, for use downstream in our pipeline. Users also have the choice of using the experimental Bonito basecaller which gives the option of training specialised models for specific basecalling applications. As mentioned above, GPUs are used to accelerate the basecalling process. Without a GPU performing basecalling becomes a very slow process, therfore it is advised that users procure a machine with a compatable Nvidia GPU (more information on this here).</p> <p> </p> <p>Activate the relevant environment, navigate to the <code>~/data/nanopore_activity/basecalling</code> folder in the home directory, and we\u2019ll start the first step.</p> <pre><code>conda activate nanopore\ncd ~/data/nanopore_activity/basecalling/raw_fast5_reads/ \n</code></pre> <p>Info</p> <p>Use the <code>ls</code> command to see what is inside this folder. Use <code>head</code> to preview one of the fast5 files. As you might find, it's completely unreadable. This is because at this stage, the data is in a binary format representing the squiggle signal we spoke about previously. We need to basecall this data before we can use it.</p> <p>Basecalling can be performed in a number of ways. There is an option to perform this while sequencing in the MinKNOW GUI package, however this software provides fewer options in the ways basecalling can be completed and is less powerful. Here, we will use Guppy for maximum flexibility. Since the machines we are working on do not have a GPU available we will have to use the two CPU cores available to us. Therefore, we will only basecall a subset (&lt;1%) of the dataset as an example, and in the subsequent steps we will use a pre-basecalled output.</p> <p>Hover over the different elements of the basecalling command to see its function:</p> <pre><code>guppy_basecaller --config dna_r9.4.1_450bps_fast.cfg --trim_adapters --compress_fastq  --input_path ~/data/nanopore_activity/basecalling/raw_fast5_reads  --save_path ~/data/nanopore_activity/basecalling/fastq \n</code></pre> <p>Info</p> <p>Hover over the basecalling model 'dna_r9.4.1_450bps_fast.cfg' in the above command to see an explanation. You can navigate to <code>/home/user/software/ont-guppy-cpu/data/</code> to see all of the models available for use. Alternatively, for simplicity, you can use the <code>--kit</code> and <code>--flowcell</code> flags instead of defining a model. However, by default, using these arguments do not apply the most accurate basecalling models available.</p> <p>You should now see the bascalling process begin, and a progress bar appear. This may take some time depending on the performance of your machine.</p> <p></p> <p>When the process is completed, you will find the basecalled reads in a .fastq formatted file. Navigate there by typing the following into the terminal:</p> <pre><code>cd ~/data/nanopore_activity/basecalling/fastq/pass\n</code></pre> <p>Info</p> <p>Use the <code>ls</code> command to see what is inside this folder. This directory holds the fastq formatted 'pass' reads from the basecalling process. The reads have a quality score &gt; 7. Use <code>zcat | head</code> to preview the compresssed fastq file. Unlike the fast5 files, these are human-readable and contain all of the read data required for downstream analyses. Can you identify any of the common elements of a .fastq format files - similar to the ones you may have encountered in previous sessions? Click here to find out more about the FASTQ format.</p>"},{"location":"advanced/third-generation-sequencing/#basecalling-quality-control","title":"Basecalling: Quality Control","text":"<p>Before moving on to the analysis steps, it is important to gauge the quality of your sequencing output. There are numerous factors which dictate the quality of the output data, spanning between quality of the input material, library preparation to software and hardware failure. We will look at some important metrics produced by the sequencer which will give us a feel for how well the run went.</p> <p>In order to get the run metrics in to a useful form, we will use an pycoQC to produce a range of plots in a HTML output, which we will use to judge the quality of the sequencing run. Something to note, is that in this activity we will only use a small subset of the sequenced reads, or else the analysis would take all day. This subsetting means that the sequencing telemetry may look inconsistent, when compared to a full run.</p> <pre><code>pycoQC -f ~/data/nanopore_activity/basecalling/fastq/sequencing_summary.txt -o pycoqc_results.html\n</code></pre> <p>Info</p> <p>After executing the command you should find a file called 'pycoqc_results.html'. Open them up in the file manager or in the terminal (with the below command) and inspect some of the plots and see what you can find out. As mentioned, the data here are only a small subset of reads, so some of the plots are incomplete. But this should give you a good idea of how this analysis should look.</p> <pre><code>firefox ~/data/nanopore_activity/basecalling/fastq/pass/pycoqc_results.html\n</code></pre> <p>Before continuing, quit firefox by clicking the X in the top right corner of the web-browser window.</p> <p>Question</p> Question 1Answer 1 <p>Approximately how long did the sequencing run take?</p> <p>0.7 hours</p> <p>Question</p> Question 2Answer 2 <p>What is the N50 of the passed reads (&gt;Q7) for all basecalled reads in this run?</p> <p>N50 = 636 bp</p>"},{"location":"advanced/third-generation-sequencing/#adapter-trimming","title":"Adapter Trimming","text":"<p>Nanopore library preparation results in the addition of a sequencing adapter at each end of the fragment. Both the template and complement strands to be sequenced carry the motor protein which means both strands are able to translocate the nanopore. For downstream analysis, it is important to remove these adapters. For this we will use Porechop. This program processes all of the reads in our basecalled fastq file, and removes these adapter sequences. Furthermore, the ligation library prep process can result in conjoined reads, meaning an adapter will be found in the middle of an extra-long read. Porechop will identify these, split them and remove the adapters. In addition, if you use a multiplexing kit to maximise sample throughput, this program will split the reads based on the molecular barcode added to each sample. Our dataset only has one sample, so this demultiplexing won't be necessary.</p> <p>Let's launch porechop and remove the adapters from the basecalled fastq file:</p> <pre><code>porechop -i ~/data/nanopore_activity/basecalling/fastq/pass/*.fastq.gz -o basecalled_reads.porechop.fastq\n</code></pre>"},{"location":"advanced/third-generation-sequencing/#kraken-qc","title":"Kraken QC","text":"<p>Another method of quality control is to check our reads for sequence contamination from other 'off-target' organisms. This is important in order to firstly, understand how effective your DNA extraction, enrichment and sequencing was. And secondly, to prevent anomalous reads from being incorporated in to assemblies.</p> <p>Using our basecalled reads we will perform an analysis using Kraken. Kraken is a tool which sifts through each read in a .fastq file and crosschecks it against a database of microorganisms. The output is a taxonomic assignment of each read, enabling to identify if any contamination has occurred. In this case we will be looking for any reads which do not belong to the Zika genome.</p> <p>Let\u2019s navigate to the kraken folder to begin the analysis:</p> <pre><code>cd ~/data/nanopore_activity/kraken\n</code></pre> <p>The following line of code is composed of these elements:</p> <p><code>kraken</code> \u2013 calling the Kraken executable</p> <p><code>kraken --db ~/data/nanopore_activity/kraken/KDB/</code> - this points kraken to a vast sequence database of relevant microorganisms to cross-check our reads against</p> <p><code>--output temp.krak</code> \u2013 this argument locates the output file</p> <p><code>basecalled_reads.porechop.fastq</code> \u2013 this argument locates the input file</p> <p>Type the following command in to the terminal to unleash the Kraken:</p> <pre><code>kraken --db ~/data/nanopore_activity/kraken/KDB/ --output temp.krak ~/data/nanopore_activity/kraken/basecalled_reads.porechop.fastq\n</code></pre> <p></p> <p>This file isn't particularly easy to interpret, so we will use a program called Recentrifuge to transform these data in to a more human-readable format.</p> <pre><code>rcf -k temp.krak -o rcf.html -n taxdump/\n</code></pre> <p>Try opening the HTML file generated by recentrifuge in a web browser, what can you tell about the sequencing run? Was is successful? Note - due to constraints with the virtual machine, we have generated an alternative report, which can be loaded using the below command. Copy and paste it in to the terminal. If you have any questions about this, ask a demonstrator.</p> <pre><code>firefox rcf.html\n</code></pre> <p>Question</p> Question 3Answer 3 <p>Can you see any contaminating reads? What organism(s) is/are there?</p> <p>Cyanobacteria and Betaproteobacteria</p>"},{"location":"advanced/third-generation-sequencing/#mapping-and-visualisation","title":"Mapping and Visualisation","text":"<p>Now that we have varified a successful sequencing run, our basecalled and trimmed Zika-confirmed data are ready to go, we will now map the reads on to a reference genome and perform variant calling. For the first step we will use the BWA program. You may be familiar with the mapping process from previous sessions. Using BWA we will align our .fastq reads to a reference genome. We need to make some slight modifications to the mapping command used previously, as we need to accommodate for some of the features of the nanopore data.</p> <p>Move to the mapping directory:</p> <pre><code>cd ~/data/nanopore_activity/mapping\n</code></pre> <p>Now we can use minimap2 to align our qc complete, porechopped reads. Minimap is a fast alignment tools similar to BWA mem. You can find more information about this tool by clicking the link comparing the two alignment tools</p> <pre><code>minimap2 -ax map-ont ./reference.fasta basecalled_reads.fastq | samtools view -q 15 -b | samtools sort -o alignment.bam\n</code></pre> <p>Finally we need to index the sorted bam file:</p> <pre><code>samtools index alignment.bam\n</code></pre> <p>Now that we have successfully mapped the reads to a reference we can visualise them in Tablet, to get a closer look at what our sequencing run looks like.</p> <pre><code>tablet\n</code></pre> <p>Using the below animation as a guide, open up tablet and load the alignment.bam and the reference.fasta files, then click on the \u2018contig\u2019 in the pane on the left side of the screen. You will see a loading bar, then the alignment will be shown. In the left \u2018contig\u2019 pane you will fine some metrics associated with this dataset.</p> <p></p> <p>Info</p> <p>Note the \u2018mismatch\u2019 column in the left hand pane. This metric describes the percentage of bases which do not \u2018agree\u2019 with the reference file sequence. </p> <p>Info</p> <p>Try bringing up some of the reads you have worked with in previous session. Remember to select the appropriate \u2018sorted\u2019 .bam file and the correct reference fasta. Does anything strike you about the contrasting features of the data sets? Scrolling downwards in the alignment viewer on Tablet, what do you notice about the \u2018structures\u2019 formed by the reads?</p> <p>Question</p> Question 4Answer 4 <p>What is the percentage of the bases which are mismatched?</p> <p>16.2%</p> <p>One of the foremost caveats of nanopore sequencing is the high error rate. It is important that we use programs which are sensitive to that - which is why we used the 'ont2d' argument in the mapping stage. Try switching the colour scheme to \u2018Variants\u2019 by using the tab in the top left corner of the interface.</p> <p></p> <p>You can take a closer look at the individual bases by zooming in, either by using the sliding bar on the \u2018Home\u2019 tab at the top of the screen or by using \u2018Ctrl\u2019+Mouse Scroll. The red squares in the \u2018Variants\u2019 view represent the mismatched reads.</p> <p>Info</p> <p>An important aspect of an effective nanopore sequencing analysis is being able to differentiate between errors or low quality basecalls, and true SNPs. You can see that there are random errors dotted around the screen, like static on a TV. Do you notice that some positions in the alignment have a distinct vertical column of red squares. These are most likely the variants we are looking for, and the key to unlocking our sequence data. These columns represent positions which have a high frequency of basecalls which do not agree with the reference sequence. It is unlikely that random errors will appear in such a manner, and so, in our next analysis we will use these high frequency variants to alter the reference sequence to build a new sequence, in a process called \u2018variant calling\u2019.</p>"},{"location":"advanced/third-generation-sequencing/#coverage-depth-and-variant-calling","title":"Coverage, Depth, and Variant Calling","text":"<p>Info</p> <p>Variant calling is a process used to identify new genotypes based on the \u2018differences\u2019 found our read data. You will have used it in the previous mapping practical at the start of the course. In this case, we are going to be using the alignment you have just generated and compiling a database of SNPs, inferred from positions which have a majority allele which is different from the one found on the reference sequence.</p> <p>Before starting on variant calling, we first need to do one more QC step. This analysis will tell us how well our reads have aligned to the reference and how comprehensive our sequencing run was. Two key metrics are required for this: reference coverage and read depth. Read coverage tells us the percentage of the reference which has had sequencing reads aligned to it, which allows us to identify any regions that may have not been successfully sequenced. Depth is an equally as important metric: it tells us how many different reads have mapped to the same position. This is a particularly important statistic if you intend on doing variant calling, as regions with low depth may fall prey to false calls due to the random errors we have in our nanopore data. With a high enough read depth, we can be fairly sure that these errors will be ignored.</p> <p>We will now use R to generate a plot to allow us to assess the coverage and depth.</p> <p>Navigate to the \u2018variant_calling\u2019 folder and we\u2019ll begin:</p> <pre><code>cd ~/data/nanopore_activity/variant_calling \n</code></pre> <p>We need to use Samtools, a versatile package you will be familiar with, to extract the depth statistics from the .bam alignment file we generated in the previous section. This will generate a file called \u2018depth_statistics\u2019.</p> <pre><code>samtools depth ~/data/nanopore_activity/mapping/alignment.bam &gt; depth_statistics\n</code></pre> <p>Next, we will use the R statistical package to generate a plot based on the data samtools generated. Simply type \u2018R\u2019 in to the terminal to initialise the R interface.</p> <pre><code>R\n</code></pre> <p></p> <p>Once you have initialised R, you can enter the following two lines of code, one after the other. The first command will load the \u2018 depth_statistics\u2019 file, and the second will generate the plot</p> <pre><code>data&lt;-read.table(\"depth_statistics\")\n</code></pre> <pre><code>plot(data$V3,type=\"l\",xlab=\"Reference Position\", ylab=\"read Depth\")\n</code></pre> <p>Info</p> <p>Take a look at the plot you have just generated in R. Does the genome have adequate coverage and depth? What do you notice about the depth of the reads of nanopore generated data, as opposed to Illumina? Furthermore, why do you think there are these block-like increases and decreases in the depth? (Hint: how do you think these sequencing libraries were generated?)</p> <p>You can quit R by typing:</p> <pre><code>quit()\n</code></pre> <p>Info</p> <p>Now that we know the coverage and depth, we can move on to the variant calling. For this, we will use a package called \u2018bcftools\u2019. Bcftools will look through the alignment and \u2018call\u2019 the positions which do not agree with the reference, count them and compile them in to a database called a VCF. Enter the following code in to the terminal to begin the process:</p> <pre><code>bcftools mpileup -q 8 -B -I -Ou -f reference.fasta ~/data/nanopore_activity/mapping/alignment.bam | bcftools call -mv -Oz -o calls.vcf.gz\n</code></pre> <p></p> <p>Let's take a look inside the VCF file to see what kind of data it contains:</p> <pre><code>zless calls.vcf.gz\n</code></pre> <p>Info</p> <p>You can scroll down using the down-arrow key on your keyboard. Can you recall the common features of a VCF file? Press 'q' to quit the 'zless' program to continue with the activity:</p> <p>Next, we need to index the .vcf file:</p> <pre><code>bcftools index calls.vcf.gz\n</code></pre> <p>This next line of code will take the calls made in the VCF file and apply them to the reference sequence, changing it based of the differences observe in our reads:</p> <pre><code>bcftools consensus -f reference.fasta calls.vcf.gz -o consensus_sequence.fasta\n</code></pre> <p>Important</p> <p>It is important to understand how the reference sequence can effect the consensus outcome. If unchecked, positions with zero coverage will default to reference. This will result in the experimental sequence becoming more like the reference than it actually is. We can use BEDtools mark positions with low coverage which BCFtools will use to mask the zero coverage positions with 'N's.</p> <p>Info</p> <p>You will now have a file called \u2018consensus_sequence.fasta\u2019. this contains the modified reference, a brand new sequences based on the sequence data we have been working with.</p>"},{"location":"advanced/third-generation-sequencing/#sequence-alignment","title":"Sequence Alignment","text":"<p>Now we are going add our consensus sequence to a multiple sequence alignment, to prepare the dataset for phylogenetic inference. First, navigate to the \u2018phytogeography\u2019 folder:</p> <pre><code>cd ~/data/nanopore_activity/phylogenetics\n</code></pre> <p>In this folder you will find the consensus sequence you have just generated and a pre-built Zika virus sequence dataset, containing the entire* protein coding sequences of all publicly available ZIKV isolates. Let\u2019s open up the dataset in an alignment viewer and take a look at that this kind of data looks like. Like most of the tools you have encountered during this course there are many alternative programs out there that will perform similar tasks, in this instance we will use Aliview. Let\u2019s call up the program and load our dataset:</p> <pre><code>aliview zika_dataset.fasta\n</code></pre> <p></p> <p>With Aliview open, you should now see the dataset we will be working with for the rest of the activity. In the left pane you should see the sequence metadata: the unique sequence accession number, the species the virus was isolated from, the country of origin and the date of collection. Spanning across horizontally, you should see the genome sequence to which the metadata is associated with. On the pane at the bottom of the window, you should see a few statistics about the dataset. Have a browse and familiarise yourself with the data and the software interface.</p> <p>Question</p> Question 5Answer 5 <p>How many sequences are there in the Zika dataset?</p> <p>284</p> <p>Before we continue with any analyses, we first have to add our new sequence and then align the dataset. Alignment is the process of arranging sequence data in such a way that each sequence may be compared to each other. For this we will use a program called \u2018mafft\u2019. This program will rearrange the sequences based on similarity, so that they may be compared in future analyses.</p> <p>Close the Aliview window, and open up the terminal ensuring you are still in the \u2018phylogenetics\u2019 directory. First, we will add our new consensus sequence to the database by using the \u2018cat\u2019 (concatenate) command:</p> <pre><code>cat zika_dataset.fasta consensus_sequence.fasta &gt; zika_all.fasta\n</code></pre> <p>Next, we will call up the alignment program and point it in the direction of our unaligned \u2018zika_all.fasta\u2019 dataset. The alignment command is made up of the following elements:</p> <p><code>mafft</code> \u2013 calls the mafft alignment software</p> <p><code>zika_all.fasta</code> \u2013 the input file </p> <p><code>&gt;</code> - this symbol \u2018points towards\u2019 the output file </p> <p><code>zika_all_aligned.fasta</code> \u2013 this is the name of the output file </p> <pre><code>mafft zika_all.fasta &gt; zika_all_aligned.fasta\n</code></pre> <p>The program may take a short while to run. Alignments generally can take a very long time; some of you may be familiar with using the ClustalO or MUSCLE web-based servers. While these are a few of the more popular options, MAFFT has its merits, particularly in being able to build large alignments in an exceptionally short amount of time.</p> <p>Info</p> <p>Once the program has completed, you should find a file called \u2018zika_all_ailigned.fasta\u2019. Open it up in Aliview and inspect the sequences. Comparing this to the unaligned dataset, can you see how easy it is now, to compare each sequence and visualise the diversity across the dataset. With our sequence added to the dataset, we can now move on to the final section of the activity: phylogenetics.</p>"},{"location":"advanced/third-generation-sequencing/#optional-activity","title":"Optional Activity","text":"<p>Info</p> <p>Another functionality of Aliview is being able to translate nucleotide sequences in to amino acid sequences. Using this function, we can look further in to our dataset at significant non-synonymous mutations, which result in a change in the amino acid sequence. Microcephaly and neuropathologies associated with Zika have only been reported recently, most prevalently in the Americas. In our dataset we have some ancestral strains, from the Pacific Islands and South-East Asia. Using the translate function in Aliview, we can assess the diversity of our dataset, and investigate the prevalence of particular significant mutations.</p> <p>In order to gain a better understanding of the differences between the seemingly more pathogenic South American Zika and the less pathogenic Asian strains, Yuan et al. (2017) investigated recent stable mutations in the genome of contemporary South American Zika strains. Using mouse models and recombinant Zika strains, they studied the in vivo neurovirulence phenotypes of three contemporary ZIKV strains isolated in 2015\u20132016 in South America, and compared them with the phenotypes of their Asian ancestral strain isolated in Cambodia in 2010. They found a particular mutation, S139N, which in vitro, made the virus more infectious for mouse and human neural progenitor cells and promoted apoptosis. </p> <p>Let\u2019s find this site in our dataset and see if our isolate has the S139N genotype:</p> <p>We can translate the dataset by pressing CTRL + SHIFT + T. You should now see the same dataset expressed as amino acid sequences. Notice all of the \u2018X\u2019 residues in the sequences. These are stop codons, however they should not be there, in fact these residues are \u2018nonsense\u2019 \u2013 codon changes which result in a stop codon. In order to get the \u2018true\u2019 amino acid sequence, we need to select the correct reading frame. Press SHIFT + - (shift and the minus/hyphen key). You should now see fewer stop codons in the alignment, and a change in the amino acid sequence. </p> <p>By using the scroll bar at the bottom of the window, we can now navigate to the position in the alignment with the S139N mutation to assess what the genotype of our isolate is. In this instance you will find this mutation at the 138th position, due to our alteration of the reading frame.  What can you tell about our 'consensus' isolate? Do you notice any pattern in the geography of the isolates which contain the S genotype?</p>"},{"location":"advanced/third-generation-sequencing/#phylogenetics-raxml","title":"Phylogenetics: RAxML","text":"<p>RAxML (Randomized Accelerated Maximum Likelihood) is the tool we will be using today to infer our phylogenetic tree. It is a popular program for phylogenetic analyses of large datasets. Its main strengths are its speed and a robust search algorithm resulting in phylogenies with good likelihood scores. The maximum likelihood method uses standard statistical techniques for inferring probability distributions, and to assign probabilities to particular possible phylogenetic tree topologies (the shape of the tree). Hopefully you should be familiar with the basic concepts underpinning phylogenetics, and their applications: so let\u2019s get stuck in.</p> <p>We first need to decide on a substitution model: the mathematical framework which describes the way in which nucleotides change across time. In this instance we will use the GTRGAMMA model. Explaining these models is beyond the scope of this tutorial, however it is important to understand why you would use a particular model. The reason we have selected GTRGAMMA (General Time Reversable \u2013 Gamma + Invariant sites), is because the GTR substitution model is one of the more robust, widely used models available and most importantly is applicable to our dataset.</p> <p>Now we have our multiple sequence alignment, we can now start the phylogenetic inference. The following command consists of these elements:</p> <p><code>iqtree</code> \u2013 this calls the RAxML program</p> <p><code>-m GTR+G</code> \u2013 specifies the substitution model</p> <p><code>-s input.fasta</code> \u2013 specifies the input sequence file</p> <p><code>-bb 1000</code> \u2013 specifies the bootstrap replicates</p> <p>From inside the \u2018phylogenetics\u2019 folder, run the following command in the terminal:</p> <pre><code>iqtree -m GTR+G -s ./zika_all_aligned.fasta -bb 1000 -nt AUTO\n</code></pre> <p>Info</p> <p>Inferring phylogenies is a complex process can take a very long time, depending on the program you use and the size of the dataset. Because we will not have access to HPC (High Performance Computing) in this activity, we will run only 1 inference. However, in the next step, you will be provided with a tree inferred with the same data, but run with 1000 inferences i.e the program will have been run 1000 times. This gives the program the opportunity to explore the phylogenetic landscape further, so that the resultant tree is the most probabilistically robust.</p> <p>Info</p> <p>If you have any questions about this phylogenetic application, or any others you are aware of, don't hesitate to ask a demonstrator.</p> <p>Now that RAxML has completed its run we can now visualise the results in Figtree \u2013 a phylogenetic tree visualisation software package.</p> <p>Run the following command:</p> <pre><code>figtree ./zika_all_aligned.fasta.treefile\n</code></pre> <p></p> <p>Now that you have opened up the tree in Figtree, you first need to assign the bootstrap data to a category. We can then organise the tree in a way that it is more easy to view. Using the animation below as a guide, select the 'Midpoint Root' and 'Increasing Node Order' settings.</p> <p></p> <p>Use the scroll bars in the Figree window, and on the pane on the left hand side of the windows to zoom and navigate the tree.</p> <p>You can use the search bar in the top right corner of the screen to find our 'CONSENSUS' sequence.</p> <p>Info</p> <p>Now that we can see where our sequence falls on the tree, we can look in to the geographical and chronological metadata in our dataset and the structure of the tree, to gain a deeper understanding of what happened during the Zika outbreak in the Americas.</p> <p>Question</p> Question 6Answer 6 <p>What is the closest ancestor of our sequence?</p> <p>Brazil - 13/04/2016</p> <p>Info</p> <p>What can you tell about the outbreak from the topology (shape and order) of the tree? Can you see distinct clades (groupings) in the overall structure of the tree? As you follow the nodes in each clade further in to the tree, towards the leaves, what to notice about the geography of the isolates in this dataset? Can you tell where most of the lineages began?</p> <p>We can use the phylogenies similar to the one we have just generated to map transmission and the spread of diseases.</p> <p>Info</p> <p>Click on the 'Phylogeography Map' link. the program it will take you to was built using a similar Zika dataset that we have used today, with a few extra added isolates. To generate this map, we used Bayesian ancestral state reconstruction with a molecular clock to map the spread of Zika during the 2015-16 outbreak. Do you notice the similarities between the structure of the tree in Figtree and the phylogeographic map?</p> <p>Phylogeography Map</p>"},{"location":"advanced/third-generation-sequencing/#conclusion","title":"Conclusion","text":"<p>Hopefully having completed this activity, you will have gained a better understanding of how to process third-generation nanopore data, and the applications of such a technologies in an infectious disease outbreak scenario. The pipeline we have used in this activity is one of a hundred you can use to carry out similar tasks, but choosing the right tools for the job is paramount when performing sequence analysis.</p> <p>The use of front-line genomics during outbreaks is a somewhat novel practice, but with tools such as the MinION and its downstream applications, it is possible to monitor the dynamics of disease outbreaks in real-time to inform on-the-ground containment strategy and provide a framework for surveillance, both in a preventative and a post-outbreak context.</p> <p>If you have any further questions about this activity or your own applications of this skills learnt during this course, ask a demonstrator.</p>"},{"location":"commands/assembly/","title":"Assembly","text":"<pre><code>conda activate assembly\n\ncd ~/data/tb\nls\nzcat sample1_1.fastq.gz | head -8\n\nwc -l *\nzcat sample1_1.fastq.gz | head -2 | tail -1 | wc -c\nspades.py -1 ~/data/tb/sample1_1.fastq.gz -2  ~/data/tb/sample1_2.fastq.gz -o sample1_asm\n\ncd sample1_asm\nquast -r ../tb.fasta -o quast contigs.fasta \n\ncp ~/data/tb/tb.fasta .\nabacas.pl -r tb.fasta -q contigs.fasta -p nucmer -b -d -a -m -N -g sample1 -o sample1_asm\n\n\ncd ~/data/tb/sample1_asm\nbwa index -a is sample1_asm.fasta\nbwa mem -k 20 -c 100 -L 20 -U 20 -M -T 50 sample1_asm.fasta ~/data/tb/sample1_1.fastq.gz ~/data/tb/sample1_2.fastq.gz | samtools sort -o sample1_asm.bam\nsamtools index sample1_asm.bam\n\ncd ~/data/tb\nsamtools view -b sample1.bam Chromosome:79000-84000  | samtools fastq - &gt; region.fastq\n\nspades.py -s region.fastq -o region_assembly\n</code></pre>"},{"location":"commands/gwas/","title":"Gwas","text":"<pre><code>cd ~/data/gwas\nconda activate gwas\nplink --bfile MD --missing --out MD\nplink --bfile MD --het --out MD\nR CMD BATCH imiss-vs-het.Rscript\nR CMD BATCH imiss_het_fail.Rscript\nplink --bfile MD --indep-pairwise 200 5 0.5 --out MD\nplink --bfile MD --extract MD.prune.in --genome --out MD \nperl run-IBD-QC.pl MD\nR CMD BATCH  plot-IBD.Rscript\nR CMD BATCH plot-pca-results.Rscript\nR CMD BATCH write_pca_fail.R\ncat fail*txt | sort -k1 | uniq &gt; fail_qc_inds.txt\nplink --bfile MD --remove fail_qc_inds.txt --make-bed --out clean.MD\nplink --bfile clean.MD --missing --out clean.MD\nR CMD BATCH lmiss-hist.Rscript\nplink --bfile clean.MD --test-missing --allow-no-sex --out clean.MD\nperl run-diffmiss-qc.pl clean.MD\nplink --bfile clean.MD --exclude fail-diffmiss-qc.txt --maf 0.01 --geno 0.05 --hwe 0.00001 --make-bed --out clean.final.MD\nplink --bfile clean.final.MD --assoc --ci 0.95 --adjust --allow-no-sex --out final.MD.assoc\nR CMD BATCH GWAS_plots.R\n</code></pre>"},{"location":"commands/mapping/","title":"Mapping","text":"<pre><code>conda activate mapping\ncd ~/data/tb/\nbwa index ~/data/tb/tb.fasta\n\n# sample1\ntrimmomatic PE sample1_1.fastq.gz sample1_2.fastq.gz -baseout sample1.fastq LEADING:3 TRAILING:3 SLIDINGWINDOW:4:20 MINLEN:36\nbwa mem -R \"@RG\\tID:sample1\\tSM:sample1\\tPL:Illumina\" ~/data/tb/tb.fasta sample1_1P.fastq sample1_2P.fastq | samtools view -b - | samtools sort -o sample1.bam -\nsamtools index sample1.bam\nrm sample1_1P.fastq sample1_1U.fastq sample1_2P.fastq sample1_2U.fastq\n\n# sample2 \ntrimmomatic PE sample2_1.fastq.gz sample2_2.fastq.gz -baseout sample2.fastq LEADING:3 TRAILING:3 SLIDINGWINDOW:4:20 MINLEN:36\nbwa mem -R \"@RG\\tID:sample2\\tSM:sample2\\tPL:Illumina\" ~/data/tb/tb.fasta sample2_1P.fastq sample2_2P.fastq | samtools view -b - | samtools sort -o sample2.bam -\nsamtools index sample2.bam\nrm sample2_1P.fastq sample2_1U.fastq sample2_2P.fastq sample2_2U.fastq\n\ncd ~/data/malaria/\nbwa index ~/data/malaria/Pf3D7_05.fasta\nbwa mem ~/data/malaria/Pf3D7_05.fasta ~/data/malaria/IT.Chr5_1.fastq.gz ~/data/malaria/IT.Chr5_2.fastq.gz | samtools view -b - | samtools sort -o IT.Chr5.bam -\nsamtools index IT.Chr5.bam\nsamtools view IT.Chr5.bam | grep \"IL39_6014:8:61:7451:18170\"\n</code></pre>"},{"location":"commands/nanopore/","title":"Nanopore","text":"<pre><code>conda activate nanopore\ncd ~/data/nanopore_activity/basecalling/raw_fast5_reads/ \nguppy_basecaller --config dna_r9.4.1_450bps_fast.cfg --trim_adapters --compress_fastq  --input_path ~/data/nanopore_activity/basecalling/raw_fast5_reads  --save_path ~/data/nanopore_activity/basecalling/fastq \n\ncd ~/data/nanopore_activity/basecalling/fastq/pass\npycoQC -f ~/data/nanopore_activity/basecalling/fastq/sequencing_summary.txt -o pycoqc_results.html\nporechop -i ~/data/nanopore_activity/basecalling/fastq/pass/*.fastq.gz -o basecalled_reads.porechop.fastq\n\ncd ~/data/nanopore_activity/kraken\nkraken --db ~/data/nanopore_activity/kraken/KDB/ --output temp.krak ~/data/nanopore_activity/kraken/basecalled_reads.porechop.fastq\nrcf -k temp.krak -o rcf.html -n taxdump/\n\ncd ~/data/nanopore_activity/mapping\nminimap2 -ax map-ont ./reference.fasta basecalled_reads.fastq &gt; alignment.sam\nsamtools view -q 15 -b -S alignment.sam &gt; alignment.bam\nsamtools sort alignment.bam -o sorted.bam\nsamtools index sorted.bam\n\ncd ~/data/nanopore_activity/variant_calling \nsamtools depth ~/data/nanopore_activity/mapping/sorted.bam &gt; depth_statistics\n\n# R\n# data&lt;-read.table(\"depth_statistics\")\n# plot(data$V3,type=\"l\",xlab=\"Reference Position\", ylab=\"read Depth\")\n# quit()\n\nbcftools mpileup -q 8 -B -I -Ou -f reference.fasta ~/data/nanopore_activity/mapping/sorted.bam | bcftools call -mv -Oz -o calls.vcf.gz\nbcftools index calls.vcf.gz\nbcftools consensus -f reference.fasta calls.vcf.gz -o consensus_sequence.fasta\n\ncd ~/data/nanopore_activity/phylogenetics\ncat zika_dataset.fasta consensus_sequence.fasta &gt; zika_all.fasta\nmafft zika_all.fasta &gt; zika_all_aligned.fasta\n\niqtree -m GTR+G -s ./zika_all_aligned.fasta -bb 1000\n</code></pre>"},{"location":"commands/phylogenetics/","title":"Phylogenetics","text":"<pre><code>conda activate phylogenetics\ncd ~/data/phylogenetics/\nraxmlHPC -m GTRGAMMA -s H1N1.flu.2009.fas -n H1N1.flu.2009.ML -p 11334 -k -f a -x 13243 -N 100\nmv RAxML_bipartitions.H1N1.flu.2009.ML RAxML_bipartitions.H1N1.flu.2009.ML.tre\n</code></pre>"},{"location":"commands/third-generation-sequencing/","title":"Third generation sequencing","text":"<pre><code>conda activate nanopore\ncd ~/data/nanopore_activity/basecalling/raw_fast5_reads/ \nguppy_basecaller --config dna_r9.4.1_450bps_fast.cfg --trim_adapters --compress_fastq  --input_path ~/data/nanopore_activity/basecalling/raw_fast5_reads  --save_path ~/data/nanopore_activity/basecalling/fastq \n\ncd ~/data/nanopore_activity/basecalling/fastq/pass\npycoQC -f ~/data/nanopore_activity/basecalling/fastq/sequencing_summary.txt -o pycoqc_results.html\nfirefox ~/data/nanopore_activity/basecalling/fastq/pass/pycoqc_results.html &amp;\n\nporechop -i ~/data/nanopore_activity/basecalling/fastq/pass/*.fastq.gz -o basecalled_reads.porechop.fastq\n\ncd ~/data/nanopore_activity/kraken\nkraken --db ~/data/nanopore_activity/kraken/KDB/ --output temp.krak ~/data/nanopore_activity/kraken/basecalled_reads.porechop.fastq\nrcf -k temp.krak -o rcf.html\nfirefox rcf.html &amp;\n\ncd ~/data/nanopore_activity/mapping\nminimap2 -ax map-ont ./reference.fasta basecalled_reads.fastq | samtools view -q 15 -b | samtools sort -o alignment.bam\n\ncd ~/data/nanopore_activity/variant_calling \nsamtools depth ~/data/nanopore_activity/mapping/alignment.bam &gt; depth_statistics\n\nR\ndata&lt;-read.table(\"depth_statistics\")\nplot(data$V3,type=\"l\",xlab=\"Reference Position\", ylab=\"read Depth\")\nquit()\n\nbcftools mpileup -q 8 -B -I -Ou -f reference.fasta ~/data/nanopore_activity/mapping/alignment.bam | bcftools call -mv -Oz -o calls.vcf.gz\nbcftools index calls.vcf.gz\n\nbcftools consensus -f reference.fasta calls.vcf.gz -o consensus_sequence.fasta\n\ncd ~/data/nanopore_activity/phylogenetics\naliview zika_dataset.fasta\ncat zika_dataset.fasta consensus_sequence.fasta &gt; zika_all.fasta\nmafft zika_all.fasta &gt; zika_all_aligned.fasta\niqtree -m GTR+G -s ./zika_all_aligned.fasta -bb 1000 -nt AUTO\n\nfigtree ./zika_all_aligned.fasta.treefile\n</code></pre>"},{"location":"commands/transcriptomics/","title":"Transcriptomics","text":"<pre><code>conda activate rnaseq\ncd ~/data/transcriptomics\n\n\nbwa index H37Rv.fa\n\nbwa mem -t 2 H37Rv.fa Mtb_L1_1.fastq.gz Mtb_L1_2.fastq.gz | samtools sort -@ 2 - -o Mapping_Mtb/Mtb_L1.bam\nsamtools index Mapping_Mtb/Mtb_L1.bam\nbwa mem -t 2 H37Rv.fa Mtb_L4_1.fastq.gz Mtb_L4_2.fastq.gz | samtools sort -@ 2 - -o Mapping_Mtb/Mtb_L4.bam\nsamtools index Mapping_Mtb/Mtb_L4.bam\n\npython -m HTSeq.scripts.count -f bam -r pos -s reverse -t gene ./Mapping_Mtb/Mtb_L1.bam Mtb.gtf &gt; ./Mapping_Mtb/Mtb_L1_htseq_count.txt\npython -m HTSeq.scripts.count -f bam -r pos -s reverse -t gene ./Mapping_Mtb/Mtb_L4.bam Mtb.gtf &gt; ./Mapping_Mtb/Mtb_L4_htseq_count.txt\n\ncd ~/data/transcriptomics/Mapping_Mtb/HTSeqCounts\n\nR\nlibrary(DESeq2)\nlibrary(gplots)\n\ndirectory &lt;- \"~/data/transcriptomics/Mapping_Mtb/HTSeqCounts/\" \n\nsampleFiles &lt;- grep(\"Mtb\", list.files(directory), value = TRUE)\n\nsampleCondition &lt;- c(\"l4\",\"l1\",\"l1\",\"l4\",\"l1\",\"l4\")\n\nsampleTable &lt;- data.frame(\n    sampleName = sampleFiles,\n    fileName = sampleFiles,\n    condition = sampleCondition\n)\n\ndds &lt;- DESeqDataSetFromHTSeqCount(\n    sampleTable = sampleTable,\n    directory = directory,\n    design = ~ condition\n)\n\nkeep &lt;- rowSums(counts(dds)) &gt;= 10\ndds &lt;- dds[keep,]\n\ndds$condition &lt;- factor(dds$condition, levels = c(\"l1\",\"l4\"))\ndds &lt;- DESeq(dds)\nres &lt;- results(dds)\n\nsummary(res)\nres &lt;- res[!is.na(res$padj),]\nresOrdered &lt;- res[order(res$pvalue),]\nresOrdered\n\ncounts_heatmap &lt;- counts(dds, normalized = TRUE)\nidx &lt;- rownames(resOrdered)[1:17]\ncounts_heatmap &lt;- counts_heatmap[rownames(counts_heatmap)%in%idx,]\n\ncounts_heatmap\n\ncolnames(counts_heatmap) &lt;- c(\"L4_1\",\"L1_2\",\"L1_3\",\"L4_4\",\"L1_5\",\"L4_6\")\nheatmap.2(as.matrix(counts_heatmap), scale=\"row\", col=greenred(75), Rowv=NA, dendrogram = \"col\", trace=\"none\", density.info = \"none\")\n</code></pre>"},{"location":"commands/variant-detection/","title":"Variant detection","text":"<pre><code>conda activate variant_detection\n\nmkdir ~/data/tb/variants\ncd ~/data/tb/variants/\n\nbcftools mpileup -B -Q 23 -d 2000 -C 50 -f ~/data/tb/tb.fasta ~/data/tb/sample1.bam | bcftools call --ploidy 1 -m -v -O v - &gt; sample1.raw.vcf\ncat sample1.raw.vcf | vcfutils.pl varFilter -d 10 -D 2000 &gt; sample1.filt.vcf\n\nbcftools mpileup -B -Q 23 -d 2000 -C 50 -f ~/data/tb/tb.fasta ~/data/tb/sample2.bam | bcftools call --ploidy 1 -m -v -O v - &gt; sample2.raw.vcf\ncat sample2.raw.vcf | vcfutils.pl varFilter -d 10 -D 2000 &gt; sample2.filt.vcf\n\n\ncd ~/data/tb/variants/\ngatk HaplotypeCaller -R ~/data/tb/tb.fasta -I ~/data/tb/sample1.bam -O sample1.gatk.raw.vcf -ploidy 1\ngatk HaplotypeCaller -R ~/data/tb/tb.fasta -I ~/data/tb/sample2.bam -O sample2.gatk.raw.vcf -ploidy 1\n\ncd ~/data/tb/variants/\ndelly call -o sample1.delly.bcf -q 20 -s 3 -g ~/data/tb/tb.fasta ~/data/tb/sample1.bam\ndelly call -o sample2.delly.bcf -q 20 -s 3 -g ~/data/tb/tb.fasta ~/data/tb/sample2.bam\n\nbcftools view -i 'FILTER=\"PASS\" &amp;&amp; ALT=\"&lt;DEL&gt;\" &amp;&amp; (END-POS)&lt;20000' -c 2 sample1.delly.bcf &gt; sample1.delly.vcf\nbcftools view -i 'FILTER=\"PASS\" &amp;&amp; ALT=\"&lt;DEL&gt;\" &amp;&amp; (END-POS)&lt;20000' -c 2 sample2.delly.bcf &gt; sample2.delly.vcf\n</code></pre>"},{"location":"introduction/assembly/","title":"Assembly","text":""},{"location":"introduction/assembly/#introduction","title":"Introduction","text":"<p>The main challenge of sequencing a genome is determining how to arrange reads into chromosomes. We have already explored mapping, where the sequence data is aligned to a known reference genome. A complementary technique, where no reference is used or available, is called de novo assembly.</p> <p>De novo assembly is the process of reconstruction of the sample genome sequence without comparison to other genomes. It follows a bottom-up strategy by which reads are overlapped and grouped into contigs. Contigs are joined into scaffolds covering, ideally, the whole of each chromosome in the organism. However de novo assembly from next generation sequence (NGS) data faces several challenges. Read lengths are short and therefore detectable overlap between reads is lower and repeat regions harder to resolve. Longer read lengths will overcome these limitations but this is technology limited. These issues can also be overcome by increasing the coverage depth, i.e. the number of reads over each part of the genome. The higher the coverage then the greater the chance of observing overlaps among reads to create larger contigs and being able to span short repeat regions. In addition, the modern hardware commonly output paired-end reads. These are two reads that are separated by a gap of known size. They enable the resolution of repeats greater than the read length by employing their expected separation and orientation as location constraints. Sequencing errors add difficulty since algorithms must allow certain mismatches when overlapping reads and joining regions, possibly leading to discarding true overlaps and false positives (Miller, Koren, &amp; Sutton, 2010)</p> <p>Despite all mentioned limitations, the high coverage currently achieved, growing read lengths (e.g. 100 read depth, &gt;150bp \u2013 Illumina HiSeq2500) and paired-end information make it feasible to obtain assemblies from small genomes (e.g. bacterial) with relatively low fragmentation. Current assemblers employ graph theory to represent sequences and their overlaps as a set of nodes and edges, being classified into three main groups: greedy, Overlap/Layout/Consensus (OLC) and de Bruijn graph assemblers</p> <p>Graph theory is a branch of discrete mathematics that studies problems of graphs. Graphs are sets of points called \u2018vertices\u2019 or \u2018nodes\u2019 joined by lines called \u2018edges\u2019. In the graph to the left there are 6 nodes and 7 edges. The edges are unidirectional which means that they can only be traversed in the direction of the arrow. For example paths in this graph include 3-2-5-1, 3-4-6 and 3-2-1. </p> <p></p> <p>The greedy algorithms consist of progressively adding single reads into contigs by end-to-end overlapping, starting with those reads with the highest overlap score and ending once no more joins can be found. Such scores are normally measured as the overlap length or the percentage of identity between reads along their joining region. SSAKE (Warren, Sutton, Jones, &amp; Holt, 2007), the first short-read assembler, is based on this approach as well as its two descendants SHARCGS (Dohm, Lottaz, Borodina, &amp; Himmelbauer, 2007) and VCAKE (Jeck et al., 2007).</p> <p>OLC assemblers build an overlap graph, in which nodes represent the reads and edges the overlaps. It requires a very time-consuming first step, where all reads are compared against each other. Paths along the graph show likely contigs.</p> <p>These two algorithms (greedy and OLC) are more suited to fewer, longer reads than those generated with NGS technologies. De Bruijn graph assemblers are the state-of-the-art approach for data sets composed of many thousands of short reads.</p> <p></p> <p>De Bruijn graph assemblers start by splitting the set of reads into k-mers, a set of overlapping sub-reads, and then use the latter to build the graph. Each edge represents an observed k-mer and its adjacent nodes the prefix and suffix of the original k-mer. Therefore, groups of overlapping reads are not actually computed but rather represented as paths in the graph. In the figure above the sequence ATGGCGTGCA with 3-mers and overlap of 2 base pairs (bp).</p> <p>Since edges correspond to all k-mers existing in the sampled genome, the assembly is resolved by finding a path that visits every edge in the graph. This approach is especially suitable to handle the large number of reads because each k-mer is stored at most once regardless of how many times it occurs in the reads. Several programs implement de Bruijn graph algorithms, including Euler (Pevzner, Tang, &amp; Waterman, 2001), Velvet (Zerbino &amp; Birney, 2008), ABySS (Simpson et al., 2009), AllPaths (Butler et al., 2008) and SOAPdenovo (Li et al., 2010).</p> <p>In conclusion, the de Bruijn graph assemblers are more appropriate for large amounts of data from high-coverage sequencing and have become the programs of choice when processing short reads produced by Illumina and other established platforms (&gt;100bp).</p>"},{"location":"introduction/assembly/#exercise-1-de-novo-assembly-using-spades","title":"Exercise 1: De Novo Assembly using Spades","text":"<p>De novo assembly is one the most computationally demanding processes in bioinformatics. Large genomes require many hours or days of processing. A small bacterial genome may take up to several hours to assemble. Here we will assemble M. tuberculosis genomes using Spades (Bankevich et al, 2012). We will compute assembly statistics to check the quality, and review how resulting contigs can be aligned, ordered and orientated along the reference genome using Abacas (Assefa, Keane, Otto, Newbold, &amp; Berriman, 2009).</p>"},{"location":"introduction/assembly/#quick-start-and-data-checks","title":"Quick start and data checks","text":"<p>Activate the relevant conda environment</p> <pre><code>conda activate day1\n</code></pre> <p>Change to the data directory:</p> <pre><code>cd ~/data/tb\n</code></pre> <p>And list the files there:</p> <pre><code>ls\n</code></pre> <p>There are 6 fastq files in this directory; these are the input data for this practical. There are paired-end reads for three samples. To view the first 8 lines of a fastq file type:</p> <pre><code>zcat sample1_1.fastq.gz | head -8\n</code></pre> <p>Terminal output</p> <pre><code>@HS3_5961:1:2207:17088:52965#1/1\nGCCGGTTGTTCGGCTGGAAGGTCCTTTTGCCCTTGGTCACGGGCGTCTCCTCGCTATGTCTGGCAACATCACCAT\n+\nGGGGGGEGFGFGGGFGDBEBF:AFDEFG?CFFDD:FBEE?EFEDFDEFEAEBFB8CCC?A@CD;ADDD86CCC&gt;&lt;\n@HS3_5961:1:1208:19785:88067#1/1\nCTTAGGGTCGCCGTTAAGTTCGGAGACGACCGCGTTCCACACTGTGGTGAAGCCTGAACCGGGGTCATCGGTCAA\n+\nHHHHFFHBHHHHHHHEGDDDGGEGFHHHHHHGHHHDFFEFFFDD&gt;DB@CDAEFF?FBFGDDCCC.=?5=&lt;@@;DA\n</code></pre> <p>Each read consists of 4 lines, the first is the read name, the second are the bases read, the third line is ignored and the fourth is the encoded quality of each read (one character per base).</p> <p>To see the number of lines in a .fastq.gz file we can run the following command: </p> <pre><code>zcat sample1_1.fastq.gz | wc -l\n</code></pre> <p>and therefore the number of reads can be calculated by dividing by 4. The length of the first read can be calculated by running:</p> <pre><code>zcat sample1_1.fastq.gz | head -2 | tail -1 | wc -c\n</code></pre> <p>This gives the number of characters in the line including the new line character, so it gives a number one greater than the first read length.</p>"},{"location":"introduction/assembly/#running-spades","title":"Running Spades","text":"<p>Spades is implemented in a single program that performs several steps in the assembly pipeline with a single command. The options are explained by running the command with no parameters (spades.py):</p> <p>You will see that there are many options and the pipeline can be customised extensively. However, for most purposes we can run the tool with the default settings. You only need to supply the reads and an output directory.</p> <p>An example invocation that takes the fastq formatted paired end files (Mtb_sample1_1.fastq.gz and Mtb_sample1_2.fastq.gz) and performs assembly.</p> <pre><code>spades.py --isolate -1 ~/data/tb/sample1_1.fastq.gz -2  ~/data/tb/sample1_2.fastq.gz -o sample1_asm\n</code></pre> <p>Employing paired-end reads (rather than single-end) increases the assembly quality. In general, libraries with smaller insert size produce more fragmented and shorter assemblies, whilst the genome coverage does not increase that much. The combination of libraries with different insert size always gives the best results.</p> <p>De novo assembly gives much better results if reads are aggressively pre-filtered before passing them to the assembler. This is the same as we have seen for mapping and variant detection earlier in the course.</p> <p>Although anecdotal recommendations for the k-mer length range from lower than half of the read length up to 80% of read length, the parameter is difficult to determine in advance since it depends on coverage, read length, error rates and the sample properties (repeat regions, etc). Therefore, it is advisable to perform several assemblies using different values of k-mer length to decide a value.</p> <p>Thankfully spades automatically chooses a range of k-mer lengths to test and chooses the best one. The pipeline also performs reads correction, which aims to correct random errors in reads which will have a knock on effect on the k-mers used in the assembly.</p> <p>Eventually, after spades finishes running there will be several files will be obtained in the directory the most important for our purposes are:</p> <ul> <li>A fasta file (contigs.fasta) containing all assembled contigs</li> <li>A fasta file (scaffolds.fasta) containing all assembled scaffolds</li> </ul> <p>Scaffolds are contigs that have been chained together into larger sequences using information such from mate pair libraries and reference sequences. The scaffolds should be very similar to the contigs in this assembly because the insert sizes of our library are relatively small. For the rest of this practical we will only be analysing the contigs. </p>"},{"location":"introduction/assembly/#assembly-quality","title":"Assembly quality","text":"<p>The outcome of de novo assembly is a set of contigs. In a perfect assembly each contig covers exactly one chromosome in its entirety. In practice many contigs are created and the assembly quality is measured by examining their length; individually and in combination. An assembly is considered to be of good quality if relatively few long contigs are obtained covering the majority of the original or expected genome. There are different metrics used to assess the quality of assemblies:</p> <ul> <li>N50. The length of the contig that contains the middle nucleotide when the contigs are ordered by size. Note, N80 or N60 may also be used.</li> <li>Genome coverage. If a genome reference exists then the genome coverage can be computed as the percentage of bases in the reference covered by the assembled contigs.</li> <li>Maximum/median/average contig size. Usually computed after removing the smallest contigs (e.g. discarding all contigs shorter than 150 bp).</li> </ul> <p>We can compute these statistics with the help of a tool called quast (made by the same developers as spades). To get the statistics first navigate to the assembly directory and run quast </p> <pre><code>cd sample1_asm\nquast -r ../tb.fasta -o quast contigs.fasta \n</code></pre> <p>After it has finished you can examing the outputs. To view the report enter <code>cat quast/report.txt</code>. </p> <p>Question</p> Question 1Answer 1 <p>See if you can spot the statistics we mentioned above. Compare the size of the reference with the size of the assembly. Are they different?</p> <p>Quast report includes information on statistics N50, Genome fraction and Maximum contig size that were mentioned before as well as additional metrics worth exploring. Average contig size can be additionally calculated by dividing values from fields Total length (&gt;= 0 bp) and # contigs (&gt;= 0 bp) or directly from FASTA file with external tools for example seqmagick. It is advised to look at a variety of metrics and supporting analyses to determine quality of assembly. The total size of reference genome (1000020 bp) and resulting genome assembly (1005736 bp) are comperable.</p> <p></p> <p>High depth of coverage is essential to obtain high quality assembled genomes. It is always the case that contig sizes drop when coverage decreases under a certain value, generally considered to be 50x, although such threshold will depend on the size and repeat content of the sequenced genome. A coverage limit is reached above which no improvement on assembly metrics is observed. As shown to the left, coverage greater that 50x does not significantly improve N50 in an assembly of E. coli. Although such thresholds will not necessarily be the same for other genomes, the principles apply for all.</p> <p>More data requires more memory too, as the lower graph demonstrates. Again the figures are genome specific but it is easy to see how large computers can quickly become necessary as coverage and genome size increase. (Illumina, 2009)</p>"},{"location":"introduction/assembly/#exercise-2-contig-ordering","title":"Exercise 2: Contig Ordering","text":""},{"location":"introduction/assembly/#ordering-with-abacas","title":"Ordering with ABACAS","text":"<p>To align, order and orientate contigs along the reference genome we will be using ABACAS (Assefa et al., 2009). The output will be in the form of a pseudo molecule, where all of the contigs will be joined by N\u2019s. To display the complete list of possible parameters by typing:</p> <pre><code>abacas.pl \u2013h\n</code></pre> <p>The basic usage (for this exercise) is:</p> <pre><code>cp ~/data/tb/tb.fasta .\nabacas.pl -r tb.fasta -q contigs.fasta -p nucmer -b -d -a -m -N -g sample1 -o sample1_asm\n</code></pre> <p>This will create a pseudomolecule (into sample1_asm.fasta) from the assembled contigs (in contigs.fa). The program creates a number of files with the same base filename for the other requested processing output, including gap information (-g, sample1_asm.gaps, sample1_asm.gaps.tab), a comparison file (sample1_asm.crunch), no padding version (-n, sample1_asm.NoNs.fasta), etc.</p>"},{"location":"introduction/assembly/#visualisation-with-act","title":"Visualisation with ACT","text":"<p>Artemis Comparison Tools (ACT) (Carver et al., 2005) is a tool for displaying pair-wise comparison between two or more DNA sequences. It was developed to help identify and analyse regions of similarity and difference between genomes along their entire sequences and their annotation. ABACAS produces the required files to compare the original reference genome with the pseudo molecule resulting after ordering assembled contigs.</p> <p>Launch ACT by running:</p> <pre><code>act\n</code></pre> <p>Click File &gt; Open. Select the reference fasta file (tb.fasta) and the pseudomoelcule (sample1_asm.fasta) as the two sequence files with the associated comparison file (sample1_asm.crunch) \u2013 you will have to navigate the directories. Then press Apply. </p> <p></p> <p>A map between the two sequences will be displayed. To load annotation click on File &gt; tb.fasta &gt; Read An Entry... and select the tb.gff file from the ~/data/tb/ directory. To make stop codons disappear (represented as black vertical lines) right-click on the reference and the pseudo molecule panels and deselect Stop Codons option in the right-click menu.</p> <p>Right-click on one of the red ribbons linking the contigs and select View Selected Matches to find out the first and last position the contig maps to the reference and to the pseudo molecule. The percentage of identity between them is also shown.</p> <p></p>"},{"location":"introduction/assembly/#exercise-3-mapping-back-to-the-reference","title":"Exercise 3: Mapping back to the Reference","text":"<p>Contigs aligning with large internal gaps or low confidence indicate either structural variants or misassemblies, and should be carefully monitored. If paired-end reads are available then mapping them back onto the contigs (pseudo molecule) is another source of information. Large deviations in the mapping distance or read pairs in which only one read maps against the contigs may hint at misassemblies rather than structural variation</p> <p>The following commands maps the raw reads against the pseudo molecules from the previous exercise. This is a similar process to those you have used in the previous classes.</p> <pre><code>cd ~/data/tb/sample1_asm\nbwa index -a is sample1_asm.fasta\nbwa mem -k 20 -c 100 -L 20 -U 20 -M -T 50 sample1_asm.fasta ~/data/tb/sample1_1.fastq.gz ~/data/tb/sample1_2.fastq.gz | samtools sort -o sample1_asm.bam\nsamtools index sample1_asm.bam\n</code></pre> <p>This process indexes, maps and stores the fastq paired-end reads against the ABACAS ordered pseudomolecule (sample1/sample1_asm.fasta) for sample1 created by spades. The output is a BAM file (sample1/sample1_asm.bam).</p> <p>This script can be modified to create a BAM file for the sequence data mapped against any reference genome, just as you have done in the previous class.</p> <p>These files can be visualised in ACT. File &gt; sample1_asm.fasta &gt; Read BAM/VCF... and choose the remapped.bam file (sample1_asm.bam). Do the same for tb.fasta and choose the originally mapped BAM file (sample1.bam). These BAM data are represented as pileup data \u2013 each read stacked up over the section of the reference that it has been mapped to. Extra information can be presented by right-clicking on the pileups and selecting Graph &gt; Coverage or View &gt; Inferred Size.</p> <p>The graph shown gives a good insight into the quality and depth of the assembly and is very useful for finding regions that have been misaligned (i.e. joined in error) as their coverage should be very low or very high.</p> <p></p> <p>Question</p> Question 2Answer 2 <p>In ACT inspect the region between 76,000 and 89,000. How does the coverage vary across the pseudo molecule? </p> <p>Coverage across the pseudo molecule is sufficient across most regions. There are few visible fragmentations for which coverage is not existent that should be expected from the nature of the assembly.</p> <p>Question</p> Question 3Answer 3 <p>How does this compare to the coverage across the matching regions mapped against the reference? </p> <p>Overall coverage trends in comparison to original mapping assembly are similar. In the region of interest, we can notice pseudo molecule having uniform coverage whilst in mapping assembly the lack of supporting reads spanning between 795000-83000.</p> <p>Question</p> Question 4Answer 4 <p>Is there any kind of structural variant involved? </p> <p>A few regions based on the coverage distribution might indicate the existence of structural variants that should be investigated further.</p>"},{"location":"introduction/assembly/#exercise-4-structural-variant-validation","title":"Exercise 4: Structural Variant Validation","text":"<p>Structural variant (SV) software aims to detect signatures left by changes in chromosome material and chromosome rearrangements. However, reads mapping to repetitive or highly polymorphic regions may mimic the same signatures (false positives).</p> <p>Here we describe a de novo assembly approach that we call targeted assembly of candidate SV regions. It consists of obtaining the reads from the predicted SV region, performing de novo assembly and mapping the resulting contig back to the reference. The nature of the contig alignment can reveal the SV.</p> <p>We have previously identified a deletion in sample1 in the region 79570-83050 using delly. This region has been investigated above, in the context of a large assembly, and interesting mapping and assembly results have been found.</p> <p>The reads are extracted from the BAM file using samtools. Extracting a wider range (by at least 400bp) than the candidate region enables spades to construct good contigs either side of the possible SV.</p> <pre><code>cd ~/data/tb\nsamtools view -b sample1.bam Chromosome:79000-84000  | samtools fastq - &gt; region.fastq\n</code></pre> <p>The next step is to perform de novo assembly of the extracted reads. </p> <pre><code>spades.py -s region.fastq -o region_assembly\n</code></pre> <p>We map the output contigs back to the reference with BLAST. This powerful alignment algorithm is available online or for download as a command line tool. Go to the web address http://blast.ncbi.nlm.nih.gov/Blast.cgi and navigate to the Nucleotide BLAST page. Check the \u201cAlign two or more sequences\u201d box. Upload the contigs file (~/data/tb/region_assembly/contigs.fasta) as the subject and the tb reference (~/data/tb/tb.fasta) as the query. Enter an appropriate region of the reference to blast against as there is little point in searching the entire genome when we are only interested in a small range (79100-83900).</p> <p>See screenshots below showing the blast Alignment input screen. Choose the appropriate reference and sample sequences, enter the range of interest and click BLAST.</p> <p></p> <p>Once the server has blasted your data, it will create a web page for the results with graphical and textual information on how well the sequences aligned.</p> <p></p> <p>Question</p> Question 4Answer 4 <p>Is it a deletion? Or was it a false positive? Using the output from delly in the variant detection practical have a look at other SVs as well. </p> <p>BLAST Graphic Summary of the resulting query contains a few rectangles (size correlates with alignment length) that represent regions of the sequence that maps to the reference. Rectangles joined by a thin line show that the contig sequence was split during alignment. It is indicating that the reads assembled into this single contig needed to be separated by a gap when aligning to the reference \u2013 therefore we can conclude a deletion compared to the reference. In order to validate in-depth deletion additional information could be extracted from long-read sequencing to prove or disprove the hypothesis.</p> <p>Shorter rectangles are matches coming from other smaller assembled contigs that may potentially reveal alternative areas of similarity or small repetitive sequences.</p>"},{"location":"introduction/assembly/#advanced-topics","title":"Advanced Topics","text":""},{"location":"introduction/assembly/#annotation-transfer","title":"Annotation Transfer","text":"<p>Once the contigs ordered are against the reference, it is useful to determine the position and function of possible genes. It is possible to implement ab initio gene finding, but an alternative approach is to use the annotation of the reference and adapt it to the new assembly. A tool called RATT (Otto et al., 2011 \u201cRapid annotation transfer tools\u201d) has been developed to transfer the annotation from a reference to a new assembly. In the first step the similarity between the two sequences is determined and a synteny map is constructed. This map is used to align the annotation of the reference onto the new sequence. In a second step, the algorithm tries to correct gene models. One advantages of RATT is that the complete annotation is transferred, including descriptions. Thus careful manual annotation from the reference becomes available in the newly sequenced genome. Where no synteny exists, no transfer can be performed. An outputted combined \u201cembl\u201d format file with sequence and inferred annotation can be viewed in artemis to assess the quality of the transfer.</p>"},{"location":"introduction/assembly/#improving-assemblies","title":"Improving assemblies","text":"<p>The quality of the assembly is determined by a number of factors, including the type of software used, the length of reads and the repetitive nature or complexity of the genome. For example, assembly of Plasmodium is more difficult than that of most bacteria (due to the high AT content and repeats). A good assembly of a bacterial genome will return 20-100 supercontigs. It is possible to manually / visually improve assemblies and annotation within Artemis. To improve the assembly in a more automated fashion, there is other software:</p> <p>SSPACE: This tool can scaffold contigs. Although spades can also scaffold contigs, SSPACE generally performs better.</p> <p>Abacas: has the option to design primers that can be used to generate a PCR product to span a possible gap. The resulting new sequence can then be included in the assembly. This process is called finishing.</p> <p>Image: is a tool that can close gaps in the assembly automatically. First the reads are mapped against the assembly. Using all reads that map close to a gap and with their mate pairs within the gap, it is possible to perform a local assembly. This process is repeated iteratively. This procedure can close up to 80% of the sequencing gaps (Tasi et al, 2010).</p> <p>iCORN (Otto et al, 2010) can correct base errors in the sequence. Reads are mapped against the reference and differences are called. Those differences or variants that surpass a specified quality threshold are corrected. A correction is accepted if the amount of perfect mapping reads does not decrease. This algorithm also runs iteratively. Here, perfect mapping refers to the read and its mate pair mapping in the expected insert size without any difference to the iteratively derived reference.</p> <p>All these programs are available through the PAGIT suite (post assembly genome improvement toolkit, http://www.sanger.ac.uk/resources/software/pagit/).</p>"},{"location":"introduction/assembly/#references","title":"References","text":"<p>Assefa, S., Keane, T. M., Otto, T. D., Newbold, C., &amp; Berriman, M. (2009). ABACAS: algorithm-based automatic contiguation of assembled sequences. Bioinformatics (Oxford, England), 25(15), 1968-9. doi:10.1093/bioinformatics/btp347</p> <p>Butler, J., MacCallum, I., Kleber, M., Shlyakhter, I. A, Belmonte, M. K., Lander, E. S., Nusbaum, C., et al. (2008). ALLPATHS: de novo assembly of whole-genome shotgun microreads. Genome research, 18(5), 810-20. doi:10.1101/gr.7337908</p> <p>Carver, T. J., Rutherford, K. M., Berriman, M., Rajandream, M.-A., Barrell, B. G., &amp; Parkhill, J. (2005). ACT: the Artemis Comparison Tool. Bioinformatics (Oxford, England), 21(16), 3422-3. doi:10.1093/bioinformatics/bti553</p> <p>Compeau, P.E.C., Pevzner, P.A., Tesler, G. (2007) How to apply de Bruijn graphs to genome assembly. Nature Biotechnology 29(11) 987-991</p> <p>Dohm, J. C., Lottaz, C., Borodina, T., &amp; Himmelbauer, H. (2007). SHARCGS, a fast and highly accurate short-read assembly algorithm for de novo genomic sequencing. Genome research, 17(11), 1697-706. doi:10.1101/gr.6435207</p> <p>Illumina, I. (2009). De Novo Assembly Using Illumina Reads. Analyzer. Retrieved from http://scholar.google.com/scholar?hl=en&amp;btnG=Search&amp;q=intitle:De+Novo+Assembly+Using+Illumina+Reads#0</p> <p>Jeck, W. R., Reinhardt, J. a, Baltrus, D. a, Hickenbotham, M. T., Magrini, V., Mardis, E. R., Dangl, J. L., et al. (2007). Extending assembly of short DNA sequences to handle error. Bioinformatics (Oxford, England), 23(21), 2942-4. doi:10.1093/bioinformatics/btm451</p> <p>Li, R., Zhu, H., Ruan, J., Qian, W., Fang, X., Shi, Z., Li, Y., et al. (2010). De novo assembly of human genomes with massively parallel short read sequencing. Genome research, 20(2), 265-72. doi:10.1101/gr.097261.109</p> <p>Margulies, M., Egholm, M., Altman, W. E., Attiya, S., Bader, J. S., Bemben, L. a, Berka, J., et al. (2005). Genome sequencing in microfabricated high-density picolitre reactors. Nature, 437(7057), 376-80. doi:10.1038/nature03959</p> <p>Miller, J. R., Koren, S., &amp; Sutton, G. (2010). Assembly algorithms for next-generation sequencing data. Genomics, 95(6), 315-27. Elsevier Inc. doi:10.1016/j.ygeno.2010.03.001</p> <p>Pevzner, P. a, Tang, H., &amp; Waterman, M. S. (2001). An Eulerian path approach to DNA fragment assembly. Proceedings of the National Academy of Sciences of the United States of America, 98(17), 9748-53. doi:10.1073/pnas.171285098</p> <p>Simpson, J. T., Wong, K., Jackman, S. D., Schein, J. E., Jones, S. J. M., &amp; Birol, I. (2009). ABySS: a parallel assembler for short read sequence data. Genome research, 19(6), 1117-23. doi:10.1101/gr.089532.108</p> <p>Warren, R. L., Sutton, G. G., Jones, S. J. M., &amp; Holt, R. a. (2007). Assembling millions of short DNA sequences using SSAKE. Bioinformatics (Oxford, England), 23(4), 500-1. doi:10.1093/bioinformatics/btl629</p> <p>Zerbino, D. R., &amp; Birney, E. (2008). Velvet: algorithms for de novo short read assembly using de Bruijn graphs. Genome research, 18(5), 821-9. doi:10.1101/gr.074492.107</p> <p>Acknowledgements: Thomas Otto and Wellcome Trust.</p>"},{"location":"introduction/intro-to-linux/","title":"Linux","text":"<p>Most bioinformatic software is written for Unix-type systems and is usually operated from the command line. This means that these programs are intended to be used by typing commands and they don't have a graphical user interface (GUI). There are many reasons for this, but two important ones are that command line programs are quicker to develop and also very easy to automate. To get from raw sequences to final results often requires many steps. Imagine processing thousands of isolates: this would potentially involve hundreds of hours and even more mouse clicks if performed with a GUI. Using a command line tool, a workflow can be defined by chaining a series of commands together in a script which can then be rapidly applied to thousands of isolates without much effort. </p> <p>Over the duration of this course you will be learning the skills required to independently undertake bioinformatic projects. By the end of this course you should be comfortable to go all the way from raw data to biological insights.</p> <p>In order to learn how to use bioinformatic programs we must first become comfortable with how to use the terminal. This may be daunting if you have never used a command line environment before, but hopefully by the end of this practical session you should have a basic understanding which will be reinforced over the next few sessions.</p> <p>Let's start by opening a terminal by clicking on the icon on the sidebar. Your screen should now look like the one below:</p> <p></p> <p>The terminal is very similar to the file explorer in other operating systems. It allows you to see files and interact with them. The only difference is that we have to do this by typing a command into the terminal window. To demonstrate this, let's open up Linux's file explorer by clicking on the icon in the side-bar on the left-hand side of the screen. When you open the file explorer it will go to your home folder and show you all the sub-folders and files present. The home folder is simply the root folder which contains all your files and folders.</p> <p>You can see a number of sub-folders and files present in the file explorer. Let's see if we can find them in the terminal. To do this, we have to type ls into the terminal and hit enter. You should now see a screen similar to the one below. Now you can see the same files and folders in the terminal. </p> <p></p> <p>In the file explorer double click on the data folder. You will see several new folders which hold the data for the proceeding practicals. Let's try find them using the terminal!</p> <p>When we double clicked on the data folder we effectively moved from the home folder to the data folder. We will have to do the same using the terminal. To do this, type in <code>cd data</code> and hit enter. Now try using ls again to produce the same list of folders that you see in the file explorer.</p> <p>Info</p> <p>Folders are often referred to as directories in Linux. We will use directory from now on, but the two words are synonymous. </p> <p>You will notice that text in front of the $ sign changed when you moved from home to data. You should see something similar to what is shown below: </p> <p>Terminal output</p> <pre><code>user@user-VirtualBox:~/data$ \n</code></pre> <p>The blue text shows the location where the terminal is currently. In this case we are in <code>~/data</code>. Another name for this is the current directory. The <code>~</code> character is a special character which symbolises your home directory. So, we can interpret this as \"We are currently in the data directory which is in the home directory\"</p> <p>Question</p> <p>Another way of finding the current location of the terminal is by using pwd which stands for print working directory. Try using this command. Does it produce a similar output to what we have seen above? </p> <p>We have now successfully used the terminal and the <code>ls</code>, <code>cd</code> and <code>pwd</code> commands. It is as simple as that! Some commands can be used by themselves such as <code>ls</code>, however others such as <code>cd</code> need additional information called arguments to do something useful. In the example above, we need to tell <code>cd</code> that we wanted to go to the <code>data</code> directory, so we typed <code>data</code> after <code>cd</code> separated by a space. While <code>cd</code> only took one argument, some programs take many more. You will see examples of this in a bit.</p> <p>Now that you have mastered using the terminal you can close the file explorer, this is the last time we will use it. We must now face using the terminal! </p>"},{"location":"introduction/intro-to-linux/#useful-commandsprograms","title":"Useful commands/programs","text":""},{"location":"introduction/intro-to-linux/#head","title":"head","text":"<p>Change to the tb directory and have a look at the files. Hopefully you will be able to see the tb.fasta file. The file is just a very large text file which stores the sequence data of the M. tuberculosis reference genome. We can use <code>head</code> to take a look at the first few lines of a file. Let's try it: </p> <pre><code>cd ~/data/tb\nls\nhead tb.fasta\n</code></pre> <p>Hopefully you will see something like this: </p> <p>Terminal output</p> <pre><code>&gt;Chromosome\nTTGACCGATGACCCCGGTTCAGGCTTCACCACAGTGTGGAACGCGGTCGTCTCCGAACTTAACGGCGACC\nCTAAGGTTGACGACGGACCCAGCAGTGATGCTAATCTCAGCGCTCCGCTGACCCCTCAGCAAAGGGCTTG\nGCTCAATCTCGTCCAGCCATTGACCATCGTCGAGGGGTTTGCTCTGTTATCCGTGCCGAGCAGCTTTGTC\nCAAAACGAAATCGAGCGCCATCTGCGGGCCCCGATTACCGACGCTCTCAGCCGCCGACTCGGACATCAGA\nTCCAACTCGGGGTCCGCATCGCTCCGCCGGCGACCGACGAAGCCGACGACACTACCGTGCCGCCTTCCGA\nAAATCCTGCTACCACATCGCCAGACACCACAACCGACAACGACGAGATTGATGACAGCGCTGCGGCACGG\nGGCGATAACCAGCACAGTTGGCCAAGTTACTTCACCGAGCGCCCGCACAATACCGATTCCGCTACCGCTG\nGCGTAACCAGCCTTAACCGTCGCTACACCTTTGATACGTTCGTTATCGGCGCCTCCAACCGGTTCGCGCA\nCGCCGCCGCCTTGGCGATCGCAGAAGCACCCGCCCGCGCTTACAACCCCCTGTTCATCTGGGGCGAGTCC\n</code></pre> <p>We can see that by default <code>head</code> prints out the first 10 lines. We can modify this number by providing an additional parameter. </p> <pre><code>head -5 tb.fasta\n</code></pre> <p>This command it will print out the first five lines instead. Here we have used an optional parameter (the program will still function properly without it). Optional parameters are usually given by providing two values, in this case <code>-n</code> and <code>5</code>. The first value specified what parameter we want to provide (e.g. <code>-n</code> = number of lines) and the second value is that which the parameter must take (e.g. <code>5</code> will tell the command to print the first five lines). </p>"},{"location":"introduction/intro-to-linux/#less","title":"less","text":"<p>Another tool that we can use to view text files is the <code>less</code> command. Let's try to view the reference file again with this method. </p> <pre><code>less tb.fasta\n</code></pre> <p>This will open an interactive viewer which fills up the screen. You can move up and down the file using the up and down keys on the keyboard. When you are finished viewing, hit the <code>q</code> key to quit the viewer. </p>"},{"location":"introduction/intro-to-linux/#cp","title":"cp","text":"<p>If we need to copy a file we can use the <code>cp</code> command. We need to give two arguments: the file we want to copy and the filename of the copy. Let's try it: </p> <pre><code>cp tb.fasta tb_copy\n</code></pre> <p>Info</p> <p>Two things to note here:</p> <ol> <li>Spaces are not allowed in file names! Always use an underscore instead of a space.</li> <li>We have not given our new file an extension. Extensions are useful as they tell us something about how a file is formatted and what it might contain. If we come back a year later, we might have forgotten that <code>tb_copy</code> contained sequence data.</li> </ol>"},{"location":"introduction/intro-to-linux/#mv","title":"mv","text":"<p>Let's give our file an extension. To do this we have to rename the file. The command for this is <code>mv</code> which stands for move. This command allows you to both move and rename files. Try to add an extension to the file by using the following code: </p> <pre><code>mv tb_copy tb_copy.fasta\n</code></pre> <p>You can see that this command also required two parameters:</p> <ol> <li>The file we want to rename</li> <li>The new file name</li> </ol>"},{"location":"introduction/intro-to-linux/#rm","title":"rm","text":"<p>We now have two files with the same content. Let's remove our copy of the reference by using the <code>rm</code> command: </p> <pre><code>rm tb_copy.fasta\n</code></pre> <p>Danger</p> <p>Be very careful with this command! Once you remove a file using rm there is no way of getting it back. Always double check you are removing the right file before hitting enter. </p>"},{"location":"introduction/intro-to-linux/#pipes","title":"Pipes","text":"<p>Imagine a cake factory production line. There are many steps that need to be taken to build the final cake (adding different ingredients). Let's say we have 5 people working to make 100 cakes per day and each person specialises in adding a particular ingredient. We could get 100 bowls and add the flour to each one, then add eggs to each one and repeat this with each ingredient until we have the complete mix however we would be losing a lot of time as at any point there would be only one person working while the others wait for their turn. A more efficient way to do this by installing in a conveyor belt and having the employees sequentially add their ingredients to each bowl. This way everyone is working at the same time and the cakes will be made a lot faster. </p> <p></p> <p>The same principals apply to bioinformatic analyses. For example, take a look at the steps required to print the 9th and 10th line of the tb reference fasta file. The easiest way to do this is to:</p> <ol> <li>first extract the first 10 lines</li> <li>take the last two lines of that extract</li> </ol> <p>There are two jobs to be done and the first job passes its data to the second job. This is a good place to use pipes. We can use <code>head</code> to get the first 10 lines as we have done above. We will also intoduce <code>tail</code> which can be used to print the last lines. Try running this command:</p> <pre><code>head -n 10 tb.fasta | tail -n 2\n</code></pre>"},{"location":"introduction/intro-to-linux/#troubleshooting","title":"Troubleshooting","text":"<p>Sometimes an error may occur while trying to run a command. There may be a number of different reasons for this and troubleshooting is an important skill to master. </p> <p>There are a number of ways to check if your commands have failed including:</p> <ol> <li>Run time: Some commands (such as alignment) are expected to run for at least a few minutes. If the command finished instantly and you do not expect it to do so then something has probably gone wrong.</li> <li>Error messages: Usually if a command has failed it will print out an error message. Check the last few lines of output from a program and check to see if you see any.</li> </ol> <p>There are some errors which are are commonly made. We have listed a few below. </p>"},{"location":"introduction/intro-to-linux/#file-or-folder-not-present","title":"File or folder not present","text":"<p>These types of error may present in different ways depending on the command used. For example, let's try change to a directory that doesn't exist. </p> <pre><code>cd fake_directory\n</code></pre> <p>You should see something like this: </p> <p>Terminal output</p> <pre><code>cd: fake_directory: No such file or directory \n</code></pre> <p>This tells us that <code>cd</code> could not find the directory called <code>fake_directory</code>. </p> <p>Info</p> <p>Remember that the terminal is case sensitive and will also not tolerate any spaces in places where they shouldn't be. For example try running this command and see what you get:</p> <pre><code>head TB.fasta\n</code></pre> <p>Can you fix it?</p>"},{"location":"introduction/intro-to-linux/#missing-a-required-argument","title":"Missing a required argument","text":"<p>If you run a program and fail to specify an arguement it may give an error. Alternatively, it may display the program usage help and quit. For example, lets run <code>head</code> without passing the number of lines to the `-n`` flag: </p> <pre><code>head -n tb.fasta\n</code></pre> <p>You should see an error similar to that displayed below: </p> <p>Terminal output</p> <pre><code>head: invalid number of lines 'tb.fasta' \n</code></pre> <p>The error is a bit cryptic but from the illegal line count, we can see that whatever parameter we passed to <code>-n</code> (which determines how many lines are printed) is not a valid value. It turns out that this value always has to be a number, however since we left out that parameter it parses tb.fasta as the value and that causes the error. </p>"},{"location":"introduction/intro-to-linux/#command-not-found","title":"Command not found","text":"<p>The different software used for the practicals are installed in \"conda environments\". These allow to have multiple versions of the same program to be installed. If you run into a <code>command not found</code> error, it is always a good idea to check if the correct conda environment for the current practical is activated (the name of the currently active environment is usually printed in parenthesis to the left of your prompt). You can activate an environment with </p> <pre><code>conda activate environment_name\n</code></pre> <p>Make sure to replace \"environment_name\" with your chosen environment. You can check which environments are available with:</p> <pre><code>conda env list\n</code></pre> <p>Question</p> <p>Try activate the tb-profiler environment. Did it work?</p>"},{"location":"introduction/mapping/","title":"Mapping","text":"<p>Next-generation sequencing data is being produced at an ever-increasing rate. The raw data is not meaningful by itself and needs to be processed using various bioinformatic software. This practical will focus on genomic resequencing data where the raw data is aligned to a reference genome.</p>"},{"location":"introduction/mapping/#introduction","title":"Introduction","text":"<p>Improvements in DNA sequencing technology have led to new opportunities for studying organisms at the genomic and transcriptomic levels. Applications include studies of genomic variation within species and gene identification. In this module we will concentrate on data generated using the Illumina Genome Analyzer II technology, although the techniques you will learn are applicable to other technologies (e. g. 454 GS FLX and ABI SOLiD). A single machine can produce around 20 Gigabases of sequence data in a week. This is the equivalent to over 6 human genomes. The data from the Illumina machine comes as relatively short stretches of 35 - 250 base pairs (bp) of DNA - around 300 million of them. These individual sequences are called reads. The older capillary sequencing technology generates longer reads of ~500 bp, but the approach is much slower and more expensive.</p> <p>One of the greatest challenges of sequencing a genome is determining how to arrange sequencing reads into chromosomes. This process of determining how the reads fit together by looking for overlaps between them is called genome assembly. Capillary sequencing reads (~500bp) are considered a good length for genome assembly. Genome assembly using sequence reads of &lt;100bp is more complicated due to the high frequency of repeats longer than the read length. Assemblies for bacterial genomes can comprise of at least 50 pieces (called \u201ccontigs\u201d), whilst for Eukaryotes more than 1000 pieces is common. Therefore new sequencing technologies are applied predominantly where a reference genome already exists. A reference genome is a well assembled genome from the same or a similar organism that is undergoing sequencing. Sequencing a genome with new technology when there is an existing reference is called resequencing.</p> <p>In this practical, we will focus on mapping reads to a reference genome and visualising the resulting alignments using Tablet.</p>"},{"location":"introduction/mapping/#sequencingmapping-workflow","title":"Sequencing/Mapping workflow","text":"<p>The diagram below describes the workflows for genomic resequencing and RNA sequencing. We will cover the in silico (computational) aspects of these workflows.</p> <p></p> <p>When resequencing, instead of assembling the reads to produce a new genome sequence and then comparing the two genome sequences, we map the new sequence data to the reference genome. We can then identify Single Nucleotide Polymorphisms (SNPs), insertions and deletions (indels) and Copy Number Variants (CNVs) between two similar organisms.</p> <p>The first example we use here is a reference genome for Plasmodium falciparum (3D7 clone, size 23Mb, 81% AT content) (Gardener et al., 2002). This parasite is the causative agent of malaria. Malaria is widespread in tropical and subtropical regions, including parts of the Americas, Asia, and Africa. Each year, there are approximately 350\u2013500 million cases of malaria, killing between one and three million people, the majority of whom are young children in sub-Saharan Africa. Recently we sequenced two laboratory strains, IT and DD2, using 76 bp paired end read technology. The second example is using the reference genome for Mycobacterium tuberculosis (H37Rv, size 4.4Mb, GC content 65.6%) (Cole et al, 1998). This bacterium causes tuberculosis disease.</p> <p>Rather than attempting to assemble these very short reads into a whole new genome, we will map them against the existing genome assembly of P. falcipraum (3D7) or M. tuberculosis (H37Rv). Furthermore, we will identify differences between the genomes of the three clones, find mutations and CNVs associated to drug resistance, whilst determining how the reads fit together.</p> <p>Here is the general workflow of mapping: 1. The paired end reads (F=forward; R=reverse) are mapped against the reference. Different tools can be used for that. The results can be transformed with samtools to an ordered and indexed bam file. 2. Those bam files can be read into programs like IGV to visualize the alignments of the short sequences. 3. From the bam file it is possible to call variants. The output format is BCF or VCF. VCF can be loaded easily into excel like tools.</p> <p></p> <p>Quality control of the reads is always important, to correct for any GC content biases, possible contamination, and read quality.</p> <p>Important</p> <p>Before doing anything, we need to first activate the conda environment for this practical by typing the following: <code>conda activate day1</code>. This environment contains most of the software we need for this practical. This command needs to be run each time we open up a new terminal or switch from a different environment. </p>"},{"location":"introduction/mapping/#file-formats","title":"File formats","text":"<p>You have the P. falciparum 3D7 clone reference file (Pf3D7_05.fasta). This contains the assembled sequence of the 3D7 genome. You also have two files of sequence reads from the IT clone (IT.Chr5_1.fastq.gz and IT.Chr5_2.fastq.gz). Look in both the reference file and the read files. </p>"},{"location":"introduction/mapping/#fasta-format","title":"FASTA format","text":"<p>In the terminal navigate to the data/malaria directory, by typing:</p> <pre><code>cd ~/data/malaria/\nhead -n 5 Pf3D7_05.fasta\n</code></pre> <p>Output</p> <pre><code>&gt;Pf3D7_05\nctaaaccctgaaccctaaaccctgaaccctaaaccctaaaccctgaaccctaaaccctga\naccctaaaccctgaaccccctaaaccctaaaccctgaaccctaaaccctaaaaccctgaa\nccctaaaccctgaaccctaaaccctgaaccctgaaccctaaaccctgaaccctaaaccct\ngaaccctaaaccctaaaccctaaaccctaaaccctaaaccctaaaccctaaaccctgaac\n</code></pre> <p>The line starting with \"&gt;\" defines the header and all subsequent lines define the sequence.</p>"},{"location":"introduction/mapping/#fastq-format","title":"FASTQ format","text":"<p>Now type the commands below to see what FASTQ format looks like.</p> <pre><code>zcat IT.Chr5_1.fastq.gz | head -n 12\n</code></pre> <p>Output</p> <pre><code>@HS3_5961:1:2207:17088:52965#1/1\nGCCGGTTGTTCGGCTGGAAGGTCCTTTTGCCCTTGGTCACGGGCGTCTCCTCGCTATGTCTGGCAACATCACCAT\n+\nGGGGGGEGFGFGGGFGDBEBF:AFDEFG?CFFDD:FBEE?EFEDFDEFEAEBFB8CCC?A@CD;ADDD86CCC&gt;&lt;\n</code></pre> <p>Each read is represented by four lines:</p> <ol> <li>The read name and additional info from the sequencing platform</li> <li>The sequence of the read</li> <li>A row in which comments can be added (rarely used anymore)</li> <li>Sequence quality. There is one character for each nucleotide. The characters relate to a sequence quality score e.g. how likely is the nucleotide correct? \u2018&gt;\u2019 is higher quality than \u20186\u2019. Sequence reads tend to have more errors at the end than the start</li> </ol> <p>Question</p> QuestionAnswer <p>Which format allows the storage of base quality data?</p> <p>FASTQ: the 4th line for each read in fastq format represents the sequence quality</p>"},{"location":"introduction/mapping/#quality-control","title":"Quality Control","text":"<p>Prior to the analysis of these sequences, it is highly advisable to perform quality control checks and filtering in order to minimise artefacts like base calling errors, poor quality reads or primer contamination. The difference in passing filtered data to downstream programs can be night-and-day for processes like de novo assembly or SNP calling. </p> <p>FastQC is a tool written in Java to perform quality control checks on raw sequence data. It computes several statistics and shows the results on summary graphs and tables. We will analyse several datasets in order to check whether the samples look good or, on the contrary, they require further filtering. </p> <p>First we will activate the software environment using: </p> <pre><code>conda activate day1\n</code></pre> <p>Open up a new terminal window and then type: <code>fastqc</code> Once the program is running, select <code>File &gt; Open</code> to open one or more Sequence files (see below). Browse to <code>~/data/tb/</code> to choose <code>sample1_1.fastq</code> and click  <code>OK</code>. </p> <p>FastQC supports FASTQ files (all quality encoding variants), GZip compressed FastQ and alignment files (SAM and BAM formats). </p> <p>Newly opened files will be immediately processed. Because of the size of FASTQ files it can sometimes take a couple of minutes to open them. </p> <p>FastQC performs several analyses, described in modules on left hand side of the main window (see below). Note that these results should be taken as guidance in the context of our library and never as pass/fail results. Nevertheless, they will highlight major problematic aspects. Let us now interpret the output of some of the modules.</p> <p>Normal results are marked with green ticks, slightly abnormal with orange exclamation marks and very unusual with red crosses</p> <p>Information</p> Basic sequence statisticsPer Base Sequence QualityPer Tile Sequence QualityPer base N contentSequence length distributionSequence duplication levelsOverrepresented sequences <p>The Basic Statistics module provides overall information such as the total number of reads, read length and %GC content.</p> <p>This view shows the quality values (y-axis) along the read at each nucleotide position (x-axis). The greater this value, the lower the probability that the corresponding base call is incorrect. At each position a Box-Whisker plot is drawn where the central red line is the median; the yellow box represents the inter-quartile range (25-75%); the upper and lower whiskers the 10% and 90% points; and the blue line correspond to the mean quality. This analysis computes the mean quality of each read and then displays for all observed values the number of reads having such quality. It is expected that a small subset will have universally poor quality. FastQC outputs a warning if the most frequently observed mean is below 27 (corresponding to a 0.2% error rate) and a failure if is below 20 (1% error rate).</p> <p>This graph will only appear in your analysis results if you're using an Illumina library which retains its original sequence identifiers. Encoded in these is the flowcell tile from which each read came. The graph allows you to look at the quality scores from each tile across all of your bases to see if there was a loss in quality associated with only one part of the flowcell.</p> <p>The plot shows the deviation from the average quality for each tile. The colours are on a cold to hot scale, with cold colours being positions where the quality was at or above the average for that base in the run, and hotter colours indicate that a tile had worse qualities than other tiles for that base. In the example below you can see that certain tiles show consistently poor quality. A good plot should be blue all over.</p> <p>This module outputs the percentage of N base calls at each position. N is usually called when the sequencer is not confident enough to call a base (A, T, C or G). Small proportion of Ns may appear especially towards the last positions.</p> <p>Current sequencers frequently produce reads of uniform length. Nevertheless, if reads are trimmed due to quality issues we will observe variable read lengths.</p> <p>This module computes the degree of duplication of every read in the file by looking for exact matches over the whole length of the rest of reads. A dataset with high depth of coverage, namely an over-represented target genome, will report higher duplication levels than one would expect since multiple reads starting at the same position will be found by chance.</p> <p>In a normal library, every individual sequence will make up a tiny fraction of the whole set. Therefore, overrepresented reads may be indicators of biologically significant sequences or contaminants. All reads representing more than 0.1% of the total amount will be listed. Primers and adaptors are the most common sources of contamination.</p> <p>Question</p> QuestionAnswer <p>At this point you should be able to open sequence files with FastQC and interpret the quality plots described. First, open sample IT.Chr5_1.fastq.gz in ~/data/malaria/. Second, open sample1_1.fastq.gz, sample2_1.fastq.gz and sample3_1.fastq.gz in ~/data/tb/.</p> <p>Questions:</p> <p>Can you spot any quality issue with the reads we should be worried about? If so, what may be their source?</p> <p>All of the FASTQ files look ok except sample3_1.fastq.gz. If you look at the 'Per base sequence quality' graph, you will see that many bases fall into the low quality zone (Q&lt;20).</p>"},{"location":"introduction/mapping/#saving-a-report","title":"Saving a report","text":"<p>We can keep a record of the FastQC results. To create a report simply select File &gt; Save Report from the main menu. This will create a folder containing the images from the analyses, an HTML version of module results and text files containing results numeric data. </p> <p>You may find useful the information in the following links:</p> <ol> <li>http://www.bioinformatics.babraham.ac.uk/projects/fastqc/</li> <li>http://bioinfo-core.org/index.php/9th_Discussion-28_October_2010</li> <li>http://www.youtube.com/watch?v=bz93ReOv87Y (FastQC video tutorial)</li> </ol> <p>We have used the FastQC tool to identify problems in raw sequence data. Nevertheless, bad quality issues in our data are not a death sentence for the experiment; it can still be useful for downstream analyses if we applied the right filtering before processing. In general, the problem in the sequence files is the degradation in quality of bases at end of the reads, leading to the need to hard clip or trim them. </p>"},{"location":"introduction/mapping/#trimming-reads-using-trimmomatic","title":"Trimming reads using Trimmomatic","text":"<p>The quality of the last few bases of the read can be substantially lower than the rest of the sequence read. In general, QC tools are unable to filter these reads out due to their overall high quality. Besides, it is not advisable to discard the whole read just because of few low-quality bases at the ends. Therefore, it is crucial to trim such bases before any further analysis. Trimmomatic (Lohse et al., 2012) is a fast command line tool that can be used to trim and crop Illumina data as well as to remove adapters (see Table below). We will be using the paired-end mode which will process both forward and reverse reads simultaneously, keeping the correspondence of read pairs intact. </p> Parameter Description ILLUMINACLIP Cut adapter and other Illumina-specific sequences from the read SLIDINGWINDOW Performs a sliding window trimming approach. It starts scanning at the 5'   end and clips the read once the average quality within the window falls below   a threshold MAXINFO An adaptive quality trimmer which balances read length and error rate to   maximise the value of each read LEADING Cut bases off the start of a read, if below a threshold quality TRAILING Cut bases off the end of a read, if below a threshold quality CROP Cut the read to a specified length by removing bases from the end HEADCROP Cut the specified number of bases from the start of the read MINLEN Drop the read if it is below a specified length AVGQUAL Drop the read if the average quality is below the specified level TOPHRED33 Convert quality scores to Phred-33 TOPHRED64 Convert quality scores to Phred-64 <p>Try to understand what the following command line specifies by looking at the Trimmomatic parameters in the previous table, and run it for sample1. </p> <pre><code>cd ~/data/tb/\ntrimmomatic PE sample1_1.fastq.gz sample1_2.fastq.gz -baseout sample1.fastq LEADING:3 TRAILING:3 SLIDINGWINDOW:4:20 MINLEN:36\n</code></pre>"},{"location":"introduction/mapping/#alignment-of-short-reads","title":"Alignment of short reads","text":"<p>A major step in most sequence analysis pipelines involves the alignment of reads against a reference genome, a requisite stage for downstream analyses. Traditional alignment software have become obsolete when dealing with the large amounts of short reads (millions) produced by NGS platforms and, during the last few years, software particularly designed to map short queries onto a long reference sequence have been produced. </p> <p>Currently popular alignment tools are summarised in Li &amp; Homer (2010). As shown in column 5 in the Table below, most of them allow gaps in alignments. Effective gapped aligners avoid calling false SNPs predicted due to the presence of insertions and deletions (indels). Furthermore, software including paired-end information are capable of mapping repeat reads with uniquely mapped mates; they also correct alignment errors that are inconsistent with the constraints imposed by mate pairs in terms of distance and orientation. Base quality in reads has also been exploited by programs like MAQ improving alignment accuracy. </p> <p>All aforementioned programs output both aligned and unaligned reads in Sequence Alignment/Map (SAM) format (H. Li et al. 2009), a standardised format widely supported by downstream bioinformatic tools (e.g. variant callers, alignment viewers). The development and improvement of efficient short-read alignment algorithms has made the fast processing of data produced by current sequencing platforms feasible which, initially, entailed a bottleneck in the analysis of such high-coverage sequence datasets. </p>"},{"location":"introduction/mapping/#sequence-alignment-exercise-1","title":"Sequence alignment Exercise 1","text":"<p>In this exercise we will be running the Burrows-Wheeler Aligner (BWA), an efficient program that aligns relatively short nucleotide sequences (i.e. short reads) against a long reference sequence. It implements two algorithms, bwa-short and BWA-SW. The former works for query sequences shorter than ~200bp with low error rate (&lt;3%) (e.g. reads produced by Illumina technology). It performs gapped global alignment, supports paired-end reads, and is one the fastest short read alignment algorithms currently available. BWA-SW is designed for longer sequences (e.g. 454 reads) with more errors. </p> <p>Open up a new terminal window and type <code>bwa</code></p> <p>All available BWA commands will appear on the terminal screen as shown below.</p> <p>Output</p> <pre><code>Program: bwa (alignment via Burrows-Wheeler transformation)\nVersion: 0.7.17-r1188\nContact: Heng Li \n\nUsage:   bwa  [options]\n\nCommand: index         index sequences in the FASTA format\n        mem           BWA-MEM algorithm\n        fastmap       identify super-maximal exact matches\n        pemerge       merge overlapping paired ends (EXPERIMENTAL)\n        aln           gapped/ungapped alignment\n        samse         generate alignment (single ended)\n        sampe         generate alignment (paired ended)\n        bwasw         BWA-SW for long queries\n\n        shm           manage indices in shared memory\n        fa2pac        convert FASTA to PAC format\n        pac2bwt       generate BWT from PAC\n        pac2bwtgen    alternative algorithm for generating BWT\n        bwtupdate     update .bwt to the new format\n        bwt2sa        generate SA from BWT and Occ\n</code></pre> <p>In order to map our paired-end reads, we will use \u2018index\u2019 and \u2018mem\u2019 commands. The \u2018index\u2019 command takes the reference genome (i.e. the database) in FASTA format as input and indexes it, which typically will take a few hours. This command only needs to be executed once for a particular reference genome.</p> <p>To see all options for the index command type: <code>bwa index</code></p> <p>Most options will be set automatically according to the data we are running it on. Thus, we do not have to worry about setting them.</p> <p>Type the following line:</p> <pre><code>bwa index ~/data/tb/tb.fasta\n</code></pre> <p>Several alignment algorithms are implemented in BWA. We will be using the BWA-MEM, which is the latest and generally recommended for high-quality queries as it is faster and more accurate. To see which parameters can be tuned for the \u2018mem\u2019 command type: <code>bwa mem</code></p> <p>We will be using the trimmed files output (from applying trimmomatic) as input for BWA:</p> <pre><code>bwa mem -R \"@RG\\tID:sample1\\tSM:sample1\\tPL:Illumina\" ~/data/tb/tb.fasta sample1_1P.fastq sample1_2P.fastq | samtools view -b - | samtools sort -o sample1.bam -\n</code></pre> <p>Then we can index the bam file using:</p> <pre><code>samtools index sample1.bam\n</code></pre> <p>The previous commands transformed SAM files into binary format, sorted the reads by occurrence against the reference, and then indexed the bam files. At this point, we can use the \u2018flagstat\u2019 samtools command to print summary statistics from indexed bam files:</p> <p><pre><code>samtools flagstat sample1.bam\n</code></pre> The 'flagstat' output should look like this:</p> <p>Output</p> <pre><code>1345146 + 0 in total (QC-passed reads + QC-failed reads)\n0 + 0 secondary\n0 + 0 supplementary\n0 + 0 duplicates\n1330984 + 0 mapped (98.95% : N/A)\n1345146 + 0 paired in sequencing\n672573 + 0 read1\n672573 + 0 read2\n1312598 + 0 properly paired (97.58% : N/A)\n1318846 + 0 with itself and mate mapped\n12138 + 0 singletons (0.90% : N/A)\n0 + 0 with mate mapped to a different chr\n0 + 0 with mate mapped to a different chr (mapQ&gt;=5)\n</code></pre> <p>Where each line specifies:</p> <ol> <li>Total number of reads</li> <li>Duplicates</li> <li>Number of mapped reads</li> <li>Paired reads in sequencing</li> <li>Number of forward reads</li> <li>Number of reverse reads</li> <li>Number of properly mapped reads (same chromosome, opposite orientation, and within few deviations from the expected insert size)</li> <li>Number of reads mapped along with their mates (7 is a subset of 8)</li> <li>Singletons: number of reads mapped alone, i.e. mate not mapped (8+9=3)</li> <li>Reads whose mates are mapped to a different chromosome</li> <li>Reads whose mates are mapped to a different chromosome with mapping quality greater than 5; note that (11) is a subset of (10)</li> </ol> <p>Finally, let's also remove some temporary files we no longer need. Trimming has produced some files which are unzipped and take up a lot of space. Let's remove these using: </p> <pre><code>rm sample1_1P.fastq sample1_1U.fastq sample1_2P.fastq sample1_2U.fastq\n</code></pre> <p>Exercise</p> TaskHintsCheats <p>Execute all previous commands for to get a bam file for sample 2</p> <p>You need to run through the following steps:</p> <ol> <li>Trim the reads with trimmomatic</li> <li>Use bwa to align the reads and samtools to store as bam</li> </ol> <p>Here is the code you need to use:</p> <pre><code>cd ~/data/tb\ntrimmomatic PE sample1_1.fastq.gz sample1_2.fastq.gz -baseout sample1.fastq LEADING:3 TRAILING:3 SLIDINGWINDOW:4:20 MINLEN:36\nbwa mem -R \"@RG\\tID:sample2\\tSM:sample2\\tPL:Illumina\" ~/data/tb/tb.fasta sample2_1P.fastq sample2_2P.fastq | samtools view -b - | samtools sort -o sample2.bam -\n</code></pre> <p> </p>"},{"location":"introduction/mapping/#do-not-proceed-until-completing-the-exercise-above-for-sample-2","title":"Do Not Proceed Until Completing The Exercise Above For Sample 2","text":""},{"location":"introduction/mapping/#viewing-in-igv","title":"Viewing in IGV","text":"<p>Launch IGV by running the command <code>igv</code> on a New Terminal and perform the following steps:</p> <ol> <li>We first need to load the reference genome. To do this click on Genomes -&gt; Load Genome from File.... Navigate to ~/data/tb and select the tb.fasta file.</li> <li>You can also load the genes by clicking on File -&gt; Load from File..., then selecting the tb.gff file. </li> <li>Finally you can load the bam file by clicking File -&gt; Load from File... and selecting the bam file you wish to load, in our case it will be sample1.bam. </li> </ol> <p>There is a lot going on on the screen so have a look at the image below which explains some of the most important features.</p> <p></p> <ol> <li>Zoom controls: This controls the level of zoom. To see your alignments you will need to zoom in</li> <li>Search bar: Here you can specify the region you want to view using genomic coordinates in the form of Chromosome:start-end. You can also search by gene name.</li> <li>Coverage track: This displays when you have loaded a bam file and is a bargraph where each bar represents a genomic position and the high is determined by the number of reads aligning to that position. If you click on a bar it will open up a window telling you the number of ACTGs. </li> <li>Alignment track: Each grey bar represents a read. If there are coloured nucleotides, black dashes or purple vertical lines it represents a difference from the reference. Have a look at this document to find out more.</li> <li>Gene track: This shows the location and orientation of the genes. Click on a gene to find out more information.</li> </ol> <p>Additionally, you can see that the screenshot above has loaded more than one bam file. </p>"},{"location":"introduction/mapping/#navigation-in-igv","title":"Navigation in IGV","text":""},{"location":"introduction/mapping/#zooming-in-and-out","title":"Zooming in and out","text":"<p>Now you will be able to view the alignment of the reads to the reference. By default, IGV will only display the alignments when viewing a region of the genome less than 30kb. To view the alignment you can zoom in using the \"+\" button on the top right side of the window. You can zoom out using the \"-\" button.</p>"},{"location":"introduction/mapping/#viewing-a-region","title":"Viewing a region","text":"<p>You can zoom in to a specific region using the search bar. You can do this by entering your region using the specific format: chromosome_name:start-end. For example if you want to go zoom in on positions 1000-2000 on the Mtb chromosome you can type in \"Chromosome:1000-2000\" and hit Enter. </p> <p>In most cases you'll probably be searching for a certain gene. You can also enter the gene name into the search bar and it will automatically center the view on this gene. Try this with the rpoB gene for example. Type in \"rpoB\" into the search bar and hit Enter.</p>"},{"location":"introduction/mapping/#exercise-mapping-the-p-falciparum-it-reads-with-bwa","title":"Exercise: Mapping the P. falciparum IT reads with BWA","text":"<p>Info</p> <p>Now we will map the IT clone reads to the 3D7 reference, again using the short reads mapping program BWA</p> <p>The files for this exercise are in the <code>~/data/malaria</code> directory:</p> <pre><code>cd ~/data/malaria/\n</code></pre> <p>First we need to index the reference (the algorithm needs to access specific positions in the reference in an efficient way). </p> <pre><code>bwa index ~/data/malaria/Pf3D7_05.fasta\n</code></pre> <p>Next, read files (both forward/reverse) are mapped against the reference </p> <pre><code>bwa mem ~/data/malaria/Pf3D7_05.fasta ~/data/malaria/IT.Chr5_1.fastq.gz ~/data/malaria/IT.Chr5_2.fastq.gz | samtools view -b - | samtools sort -o IT.Chr5.bam -\n</code></pre> <p>The details of where each IT clone read has been mapped is now stored in the file IT.Chr5.bam. We are going to view the mapped reads in IGV. Before we can proceed, however, we must index the BAM file to allow programs suchh as IGV quick access to different sections of the file. </p> <pre><code>samtools index IT.Chr5.bam\n</code></pre> <p>Before we visualize the alignment of the reads in IGV, let us have a look at the sam/bam format. Samtools was developed to have a standard format to store reads. It contains information about the reference sequence, where a read is mapped, quality of mapping, and where it\u2019s mate is mapped. Files ending with .sam, are normally plain text, but as they might take too much space, the file is compressed into a bam file. All visualization tools will need the bam file. It has to be sorted (by chromosome and position) and indexed. Indexing enables the fast processing of alignments. </p> <pre><code>samtools view IT.Chr5.bam | grep \"IL39_6014:8:61:7451:18170\"\n</code></pre> <p></p>"},{"location":"introduction/mapping/#viewing-the-mapped-reads-in-igv","title":"Viewing the mapped reads in IGV","text":"<p>We will now examine the read mapping in IGV using the BAM view feature.</p> <p>Launch IGV by running the command <code>igv</code> on a New Terminal. We first need to load the reference genome. To do this click on <code>Genomes -&gt; Load Genome from File...</code>. Navigate to <code>~/data/malaria</code> and select the <code>Pf3D7_05.fasta</code> file. You can also load the genes by clicking on <code>File -&gt; Load from File...</code>, then selecting the <code>Pf3D7_05.gff</code> file. Finally you can load the bam file by clicking <code>File -&gt; Load from File...</code> and selecting the bam file you wish to load, in our case it will be <code>IT.Chr5.bam</code>. </p> <p>Question</p> QuestionAnswer <p>Scroll through the genome. Describe the coverage. Are there regions that are not covered? </p> <p>In general there is good coverage across the genome. This can be said by the fact that most of the positions have many reads overlapping them. There are a few positions with few or no reads aligning. This could be due to poor sequencing of those regions or a deletion.</p> <p>Let's go to the region around the PF3D7_0504700 gene by typing it into the search bar and hitting Enter.</p> <p>Zoom in and position the mouse over a read until a window pops up. This window provides information on the mapping of the reads, as it is stored in the bam file. Notice the Mapping quality (MAPQ). The maximum value for this is 60. The mapping quality depends on the accuracy of the sequence read and the number of mismatches with the reference. A value of 0 means that the read maps equally well to at least one other location and therefore is unreliably mapped. </p>"},{"location":"introduction/mapping/#differences-in-read-coverage","title":"Differences in read coverage","text":"<p>It is thought that a duplication in the mdr1 gene of P. falciparum is associated with drug resistance against the antimalarial mefloquine and that it may also modulate susceptibility to chloroquine, another antimalarial drug. For more information have a look in PubMed, e.g. at Borges et al. (2011) [PMID: 21709099] or at Mungthin et al. (2010) [PMID: 20449753] </p> <p>Once IGV has started running on your screen navigate to the mdr1 gene locus using the search bar.</p> <p>Question</p> QuestionAnswer <p>It looks like that those two clones have a copy number variation (assuming the reference was assembled correctly). Is the duplication the same in both clones? </p> <p>The region with elevated coverage is slightly different and so we can conclude that the duplication is different between the strains.</p>"},{"location":"introduction/mapping/#identifying-single-nucleotide-polymorphisms","title":"Identifying Single Nucleotide Polymorphisms","text":"<p>Let us have a look at SNPs now. To do so, zoom in on the mdr1 gene and make sure you are in Strand Stack view and have Show SNP marks selected. As mentioned before, in addition to true SNPs some differences between the reference sequence and the mapped reads are due to sequencing errors. On average, 1 in every 100 bases in the reads is expected to be incorrect. In particular, some sequencing errors may be due to a systematic problem as illustrated below. </p> <p>Differently coloured nucleotides appear on the stacked reads highlighting every base in a read which does not match to the reference. If you zoom in you can distinguish real SNPs as vertical coloured lines, while the random sequencing errors are more disperse. </p> <p>Question</p> QuestionAnswer <p>When you zoom in on position 958145 as far as you can go, you can see a SNP: here, both strain IT and Dd2 have a thymine where the reference (3D7) has an adenine. What is the consequence of this SNP? Can we tell what effect it will have for the clones? Is this the mutation N86Y (or also simply referred to as Y86) that may modulate the degree of resistance to chloroquine? (See e.g. Mula et al. (2011) [PMID: 21810256]) </p> <p>To answer the question, right-click on the gene track and make sure \"expanded\" is selected. You can see which amino acids are normally coded for around amino acid position 86. Note also that AAT codes for Asn (N) and TAT codes for Tyr (Y). This SNP modulates resistance to chloroquine.</p> <p>Question</p> QuestionAnswer <p>Now consider the neighbouring position 958146. What could this be? Is it from IT or Dd2? </p> <p>To answer this question, look at the position in the different bams. It looks like the SNP onle appears in reads from IT</p> <p>Information</p> <p>This concludes the Mapping practical. You should now be able to:</p> <ol> <li>Assess the quality of raw sequence data</li> <li>Perform read trimming to remove low quality bases</li> <li>Map reads to a reference genome using BWA</li> </ol> <p>Next up you will learn how to use the alignment you have generated to call variants. </p>"},{"location":"introduction/mapping/#important-aspects-of-the-mapping-procedure","title":"Important aspects of the mapping procedure","text":""},{"location":"introduction/mapping/#non-uniquerepeat-regions","title":"Non-unique/repeat regions","text":"<p>A sequence read may map equally well to multiple locations in the reference genome. In such cases it is unclear where the read should be placed. Different mapping algorithms have alternative strategies for this problem. Maq will randomly report only one of the mapping locations and give the read a score of 0.</p>"},{"location":"introduction/mapping/#gc-content","title":"GC content","text":"<p>Some organisms have genomes with extreme GC content. The Plasmodium genome, for instance, is 19% GC, meaning 81% of bases are A or T. The result of this is that reads are more likely to map by chance to multiple locations in the genome, when compared to a genome with neutral GC content (e.g. 40-60% GC).</p>"},{"location":"introduction/mapping/#insert-size","title":"Insert size","text":"<p>When mapping paired reads, the mapping algorithm (e.g. bwa) takes the expected insert (e.g. sequenced DNA fragment) size into account. If the fragments are expected to be, on average 400bp and the sequence reads are 150bp, then the paired reads should be ~100bp apart. If the paired reads are significantly further apart then we can say that the reads do not map reliably and discard them. This information can assist with improving the reliability of mapping.</p>"},{"location":"introduction/mapping/#tips","title":"Tips","text":"<p>It is always a good idea to try different programs for any particular problem in computational biology. If they all produce the same answer you can be more certain it is correct. Alternative short read mappers include SOAP (Li et al., 2008b), Ssaha (Ning et al., 2001), MAQ (Li et al., 2008), Bowtie(2) (Langmead et al, 2009, 2012) and SMALT. As seen, TopHat (Trapnell et al., 2009) is particularly useful for RNAseq mapping as it supports spliced mapping. New tools for mapping sequence reads are continually being developed. This reflects improvements in mapping technology but it is also due to changes in the sequence data to be mapped. The sequencing machines we are using now (e.g. Illumina Genome Analyzer II, HiSeq2000/2500, 454 GS FLX etc) will not be the ones we are using in a few years time and the data the new machines produce are likely to be sub-optimally mapped with current tools.</p>"},{"location":"introduction/task/","title":"Task","text":""},{"location":"introduction/task/#introduction","title":"Introduction","text":"<p>As we transition from our foundational lectures into practical application, it is crucial to evaluate your understanding and proficiency in the core bioinformatics processes of mapping, variant calling, and assembly. These tools are the cornerstone of genomic analysis and have wide-reaching implications in the field of infectious diseases, including tuberculosis. This test will simulate a real-world scenario where you will analyze raw sequencing data to extract meaningful insights that can inform treatment decisions and deepen our understanding of pathogen genomics.</p> <p>So your task is to utilize your bioinformatics expertise to uncover the genetic factors that may explain the variability in clinical outcomes among these patients. By analyzing the sequencing data, you will identify crucial genetic variations\u2014specifically, single nucleotide polymorphisms (SNPs) in drug-resistance genes and lineage-defining deletions. Your findings will not only classify the TB strains infecting each patient but also predict their resistance to commonly used medications, ultimately guiding more effective treatment strategies. This exercise will test your ability to apply genome mapping and variant analysis to real-world infectious disease challenges.</p> <p>These genomes are larger than what you have been working on as we have only given you a snippet of the genome. Now you will work on the whole genome so it will take a bit of time!</p>"},{"location":"introduction/task/#task_1","title":"Task","text":"<p>Six People have come into the local hospital and presented with symptoms of TB. Samples were taken, and were confirmed to be TB infection so antibiotics were given, however some patients are still not responding toward the antibiotics and are struggling to fight off infection. </p> <p>TB can also have certain deletions on the genome that can identify the lineage of the strain (check out TB lineages and locations across the world) such as an entire deletion on the PPE50 gene in lineage 1 strains and Rv0072 for lineage 2 strains. This is not the only identifies but it is what we will use for this test.</p> <p>You will find the patients data within the following directory:</p> <pre><code>cd ~/data/tb/task\n</code></pre> <p>Your tasks are below:</p> <ol> <li>Perform suitable research, what types of drug resistance are there with TB and what causes them. </li> <li>Decide whether to map with or without a reference. If you choose mapping you will need to find a suitable reference genome.</li> <li>Call the variants and find the SNPs that are within the genes you found earlier that are known to cause drug resistance.</li> <li>Identify the individuals that struggled to fight infection and what antibiotics were they using at the hospital.</li> <li>Find the strain (lineage type) that comes with each patient by finding these deletions.</li> </ol>"},{"location":"introduction/task/#tips","title":"Tips","text":"Tips for Mtb Genomic Analysis <ul> <li> <p>NCBI Datasets: Obtain datasets from NCBI Taxonomy. Required files: fasta and gff.</p> </li> <li> <p>Reference Mapping: Map your data with a reference genome for alignment.</p> </li> <li> <p>Rifampicin Resistance: Investigate genes impacting Rifampicin resistance. Use the antibiotic information to focus your search.</p> </li> <li> <p>Gene Viewing: Annotate with snpEff for gene positioning, or view vcfs/bams in IGV.</p> </li> <li> <p>Strain Identification: Examine regions containing Rv0072 and PPE50. Apply similar methods as in variant detection for strain identification.</p> </li> </ul>"},{"location":"introduction/variant-detection/","title":"Variant detection","text":""},{"location":"introduction/variant-detection/#introduction","title":"Introduction","text":"<p>New sequencing technologies are mostly used for re-sequencing, namely when a well-assembled reference genome from the same organism or one very similar is available. Instead of assembling the reads to produce a new genome sequence, re-sequencing projects aim to compare the sampled genome with the reference by read mapping. Short read alignment was considered in the previous practical. Here, we will focus on downstream analyses aiming to identify Single Nucleotide Polymorphisms (SNPs), insertions and deletions (indels) and large structural variants (SVs) in the sequenced sample.</p> <p>Most of the software for variant detection and calling require alignment files (e.g. BAM format) as input. We will be using samtools/bcftools and GATK for SNP and small indel discovery.</p> <p>Historically, structural variation refers to relatively large polymorphisms that alter the chromosome structure. Indels belong to this group of genetic variation. Among the early methods for discovering structural variants (SVs), whole-genome array comparative hybridisation (Carter<sup>1</sup>) and SNP arrays (Cooper<sup>2</sup>) have been successfully applied despite their resolution limitations.</p> <p>The advent of high-throughput sequencing platforms has opened new possibilities in this area. New tools have been developed to detect unusual patterns of reads, or pairs of reads, left by SVs. Such signatures can be broadly grouped into three categories of signature: discordant mapping of read pairs, read splitting, and depth of coverage. SV detection programs implement algorithms aiming to identify such signatures, or combinations of them, from sequence alignment files (Medvedev<sup>3</sup>).</p>"},{"location":"introduction/variant-detection/#snp-detection-and-calling","title":"SNP detection and calling","text":"<p>Some differences between the reference and the mapped reads are due to sequencing errors. By definition, a quality score of 30 (Q30), refers to, on average, 1 in every 1000 bases in the reads is expected to be incorrect (Q20 1/100, Q40 1/10000 etc.). Nevertheless, the high depth of coverage achieved by current sequencing platforms SNPs can be distinguished from sequencing errors. True SNPs are expected to be shown as mismatches occurring consistently across multiple reads at the same reference position, whereas mismatches found at spurious locations are likely to be caused by sequencing errors. SNP calling tools make use of this fact to calculate statistical significance and filter out false positives.</p>"},{"location":"introduction/variant-detection/#exercise-1-identifying-single-nucleotide-polymorphisms-snps","title":"Exercise 1: Identifying Single Nucleotide Polymorphisms (SNPs)","text":"<p>Activate the relevant <code>conda</code> environment and open up a new session of <code>igv</code>:</p> <pre><code>conda activate day1\nigv\n</code></pre> <p>Info</p> <p>You can list all installed environments with <code>conda env list</code>.</p> <ol> <li>Click \"Genomes\" on the top menu bar followed by \"Load Genome From File...\". Select the reference fasta file at \"~/data/tb/tb.fasta\".</li> <li>Click \"File\" on the top menu bar followed by \"Load From File...\". Select the genome annotation file at \"~/data/tb/tb.gff\".</li> <li>Click \"File\" on the top menu bar followed by \"Load From File...\". Select the bam file for sample 1 at \"~/data/tb/sample1.bam\".</li> </ol> <p>We are going to check at positions 187,503 and 587,585 in the genome. To zoom into a position in the genome you have to input the location into the search bar in the format of Chromosome:position or Chromosome:start-end if you want to visualise a range. For example - to go to position 187,503 type in Chromosome:187503 and hit Enter. You should see an image similar to the one below.</p> <p></p> <p>Exercise</p> Question 1Answer 1 <p>What does the coverage and alignment track show you? Do you think there is a SNP in this position? Go to position 587,585 and check if we have any variants.</p> <p>You should be able to see that almost all the reads have a different nucleotide from the reference at potision 187,503 so we can conclude that we have a SNP here. For position 587,585 it is a bit more complicated. You might notice that we have both the reference and alternate alleles at this position. Further, it looks like all alternate alleles are located at the very end of reads and alsways in the reads going in the same direction. This is most likely an artefact of alignment and not a real variant.</p> <p></p>"},{"location":"introduction/variant-detection/#exercise-2-calling-snps-and-short-indels-using-sambcftools","title":"Exercise 2: Calling SNPs and short indels using SAM/BCFtools","text":"<p>In previous exercises raw reads were mapped onto the reference genome using BWA (Li<sup>4</sup>) for several Mtb samples, resulting in alignment files of BAM format. The combination of SAMtools (Li<sup>5</sup>) and BCFtools (Danecek<sup>6</sup>) is a widely established approach to call SNPs and small indels (Li<sup>4</sup>). SAMtools gathers information from input BAM files and computes the likelihood of each possible genotype. BCFtools takes SAMtools output and performs the actual genotype calling. Results are stored in files of VCF format (Variant Call Format).</p> <p>We had an insight into the SNP detection in the previous practical, using a single c. Let us start by looking at again at bcftools mpileup command options that we will need to change to optimise SNP calling.</p> <p>Open up a new terminal window and type:</p> <pre><code>bcftools mpileup\n</code></pre> <p>Table 1 Some bcftools mpileup parameters.</p> Option Type Default Description -C INT 0 Coefficient for downgrading mapping quality for reads containing excessive mismatches. Given a read with a phred-scaled probability q of being generated from the mapped position, the new mapping quality is about sqrt((INT-q)/INT)*INT. A zero value disables this functionality; if enabled, the recommended value for BWA is 50. -d INT 250 Max per-BAM depth to avoid excessive memory usage. -f FILE null The faidx-indexed reference file in the FASTA format. The file can be optionally compressed by gzip. -q INT 0 Minimum mapping quality for an alignment to be used. -Q INT 13 Minimum base quality for a base to be considered. <p>Read through the following command (we will execute them later):</p> <pre><code># bcftools mpileup -B -Q 23 -d 2000 -C 50 -f ~/data/tb/tb.fasta ~/data/tb/sample1.bam\n</code></pre> <p>Note that we are considering many more reads (depth) per position (-d) and imposing more strict quality thresholds (-Q). bcftools mpileup output must be piped to the bcftools call command, the second step in SNP calling. To display all available parameters type:</p> <pre><code>bcftools call\n</code></pre> <p>There is no need to understand every single parameter; the options we will need to use are listed in Table 2.</p> <p>Table 2 Some bcftools call parameters.</p> Option Description File Format Options --ploidy Predefined ploidy of the organism/data. Input/Output Options -A Keep all possible alternate alleles at variant sites. -f Output format fields. -V Skip indels/snps. -v Output variant sites only. Consensus/variant Calling Options -c The original calling method (conflicts with -m) -m Alternative model for multiallelic and rare-variant calling. -n Likelihood of novel mutation for constrained trio calling (conflicts with \u2013c). -p Variant if P(ref|D) -P Mutation rate (use bigger for greater sensitivity) use qith \u2013m [1.1 e-3]. <p>The complete bcftools call command would be as follows (it will be run later):</p> <pre><code># bcftools call \u2013m \u2013v \u2013O v &gt; sample1.raw.vcf\n</code></pre> <p>Having understood bcftools mpileup and bcftools call parameters, we can combine both instructions to perform SNP and short indel calling as follows:</p> <pre><code>mkdir ~/data/tb/variants\ncd ~/data/tb/variants/\nbcftools mpileup -B -Q 23 -d 2000 -C 50 -f ~/data/tb/tb.fasta ~/data/tb/sample1.bam | bcftools call --ploidy 1 -m -v -O v - &gt; sample1.raw.vcf\n</code></pre> <p>As we saw in the Introduction to Linux Practical the character \u2018|\u2019 is used to \u2018pipe\u2019 bcftools mpileup output into bcftools call as input. We can further filter the resulting variant calls using the vcfutils varFilter command, which like bcftools, is installed as part of the samtools package. Parameters within this tool can be reviewed when executing:</p> <pre><code>vcfutils.pl varFilter\n</code></pre> <p>Terminal output</p> <pre><code>Usage:   vcfutils.pl varFilter [options] \n\nOptions: -Q INT    minimum RMS mapping quality for SNPs [10]\n        -d INT    minimum read depth [2]\n        -D INT    maximum read depth [10000000]\n        -a INT    minimum number of alternate bases [2]\n        -w INT    SNP within INT bp around a gap to be filtered [3]\n        -W INT    window size for filtering adjacent gaps [10]\n        -1 FLOAT  min P-value for strand bias (given PV4) [0.0001]\n        -2 FLOAT  min P-value for baseQ bias [1e-100]\n        -3 FLOAT  min P-value for mapQ bias [0]\n        -4 FLOAT  min P-value for end distance bias [0.0001]\n        -e FLOAT  min P-value for HWE (plus F&lt;0) [0.0001]\n        -p        print filtered variants\n\nNb: Some of the filters rely on annotations generated by SAMtools/BCFtools.\n</code></pre> <p>The <code>-D</code> option of varFilter controls the maximum read depth, which should be adjusted at least to approximately twice the average read depth. Finally, type the following commands to obtain a high quality set of variants in VCF format:</p> <pre><code>cat sample1.raw.vcf | vcfutils.pl varFilter -d 10 -D 2000 &gt; sample1.filt.vcf\n</code></pre> <p>Important</p> <p>Run the code above for sample2. The bam has already been preprocessed so all you have to do is replace \u201csample1\u201d in the command with \u201csample2\u201d.</p>"},{"location":"introduction/variant-detection/#exercise-3-calling-snps-and-short-indels-using-gatk","title":"Exercise 3: Calling SNPs and short indels using GATK","text":"<p>The Genome Analysis Toolkit (GATK) includes a wide variety of tools, with a primary focus on variant discovery and genotyping. The standard GATK data processing pipeline starts by applying the HaplotypeCaller to identify variable sites with respect to the reference. HaplotypeCaller will be set up by default to work on diploid organisms, therefore we will use the <code>--ploidy 1</code> for bacterial genomes like Mycobacterium tuberculosis. We will run GATK for SNPs and small indel calling to complement samtools/bcftools output.</p> <p>Run the following two commands to call SNPs and indels using GATK instead of Samtools:</p> <pre><code>cd ~/data/tb/variants/\ngatk HaplotypeCaller -R ~/data/tb/tb.fasta -I ~/data/tb/sample1.bam -O sample1.gatk.raw.vcf -ploidy 1\n</code></pre> <p>Be sure to read the output to make sure that the sample1.gatk.raw.vcf file has been created correctly.</p>"},{"location":"introduction/variant-detection/#exercise-4-visualisation-of-variants-in-vcf-files-using-igv","title":"Exercise 4: Visualisation of variants (in VCF files) using IGV","text":"<p>We can open the resulting VCF files within the terminal window with the more (or less or head or cat) command:</p> <pre><code>less sample1.filt.vcf\n</code></pre> <p>Terminal output</p> <p>The output should look like this: <pre><code>#CHROM  POS ID  REF ALT QUAL    FILTER  INFO    FORMAT  sample1\nChromosome  1595    .   G   T   58  .   DP=16;VDB=1.106034e-02;RPB=2.849794e+00;AF1=0.5;AC1=1;DP4=8,4,2,2;MQ=39;FQ=61\n;PV4=0.6,0.11,1,0.027   GT:PL:GQ    0/1:88,0,202:91\nChromosome  1849    .   C   A   222 .   DP=148;VDB=3.647138e-01;AF1=1;AC1=2;DP4=0,0,77,66;MQ=44;FQ=-282 GT:PL:GQ    1/1:255,255,0:99\nChromosome  1977    .   A   G   222 .   DP=70;VDB=1.915965e-02;AF1=1;AC1=2;DP4=0,0,14,54;MQ=44;FQ=-232  GT:PL:GQ    1/1:255,205,0:99\n</code></pre></p> <p>Press Enter to scroll down to the end of the file or \u2018q\u2019 to exit back to the prompt.</p>"},{"location":"introduction/variant-detection/#vcf-format","title":"VCF format","text":"<p>VCF (Variant Call Format) is a text file format employed to store genetic variation with respect to a reference genome. It contains meta-information lines (beginning with ##), a header line (starting with #) and then data lines each containing information about a variable position in the genome. All variants, including SNPs and indels, are defined by the chromosome (CHROM), position (base pairs) in the reference genome (POS), ID, reference allele (REF), alternative allele (ALT) and a quality score for the detected variant (QUAL). The FILTER field contains information about the filters applied, and INFO gathers additional information. A detailed description of VCF format specification can be found at: http://samtools.github.io/hts-specs/VCFv4.3.pdf</p> <p>VCF files can be opened and visualised in genome browsers such as IGV.</p> <p>Launch an IGV instances as before (or use the same one if you kept it open) and load sample1.bam and sample2.bam in each of them as explained in Exercise 1.</p> <p>In the menu bar, select Load From File... and select the VCF files in ~/data/tb/variants for both samples(i.e. sample1.filt.vcf and sample2.filt.vcf).</p> <p>Then the SNPs should appear in the VCF track above the reads panel as shown below. On the other hand, sequencing errors are not called SNPs at those positions. Making use of search bar to visit the two regions we visited in Exercise 1 (containing positions 187,503 and 587,585 respectively). Remember that we were trying to distinguish real SNPs from sequencing errors. </p> <p></p> <p>Exercise</p> <p>The SNPs called from GATK can be examined in the same way. Try load the VCF files you generated with GATK and check if you get the same calls as with bcftools.</p> <p>Zoom out to visualise more variants. Double-click on the reads panel to zoom in. Go back to the VCF terminal view, take note of any indels found and try to locate them using IGV. Note that insertions and deletions are represented in a different way in IGV. </p> <p>Here is an insertion at 859,131 in sample2. Insertions are represented using purple strips on the reads. You can click on the strip to find out what the sequence of the insertion is.</p> <p></p> <p>Here is a deletion at position 485,810 in sample1, note that because of how the deletion is labelled in each case (IGV or VCF) they appear shifted in position. This is because the variant falls on top of a homopolymer run of A's (AAA) so we can place the deletion on any of them.</p> <p></p>"},{"location":"introduction/variant-detection/#drug-resistance-in-mycobacterium-tuberculosis","title":"Drug resistance in Mycobacterium tuberculosis","text":"<p>One area where polymorphisms are elevated in Mycobacterium tuberculosis is in genes relating to antibiotic action. Resistance to anti-tuberculosis drugs is caused predominantly by point mutations (i.e. SNPs) that arise spontaneously and offer the bacterium survival advantage during (incomplete) drug treatment. Since SNPs conferring drug resistance have been characterised (Sandgren<sup>7</sup>) it has been proposed that drug susceptibility can be inferred based on the presence/absence of such markers (Peacock, 2013). In this exercise we will be looking at Rifampicin resistance-conferring mutations found in the rpoB gene.</p> Position gene Reference Nucleotide Alternative Nucleotide Reference Codon Alternative Codons Reference Amino Acid Alternative Amino Acid 761095 rpoB T C CTG CCG L P 761101 rpoB A T CAA CTA Q L 761109 rpoB G T GAC TAC D Y 761110 rpoB A T GAC GTC D V 761128 rpoB C T TCG TTG S L 761139 rpoB C T;G;A CAC TAC;GAC;AAC H Y;D;N 761140 rpoB A G;T CAC CGC;CTC H R;L 761155 rpoB C T;G TCG TTG;TGG S L;W 761161 rpoB T C CTG CCG L P <p>Load in the Annotation track, to do so click the File &gt; Load from File and select ~/data/tb/tb.gff. Once loaded you can search for the Rv0667 gene in the Search Box and try to spot one of the above SNPs in the gene. </p> <p></p> <p>Exercise</p> Question 2Answer 2 <p>The SNPs called from GATK can be examined in the same way. Try load the VCF files you generated with GATK and check if you get the same calls as with bcftools.</p> <p>Sample 2 has a SNP at position 761155. It changes the nucleotide from a C to a T. By looking at the table you can see that this leads to the mutation S450L which is the most prominant resistance mutation for rifampicin. </p> <p></p>"},{"location":"introduction/variant-detection/#large-structural-variant-sv-detection-and-calling","title":"Large structural variant (SV) detection and calling","text":"<p>While small indels can be detected by finding gaps within mapped reads, larger structural variation requires alternative approaches. When working with short reads and high depth of coverage, samtools/bcftools and GATK are established pipelines for detection and calling of SNPs and small indels. However, there are other algorithms and software for the identification of larger SVs (Medvedev<sup>3</sup>). In the following exercises, we will introduce the main approaches and employ representative tools deploying such strategies. The software\u2019s quality is measured in terms of sensitivity, specificity and accuracy of predicting breakpoint locations.</p> <p>Depth of coverage (DOC) methods assume that the number of mapped reads in a given region follow a (modified) Poisson distribution, and are expected to be proportional to the number of times such a region occurs in the sequenced genome. Deleted regions will have much fewer or no reads mapping to it, whereas a duplicated region will have more reads (Chiang<sup>9</sup>). The larger the event, the stronger the detected signal for these signatures. The main disadvantages of this approach are its inefficacy at detecting weaker signals caused by small events, breakpoint resolution and that gain signatures do not localise where the insertion occurred.</p>"},{"location":"introduction/variant-detection/#exercise-5-sv-detection-using-discordant-paired-end-mapping-pem-and-split-read-approaches-delly-software","title":"Exercise 5: SV detection using discordant paired-end mapping (PEM) and split read approaches (Delly software)","text":"<p>Mapping of paired-end reads (PEM) and DOC methods were the first to be implemented when reads were much shorter (~35bp). PEM is the most prominent method to detect large SVs (Medvedev<sup>3</sup>). DOC signatures also support SVs including a copy number change in the sequenced genome, namely deletions and tandem duplications. Nevertheless, with longer (paired) reads achieved nowadays (up 150bp with Illumina HiSeq machines), read split methods outperform PEM software in terms of breakpoint resolution, higher discovery rates and lower false positives (Suzuki<sup>10</sup>). Recently published tools, Delly (Rausch<sup>11</sup>) make use of a complementary strategy under the assumption that SVs supported by different signatures are called with higher confidence. Both aforementioned tools use a split read approach assisted by the PEM. This exercise will combine paired-end and split-read alignment methods to increase sensitivity and specificity in SV calling. In particular we will be using Delly software to detect large structural variants.</p> <p>Change to variants directory and run the following commands:</p> <pre><code>cd ~/data/tb/variants/\ndelly call -o sample1.delly.bcf -q 20 -s 3 -g ~/data/tb/tb.fasta ~/data/tb/sample1.bam\ndelly call -o sample2.delly.bcf -q 20 -s 3 -g ~/data/tb/tb.fasta ~/data/tb/sample2.bam\n</code></pre> <p>It should take no longer than 5 minutes per file. Type the program name to display all options:</p> <pre><code>delly\n</code></pre> <p>Terminal output</p> <p>The screen output should look like this: <pre><code>**********************************************************************\nProgram: Delly\nThis is free software, and you are welcome to redistribute it under\ncertain conditions (GPL); for license details use '-l'.\nThis program comes with ABSOLUTELY NO WARRANTY; for details use '-w'.\n\nDelly (Version: 0.7.8)\nContact: Tobias Rausch (rausch@embl.de)\n**********************************************************************\n\nUsage: delly  \n\nCommands:\n\n    call         discover and genotype structural variants\n    merge        merge structural variants across VCF/BCF files and within a single VCF/BCF file\n    filter       filter somatic or germline structural variants\n</code></pre></p> <p>The software has two sequential steps. First, structural variants are predicted based on PEM signatures, in other words, when mapped pair insert size happens to be greater than expected.</p> <p>Paired-end options include, like all PEM-based software, the insert size cut-off (-s). The minimum paired-end mapping quality (-q) has been set to 20, therefore all reads with mapping quality below this threshold will be discarded. Structural variants detected by PEM will be outputted to the file indicated with \u2018-o\u2019. The specification of the standard deviation units (-s) is probably the most important cut-off for PEM methods. A deletion is detected when the pair mapped distance exceeds the upper threshold defined as s times (s = 3) the standard deviation (\u03c3) over the mean (\u03bc + 3\u03c3).</p> <p></p> <p>Second, Delly makes use of soft-clipped information to find exact breakpoints. Read splitting approaches rely on mapping broken reads that span breakpoints and are located in nearby locations. The structural variant signature in this case consists of one of the reads mapping to the reference and its split mate mapping with one of its parts approximately one insert size away. For insertions, the broken read will have its splits parts adjacent to each other, with a missing middle part of the read. In both cases, breakpoints can be resolved with base-pair precision.</p> <p></p> <p>Split-read options include the reference genome file name (-g), the output file name containing structural variants inferred by split-read method (-b) and other parameters.</p> <p>Delly software produces one output file per run, called sample1.delly.bcf and sample2.delly.bcf in our two runs. These are in a BCF-style format.Therefore you should run the following commands to get the VCF formatted files. As with short variants, we can also do some filtering to reduce our list to high-confidence variants.</p> <pre><code>bcftools view -i 'FILTER=\"PASS\" &amp;&amp; ALT=\"&lt;DEL&gt;\" &amp;&amp; (END-POS)&lt;20000' -c 2 sample1.delly.bcf &gt; sample1.delly.vcf\nbcftools view -i 'FILTER=\"PASS\" &amp;&amp; ALT=\"&lt;DEL&gt;\" &amp;&amp; (END-POS)&lt;20000' -c 2 sample2.delly.bcf &gt; sample2.delly.vcf\n</code></pre> <p>Bcftools will allow us to perform this filtering using the <code>-i</code> followed by a filtering string. This refers to values that are stored in the VCF file for each variant. Each expression is evaluated and multiple expressions can be chained with <code>&amp;&amp;</code>. Variants will be selected using the following criteria:</p> <ol> <li>Firstly, using <code>FILTER=\"PASS\"</code> we select all variants which have the FILTER parameter set to PASS. This is filter value is generated by delly and is a measure of its confidence in the variant.</li> <li>Then we keep only deletions with <code>ALT=\"&lt;DEL&gt;\"</code></li> <li>After that, using <code>(END-POS)&lt;20000</code>, we keep variants less than 20kb as larger variants are somewhat unrealistic in Mtb.</li> </ol> <p>Finally, with <code>-c 2</code> we select variants which delly believes are homozygous alternate. Delly assumes a diploid model and so can have heterozygous homozygous alternate depending on the amount of evidence. This doens't really make sense for Mtb but it is a hack to allow us to select only those with the most evidence.</p> <p>Now you should be able to open them in your favourite text viewer/editor (cat, less, nano, more or vim):</p> <p>Terminal output</p> <pre><code>Chromosome 79569 DEL00000002 C &lt;DEL&gt; . PASS PRECISE;SVTYPE=DEL;SVMETHOD=EMBL.DELLYv0.7.8; CHR2=Chromosome;END=83036;PE=73;MAPQ=60;CT=3to5;CIPOS=-6,6; CIEND=-6,6;INSLEN=0;HOMLEN=5;SR=10;SRQ=1; CONSENSUS=CCCGGTGGACCCGGTGGACCCGGTGGACCCGGTGGACGCCGTGGTC GCCGTGGGACGGGCGCGACGGCACGCCGCGGCCGCGTTGCGCTCCGG;CE=1.67548 GT:GL:GQ:FT:RCL:RC:RCR:CN:DR:DV:RR:RV 1/1:-112.094,-9.62652,0:96:PASS:1230:199:1027:0:1:80:0:32\n</code></pre>"},{"location":"introduction/variant-detection/#exercise-6-visualisation-of-predicted-structural-variants-using-igv","title":"Exercise 6: Visualisation of predicted structural variants using IGV","text":"<p>So far we have use IGV to visualise SNPs. In this exercise we will run it again to manually annotate larger deletions predicted by Delly tools in Exercise 5.</p> <p>SV detection tools output not only potential structural differences, but also a confidence value attach to them. An important threshold is the minimum number of supporting reads (found in Delly output) for an event to be retained. Hits with very few supporting reads (column 5) are expected to be false positives (FP). Repetitive and highly polymorphic regions in the genome are the main sources of such FP events.</p> <p>Launch an IGV instances as before (or use the same one if you kept it open) and load sample1.bam and sample2.bam in each of them as explained in Exercise 1.</p> <p>Using the same method as before load the VCF files produced by delly. But this time select the compressed VCF files in ~/data/tb/ (i.e. sample1.delly.vcf for the instance with sample1 loaded and sample2.delly.vcf for the instance with sample2 loaded).</p> <p>Find the region 77,823-84,798 and establish whether differences in the alignment can be visually identified between samples . Note: The coverage plot can also give you a clear insight.</p> <p></p> <p>Exercise</p> Question 3Answer 3 <p>Which characteristic signatures of deletions (gap of coverage, long-spanning read pairs and split-reads) can be identified? (Read the notes below to help answer this question) </p> <p>We can see a gap in the coverage and many reads which end abruptly at the same positions (split reads).</p> <p>References</p> <ol> <li> <p>Carter, N. (2007). Methods and strategies for analyzing copy number variation using DNA microarrays. Nature genetics, 39, 1\u201311. doi:10.1038/ng2028.Methods\u00a0\u21a9</p> </li> <li> <p>Cooper, G. M., Zerr, T., Kidd, J. M., Eichler, E. E., &amp; Nickerson, D. a. (2008). Systematic assessment of copy number variant detection via genome-wide SNP genotyping. Nature genetics, 40(10), 1199\u2013203. doi:10.1038/ng.236\u00a0\u21a9</p> </li> <li> <p>Medvedev, P., Stanciu, M., &amp; Brudno, M. (2009). Computational methods for discovering structural variation with next-generation sequencing. nature methods, 6(11), S13\u2013S20. doi:10.1038/NMEtH.1374\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>Li, H., &amp; Durbin, R. (2010). Fast and accurate long-read alignment with Burrows\u2013Wheeler transform. Bioinformatics, 26(5), 589\u2013595. Retrieved from http://www.ncbi.nlm.nih.gov/pubmed/20080505\u00a0\u21a9\u21a9</p> </li> <li> <p>Li, H. (2011). A statistical framework for SNP calling, mutation discovery, association mapping and population genetical parameter estimation from sequencing data. Bioinformatics (Oxford, England), 27(21), 2987\u201393. doi:10.1093/bioinformatics/btr509\u00a0\u21a9</p> </li> <li> <p>Danecek, P., Auton, A., Abecasis, G., Albers, C. a, Banks, E., DePristo, M. a, \u2026 Durbin, R. (2011). The variant call format and VCFtools. Bioinformatics (Oxford, England), 27(15), 2156\u20138. doi:10.1093/bioinformatics/btr330\u00a0\u21a9</p> </li> <li> <p>Sandgren, A., Strong, M., Muthukrishnan, P., Weiner, B. K., Church, G. M., &amp; Murray, M. B. (2009). Tuberculosis drug resistance mutation database. PLoS medicine, 6(2), e2. doi:10.1371/journal.pmed.1000002\u00a0\u21a9</p> </li> <li> <p>Sharon J. Peacock, P. D. (2013). Whole-Genome Sequencing for Rapid Susceptibility Testing of M. tuberculosis. The New England Journal of Medicine.\u00a0\u21a9</p> </li> <li> <p>Chiang, D., Getz, G., Jaffe, D., &amp; O\u2019Kelly, M. (2008). High-resolution mapping of copy-number alterations with massively parallel sequencing. Nature methods, 6(1), 99\u2013103. doi:10.1038/nmeth.1276.High-resolution\u00a0\u21a9</p> </li> <li> <p>Suzuki, S., Yasuda, T., &amp; Shiraishi, Y. (2011). ClipCrop: a tool for detecting structural variations with single-base resolution using soft-clipping information. BMC, 12(Suppl 14), S7. doi:10.1186/1471-2105-12-S14-S7\u00a0\u21a9</p> </li> <li> <p>Rausch, T., Zichner, T., Schlattl, a., Stutz, a. M., Benes, V., &amp; Korbel, J. O. (2012). DELLY: structural variant discovery by integrated paired-end and split-read analysis. Bioinformatics, 28(18), i333\u2013i339. doi:10.1093/bioinformatics/bts378\u00a0\u21a9</p> </li> </ol>"},{"location":"other-omics/eqtl/","title":"EQTL","text":""},{"location":"other-omics/eqtl/#introduction","title":"Introduction","text":"<p>An eQTL or expression Quantitative Trait Loci analysis is a statistical analysis through which the association of genomic DNA sequence variants and gene expression levels can be explored. An eQTL is therefore considered a locus that explains a fraction of the gene expression variance found in a phenotype through direct association tests between genetic markers and gene expression levels (Nika &amp; Dermitzakis, 2013).</p> <p>In the picture below a typical eQTL analysis is represented. Different SNPs within a 1Mb window on a determined genomic location (represented as blue lines) are tested with the expression levels of a given gene in different samples (harbouring and not harbouring the SNPs).</p> <p></p> <p>In the diagram below we can see ploted the expression levels of the samples with the different SNPs and the frequency of appearance of these SNPs. Therefore, both polymorphisms (AG and AA alleles) will be associated with the underexpression of the gene studied compared to the reference allele (GG). Thereby, these SNPs could be considered as eQTLs.</p> <p></p> <p>The eQTLs can be considered as cis-eQTLs or trans-eQTLs depending on the physical distance from the gene they regulate. The cis-eQTLs are in the adjacent regions of the gene they regulate while trans-eQTLs can be located in a distant genomic region.  </p> <p></p>"},{"location":"other-omics/eqtl/#exercise-1-eqtl-linked-with-snps","title":"Exercise 1: eQTL linked with SNPs","text":"<p>To perform an expression quantitative trait loci (eQTL) study linked with SNPs we are going to need two datasets: one with the SNPs we are going to test and the other one with the normalised expression for each gene in each sample. We are going to try to associate SNPs with differential expression, and, to do so, a statistical analysis will be carried out. In this case, we will perform a linear regression. But there are other statistical tests that can be run to study associations, such as ANOVA or Wilcoxon rank-sum test.</p>"},{"location":"other-omics/eqtl/#preparing-the-data","title":"Preparing the data","text":"<p>All the materials necessary for this exercise are located in the eqtl directory. You will find there a table with all the SNPs present in the genomic interval 2420631:2920631 (Rv2159c-Rv2590) of M. tuberculosis, and another table with the normalised expression data of the corresponding genes located in that genomic region. We are going to use the same 6 samples previously used for the differential expression analysis in the transcriptomics session, plus 9 additional lineage 1 and lineage 4 samples we have added to the dataset in order to increase the power of the statistical analysis. In total we will be analysing 15 strains, 7 from lineage 1 and 8 from lineage 4.</p> <p>We can take a look at the tables in the terminal. Just go to the directory and with the \"less\" command you can view the tables. From the home directory, type:</p> <pre><code>cd ~/data/eqtl\nless snps_eqtl_table.txt\n</code></pre> <p>To exit the view of the tables just type \"q\".</p> <p>Terminal output</p> <pre><code>POS     REF     ALT     L1_1    L4_2    L1_3    L1_4    L4_5    L4_6    L1_7   gene\n2421619 C       A       0       0       1       0       0       0       0       Rv2159c\n2421816 A       G       1       0       1       1       0       0       1       Rv2160A\n2423386 C       T       0       0       0       0       0       0       0       Rv2162c\n2423785 C       T       1       0       1       1       0       0       1       Rv2162c\n2423819 C       G       0       0       0       0       0       0       0       Rv2162c\n2423822 A       G       0       0       0       0       0       0       0       Rv2162c\n2423993 G       C       0       0       0       0       0       0       0       Rv2162c\n2424008 G       T       0       1       0       0       0       1       0       Rv2162c\n2424864 A       G       1       0       1       1       0       0       1       -\n2424908 G       T       0       1       0       0       0       1       0       -\n2424925 A       G       1       1       1       1       1       1       1       -\n2425023 G       A       0       0       0       0       0       0       0       -\n2425097 A       C       1       0       1       1       0       0       1       Rv2163c\n2425471 T       C       1       0       1       1       0       0       1       Rv2163c\n2426385 C       G       0       0       0       0       0       0       0       Rv2163c\n</code></pre> <p>In the SNPs table, each row represents one SNP across all the samples (columns). The first column indicates the genomic position of the SNP. The second and third ones refer to the reference and alternative nucleotide. The last one indicates whether the SNP falls within a gene (i.e. Rv2159c) or an intergenic region (-). The number 1 in the sample columns represents the presence of the SNP and the number 0 the absence.</p> <p>The expression table shows the normalised expression levels of each sample (columns) for each gene (rows).</p> <pre><code>less expression_eqtl_table.txt\n</code></pre> <p>Terminal output</p> <pre><code>gene    L1_1    L4_2    L1_3    L1_4    L4_5    L4_6    L1_7    L1_8    L4_9    L1_10\nRv2158c 5.3592  6.5914  5.3647  5.0135  7.282   7.4848  3.6513  5.2832  7.4075  6.2946\nRv2159c 0.9657  6.0428  0.0975  1.2378  7.2741  7.706   1.0288  3.2491  7.0289  1.6385\nRv2160A -0.6193 5.8577  -0.8708 0.8018  6.5427  6.8666  0.2404  2.482   6.0652  -0.5727\nRv2161c 2.683   9.4069  3.2019  3.0181  8.8226  9.4403  4.2647  4.9166  9.3021  2.7425\nRv2162c 4.7906  9.045   5.1631  5.4924  8.4655  9.7019  5.1945  5.6268  9.089   5.1375\nRv2163c 7.5358  7.5722  8.1996  7.4225  7.06    7.3848  8.5059  8.0789  7.5526  7.481\nRv2164c 5.5294  5.6485  6.6888  5.4251  5.5513  5.748   5.9732  6.5069  5.8695  5.8241\nRv2165c 7.3794  7.7184  8.3536  6.9752  6.9572  7.2733  8.1759  7.9825  7.9506  7.2009\nRv2166c 8.452   8.18    9.0769  7.7743  7.3996  7.6419  9.3086  8.7389  7.7578  7.9759\nRv2169c 6.7328  6.5983  7.1653  7.4208  7.6903  7.6155  5.6544  7.3472  7.1758  6.6482\nRv2170  3.0691  2.558   3.4853  3.4285  3.9117  3.7295  2.2728  3.8404  3.7938  3.8026\nRv2171  6.6739  5.8042  6.4029  5.9551  6.0585  6.0265  5.8921  6.1924  6.7047  6.7438\n</code></pre> <p>You should see similar tables to the ones showed above but with the 15 samples.</p>"},{"location":"other-omics/eqtl/#running-a-cis-eqtl-analysis","title":"Running a cis-eQTL analysis","text":"<p>We are going to run the analysis in R. So, after activating this practical's conda environment with conda activate eqtl, let's open R in the terminal, just typing:</p> <pre><code>R\n</code></pre> <p>And then we will load the packages we will need for the analysis. We will use a package called MatrixEQTL for the analysis and a package called ggplot2 for plotting some results.</p> <pre><code>library(MatrixEQTL)\nlibrary(ggplot2)\n</code></pre> <p>Firstly, we are going to set up the directories where we have the tables we are going to use. These are going to be the same we just saw above but formatted for using with the R package. The folder where you can find them is called \"tables\", which is in the eqtl directory. Now you should type:</p> <pre><code>base.dir = \"~/data/eqtl/tables/\"\nsnp_file_name = paste(base.dir, \"snps_table.txt\", sep=\"\")\nexpression_file_name = paste(base.dir, \"expression_table.txt\", sep=\"\")\noutput_file_name = \"~/data/eqtl/results_eqtl.txt\"\noutput_file_name.cis = \"~/data/eqtl/results_eqtl_cis.txt\"\n</code></pre> <p>Now we can load the tables for the analysis. We will use the following script:</p> <pre><code>expr = read.table(expression_file_name, sep=\"\\t\", header=T, row.names=1)\nsnp = read.table(snp_file_name, sep=\"\\t\", header=T, row.names=1)\n</code></pre> <p>And to see the first 6 rows of the tables we just need to type:</p> <pre><code>head(expr)\nhead(snp)\n</code></pre> <p>You should get something like this:</p> <p>Terminal output</p> <pre><code>&gt; head(expr)\n       L1_1   L4_2    L1_3   L1_4   L4_5   L4_6   L1_7   L1_8   L4_9\nRv2158c  5.3592 6.5914  5.3647 5.0135 7.2820 7.4848 3.6513 5.2832 7.4075\nRv2159c  0.9657 6.0428  0.0975 1.2378 7.2741 7.7060 1.0288 3.2491 7.0289\nRv2160A -0.6193 5.8577 -0.8708 0.8018 6.5427 6.8666 0.2404 2.4820 6.0652\nRv2161c  2.6830 9.4069  3.2019 3.0181 8.8226 9.4403 4.2647 4.9166 9.3021\nRv2162c  4.7906 9.0450  5.1631 5.4924 8.4655 9.7019 5.1945 5.6268 9.0890\nRv2163c  7.5358 7.5722  8.1996 7.4225 7.0600 7.3848 8.5059 8.0789 7.5526\n        L1_10  L1_11  L4_12   L4_13  L4_14  L4_15\nRv2158c  6.2946 5.7084 7.7072  8.4239 7.1863 7.7056\nRv2159c  1.6385 1.3116 7.7639  8.8317 7.1415 7.7607\nRv2160A -0.5727 0.2881 6.9682  7.5626 6.3030 6.8682\nRv2161c  2.7425 3.0210 9.4097 10.1360 9.1882 9.5215\nRv2162c  5.1375 4.8047 8.6629  9.3841 8.7615 9.4024\nRv2163c  7.4810 7.7594 7.7636  7.6428 7.6123 8.1138\n</code></pre> <pre><code>&gt; head(snp)\n        L1_1 L4_2 L1_3 L1_4 L4_5 L4_6 L1_7 L1_8 L4_9 L1_10 L1_11 L4_12 L4_13\n2421619    0    0    1    0    0    0    0    0    0     1     0     0     0\n2421816    1    0    1    1    0    0    1    1    0     1     1     0     0\n2423386    0    0    0    0    0    0    0    0    0     0     1     0     0\n2423785    1    0    1    1    0    0    1    1    0     1     1     0     0\n2423819    0    0    0    0    0    0    0    0    0     0     0     0     0\n2423822    0    0    0    0    0    0    0    0    0     0     0     0     0\n        L4_14 L4_15\n2421619     0     0\n2421816     0     0\n2423386     0     0\n2423785     0     0\n2423819     0     0\n2423822     0     0\n</code></pre> <p>We already have the tables ready so now we need to set up the parameters for the analysis.</p> <p>We are going to run a cis-eQTL analysis, which means we are going to test those SNPs that are located within a distance up to 200 bp from the beginning and end of each gene.</p> <p>The first step is to establish the threshold p-value. The program runs at the same time a trans and a cis eQTL analysis. As we are only interested now in a cis study, we will establish the threshold for the trans p-value in 0, and for the cis p-value in 1 (this means that the output file will contain all the associations up to a p-value of 1).</p> <pre><code>pvOutputThreshold_cis = 1\npvOutputThreshold_tra = 0\n</code></pre> <p>Now we are going to select the statistical model we want to use. In this case we will use linear regression:</p> <pre><code>useModel = modelLINEAR\n</code></pre> <p>The last thing is to load two more tables: one with the positions of the SNPs and other with the positions of the genes, which we will call snpspos and genepos. These two tables are in the directory 'tables', so to load them in our workspace we just need to type:</p> <pre><code>snpspos &lt;- read.table(paste(base.dir, \"snpspos.txt\", sep=\"\"), sep=\"\\t\", header=T)\ngenepos &lt;- read.table(paste(base.dir, \"genepos.txt\", sep=\"\"), sep=\"\\t\", header=T)\n</code></pre> <p>We can take a look at the tables typing:</p> <pre><code>head(snpspos)\nhead(genepos)\n</code></pre> <p>The last thing to do is to set up a few more parameters, we are not going to go into too much details for the purpose of this practical. Basically they prepare the matrix used for running the analysis and store our analysis and the results. To set this up, just copy the following script:</p> <pre><code>errorCovariance = numeric()\nsnp = SlicedData$new()\nsnp$fileDelimiter = \"\\t\"\nsnp$fileOmitCharacters = \"NA\"\nsnp$fileSkipRows = 1\nsnp$fileSkipColumns = 1\nsnp$fileSliceSize = 2000\nsnp$LoadFile(snp_file_name)\ngene = SlicedData$new()\ngene$fileDelimiter = \"\\t\"\ngene$fileOmitCharacters = \"NA\"\ngene$fileSkipRows = 1\ngene$fileSkipColumns = 1\ngene$fileSliceSize = 2000\ngene$LoadFile(expression_file_name)\ncvrt = SlicedData$new()\n</code></pre> <p>And finally, we will run the analysis:</p> <pre><code>me = Matrix_eQTL_main(\n    snps = snp,\n    gene = gene,\n    cvrt = cvrt,\n    output_file_name = output_file_name,\n    pvOutputThreshold = pvOutputThreshold_tra,\n    useModel = useModel,\n    errorCovariance = errorCovariance,\n    verbose = TRUE,\n    output_file_name.cis = output_file_name.cis,\n    pvOutputThreshold.cis = 1,\n    snpspos = snpspos,\n    genepos = genepos,\n    cisDist = 200,\n    pvalue.hist = TRUE,\n    min.pv.by.genesnp = FALSE,\n    noFDRsaveMemory = FALSE)\n</code></pre> <p>Terminal output</p> <pre><code>Matching data files and location files\n439of444 genes matched\n847of847 SNPs matched\n\nTask finished in 0.00199999999949796 seconds\nReordering genes\n\nTask finished in 0.231999999999971 seconds\nProcessing covariates\nTask finished in 0 seconds\nProcessing gene expression data (imputation, residualization)\nTask finished in 0.00100000000020373 seconds\nCreating output file(s)\nTask finished in 0.00399999999990541 seconds\nPerforming eQTL analysis\n100.00% done, 1,055 cis-eQTLs\nTask finished in 0.0150000000003274 seconds\n</code></pre> <p>Now let's take a look at the results table. To do it, type the following:</p> <pre><code>res &lt;- me$cis$eqtls\nhead(res)\n</code></pre> <p>Here we have the table with the 6 top associations found by the analysis just carried out.</p> <p>Terminal output</p> <pre><code>     snps    gene statistic       pvalue          FDR      beta\n1 2423785 Rv2162c -20.05509 3.668713e-11 1.935246e-08 -3.891237\n2 2424864 Rv2162c -20.05509 3.668713e-11 1.935246e-08 -3.891237\n3 2421816 Rv2160A -13.98178 3.275455e-09 1.151868e-06 -6.379346\n4 2421816 Rv2159c -13.41609 5.424159e-09 1.430622e-06 -6.082414\n5 2450045 Rv2188c -10.62618 8.853633e-08 1.556764e-05 -3.081468\n6 2451081 Rv2188c -10.62618 8.853633e-08 1.556764e-05 -3.081468\n</code></pre> <p>Question</p> <p>Which is the gene differentially expressed with the highest statistical association? Which is the p-value? And the corresponding adjusted p-value (or FDR)?</p> <p>Question</p> <p>With how many SNPs is it associated the variable expression of Rv2162c?</p> <p>To summarise, we ran a statistical analysis in order to see whether the SNPs present in our samples are associated with differential gene expression. We therefore got a table where we can see the SNP (its position, in snps column) associated with the differential expression of the gene (gene column). A high p-value association is established when a determined SNP is present in all the samples that have a determined gene under- or over-expressed.</p> <p>However, not all the associations found will be real. cis-eQTLs are more likely to be located within the promoter region of a gene, which is the upstream region where the transcription starts. For instance, a SNP in this promoter region might change the recognition patterns for transcriptional regulators and therefore alter gene expression.</p>"},{"location":"other-omics/eqtl/#further-exploring-the-results","title":"Further exploring the results","text":"<p>We will now check one of the resulting genes in the analysis to corroborate if it is a real eQTL association. We are going to take the first gene, Rv2162c. This gene has been associated with 2 SNPs. Let's take a look at the 2 SNPs. To do it, we will load again the SNPs table and look at the rows corresponding to those 2 SNPs:</p> <pre><code>snps_table &lt;- read.table(\"~/data/eqtl/snps_eqtl_table.txt\", sep=\"\\t\", header=T)\nsnps_table[snps_table$POS==2423785 | snps_table$POS==2424864,]\n</code></pre> <p>Terminal output</p> <pre><code>      POS REF ALT L1_1 L4_2 L1_3 L1_4 L4_5 L4_6 L1_7 L1_8 L4_9 L1_10 L1_11\n4 2423785   C   T    1    0    1    1    0    0    1    1    0     1     1\n9 2424864   A   G    1    0    1    1    0    0    1    1    0     1     1\nL4_12 L4_13 L4_14 L4_15    gene\n4     0     0     0     0 Rv2162c\n9     0     0     0     0       -\n</code></pre> <p>Question</p> <p>Which samples carry the SNPs?</p> <p>Are all of them from the same lineage? Which one?</p> <p>Are these SNPs located within the coding region or intergenic regions?</p> <p>We can see that all the lineage 1 samples harbour the two SNPs. One of them is located in an intergenic region, whilst the other is within the coding region of Rv2162c. As we mentioned before, it is more likely that a SNP present in an upstream or promoter region will affect transcription.</p> <p>If we go to the website https://mycobrowser.epfl.ch we will find a genomic database for different species of Mycobacterium. We are going to look for our gene of interest: write Rv2162c in the top right white box and select search.</p> <p></p> <p></p> <p></p> <p>Question</p> <p>Is the gene located in the forward or the reverse strand?</p> <p>Which is the functional category it belongs to?</p> <p>We can see the Rv2162c gene (or PE_PGRS38) represented in purple in the Mycobrowser.</p> <p>Question</p> <p>Is the SNP (A2424864G) in the downstream or upstream region?</p> <p>The SNP (A2424864G) is located in the promoter region of the Rv2162c, therefore it is a good candiate for being an eQTL. As the association showed a high p-value, we can expect that in all the samples carrying the SNP (in this case the lineage 1 samples) the Rv2162c gene would be under- or over-expressed. Looking at the expression table we could see this difference in expression, but a more visual way to do it is through a boxplot.</p> <p>To plot it we need to load a table with the expression of the Rv2162c:</p> <pre><code>rv2162 &lt;- read.table(\"~/data/eqtl/rv2162c.txt\", sep=\"\\t\", header=T)\nrv2162\n</code></pre> <p>Terminal output</p> <pre><code>        expg    gene lineage\nL1_1  4.7906 Rv2162c       1\nL4_2  9.0450 Rv2162c       4\nL1_3  5.1631 Rv2162c       1\nL1_4  5.4924 Rv2162c       1\nL4_5  8.4655 Rv2162c       4\nL4_6  9.7019 Rv2162c       4\nL1_7  5.1945 Rv2162c       1\nL1_8  5.6268 Rv2162c       1\nL4_9  9.0890 Rv2162c       4\nL1_10 5.1375 Rv2162c       1\nL1_11 4.8047 Rv2162c       1\nL4_12 8.6629 Rv2162c       4\nL4_13 9.3841 Rv2162c       4\nL4_14 8.7615 Rv2162c       4\nL4_15 9.4024 Rv2162c       4\n</code></pre> <p>And now let's do the plot copying the following script:</p> <pre><code>rv2162$lineage &lt;- as.factor(rv2162$lineage)\nbp &lt;- ggplot(rv2162, aes(x=gene, y=expg, fill=lineage)) +\ngeom_boxplot() + geom_point(position=position_jitterdodge())\nshow(bp)\n</code></pre> <p>You should get a boxplot like this, where the y axis represents the normalised expression data and the two boxplots are representing the lineage 1 and lineage 4 samples respectively:</p> <p></p> <p>Question</p> <p>Is Rv2162c differentially expressed between lineage 1 and 4 according to the boxplot?</p> <p>In which lineage is it less expressed?</p> <p>Can you think about any possible explanation for the underexpression in the lineage 1 samples carrying the SNP?</p>"},{"location":"other-omics/eqtl/#further-exploration","title":"Further exploration","text":""},{"location":"other-omics/eqtl/#trans-eqtl-analysis","title":"trans-eQTL analysis","text":"<p>We have carried out a cis-eQTL analysis, which means that we have only looked at those SNPs close to the genes differentially expressed. However, as we mentioned before, there are also trans-eQTLs. These trans-eQTLs would be, for instance, SNPs located in transcriptional regulators that might alter their function. Hence, transcription of those genes regulated by the altered transcriptional factors could be affected.</p>"},{"location":"other-omics/eqtl/#eqtl-linked-with-methylation","title":"eQTL linked with methylation","text":"<p>Methylation is also a mechanism of gene expression control (Casadesus &amp; Low., 2006) and therefore another approach we can take with eQTL analysis is to associate methylation with gene expression. Given the position of the motifs and methylated bases we could treat them as if they were SNPs and perform the same analysis.</p>"},{"location":"other-omics/eqtl/#references","title":"References","text":"<p>Nica AC and Dermitzakis ET. (2013). Expression quantitative trait loci: present and future. Philos Trans R Soc Lond B BIol Sci 368(1620):20120362. doi: 10.1098/rstb.2012.0362</p> <p>Casadesus J and Low D. (2006). Epigenetic gene regulation in the bacterial world. Micobiol Mol Biol Rev 70:830-56.</p>"},{"location":"other-omics/methylation/","title":"Methylation","text":""},{"location":"other-omics/methylation/#objectives","title":"Objectives","text":"<p>By the end of this practical you should: </p> <ul> <li>Understand the file formats used to represent methylation data from PacBio technology</li> <li>Know how to merge and perform quality control</li> <li>Visualise methylation data</li> <li>Analyse multi sample BCF files</li> </ul>"},{"location":"other-omics/methylation/#introduction","title":"Introduction","text":"<p>Over the last few practicals we have dealt with Illlumina and MinION data. Now we are going to have a look at another platform called Pacific Biosciences Single Molecule Real Time Sequencing (PacBio SMRT sequencing). This is another third-generation platform which produces long reads, similar to Oxford Nanopore. Like other long-read platforms the error rate is much higher than short-read platforms such as Illumina. As a consequence, the methods required to analyse the data also differ. </p> <p>PacBio technology has many applications, but is particularly useful for: </p> <ul> <li>Genome assembly</li> <li>Analysing methylation</li> </ul>"},{"location":"other-omics/methylation/#genome-assembly","title":"Genome assembly","text":"<p>In the previous practical we looked at using Illumina data to assemble the genome of several M. tuberculosis isolates. Although this helped with reconstructing some of the regions for which mapping doesn't work, it is not perfect. Genome reconstructions are often broken up into many contigs using short read data and are subject to errors in highly repetitive regions. Ultimately, the completeness of the assembly is limited by the length of the read. If the read is shorter than the length of the repeat there is no way of resolving how many copies of the repeat are present. This is where the read length of PacBio comes in handy. </p> <p>PacBio produces read lengths averaging 15Kb, with some reads greater than 100Kb in size. For most microbial genomes the read lengths will be much larger than any repeat sequence and therefore this often results in assemblies producing a complete chromosome in one contig. </p> <p>With complete genome assemblies, it is possible to identify:</p> <ul> <li>SNPs</li> <li>Small indels</li> <li>Large structural variants</li> <li>Novel insertions</li> </ul>"},{"location":"other-omics/methylation/#methylation_1","title":"Methylation","text":"<p>Methylation refers to the mechanism by which methyl groups (CH3) are added to DNA. This is often referred to as epigenetic modification. Methylation of DNA is facilitated by DNA methyltransferases (MTase), which mostly are part of restriction-modification systems. Restriction enzymes cut unmethylated DNA at a specific motifs (recognition sites), while the paired methyltransferase methylates the same motif. Traditionally, DNA methylation in bacteria has been seen as a primitive immune mechanism to fight against invading phages. When a bacterium is invaded by foreign DNA, its restriction enzyme will cut it if the recognition site is present on the DNA, thus neutralising the threat. There is one problem however. The restriction enzyme is not specific to foreign DNA and will also cut its own genome. The solution comes from the MTase. The MTase will methylate the same motif on the bacterium's genome, preventing the restriction enzyme from cutting. </p> <p>Although this is an important function of methylation, it is also thought to modulate the binding of other DNA-binding proteins and play a role in gene expression which is important in the interaction of a pathogen with its environment. </p> <p>PacBio technology works by recording light signals emitted as a DNA polymerase incorporates fluorescently labelled nucleotides while replicating input DNA. The data from the PacBio platform is stored in a h5 format. This contains information on the base calls and the time it takes to incorporate each base (inter pulse duration). The polymerase takes longer to incorporate nucleotides on methylated input DNA than non-methylated. By comparing the time spent between each incorporation event and comparing it to an in-silico control it is possible to calculate the inter pulse duration ratio (IPD ratio). This makes it possible to detect DNA modification to a single base precision. </p> <p>After the location of all methylation sites have been found, the context of these sites are analysed to look for enrichment of particular sequence motifs. If a particular motif is found more than expected by chance, then it is likely a recognition site for an MTase. </p> <p></p>"},{"location":"other-omics/methylation/#exercise-1-analysing-motif-summary-reports","title":"Exercise 1: Analysing motif summary reports","text":"<p>Activate the conda environment, navigate to the methylation practical directory, and take a look at the contents: </p> <pre><code>conda activate methylation\ncd ~/data/methylation\nls\n</code></pre> <p>There are several files present. We will first take a look at the files ending with .motif_summary.csv. We can open these files using excel or a similar program. Linux has an open source package that is quite similar to Excel called gnumeric. Let's use it to view the contents of tb_pb_1.motif_summary.csv. </p> <pre><code>gnumeric tb_pb_1.motif_summary.csv\n</code></pre> <p>You should now see a spreadsheet containing a number of different columns: </p> <ul> <li>motifString: Detected motif sequence for this site such as \u201cGATC\u201d.</li> <li>centerPos: Position in motif of modification (0-based).</li> <li>modificationType: Modification type \u2013 a generic tag \"modified_base\" is used for unidentified bases. For identified bases, m6A, m4C, and m5C are used.</li> <li>fraction: The percent of time this motif is detected as modified in the genome. (Fraction of instances of this motif with modification QV or identification QV above the QV threshold.)</li> <li>nDetected: Number of instances of this motif that are detected as modified. (Number of instances of this motif with modification QV or identification QV above threshold.)</li> <li>nGenome: Number of occurrences of this motif in the reference sequence genome.</li> <li>groupTag: A name identifying the complete double-strand recognition motif. For paired motifs this is \u201c/\u201d, for example \u201cGAGA/TCTC\u201d. For palindromic or unpaired motifs this is the same as motifString.</li> <li>partnerMotifString: motifString of paired motif (motif with reverse-complementary motifString).</li> <li>meanScore: Mean Modification QV of instances of this motif that are detected as modified.</li> <li>meanIpdRatio: Mean IPD ratio of instances of this motif that are detected as modified.</li> <li>meanCoverage: Mean coverage of instances of this motif that are detected as modified.</li> <li>objectiveScore: Score of this motif in the motif finder algorithm. The algorithm considers higher objective scores to be more confidently identified motifs in the genome based on several factors.</li> </ul> <p>We can see that three unique motifs have been detected (CACGCAG, CTGGAG and CTCCAG). If you look at the second and third motifs you may have noticed that the 'groupTag' is the same. These sequences are actually palindromic sequences, i.e. the reverse complement of 'CTGGAG' is 'CTCCAG' and vice versa. At these motifs, methylation occurs on both strands of DNA on the same motifs at the A nucleotide. The 'CACGCAG' motif, on the other hand, is only methylated at one position. </p> <p>Question</p> <p>Take a look at the fraction column. Is the motif always methylated? Is there any relationship between the size of the motif and the number of times it was detected? Why do you think this is? </p> <p>Using the same method as described above, open up 'tb_pb_3.motif_summary.csv'. Can you see any new motifs? This isolate has many more motifs that have been reported. You may notice that some motifs use symbols other than the standard nucleotides. The motifs are represented using the IUPAC standard nomenclature for representing nucleotides. For example, in the 'GATNNNNRTAC' motif the N represents any nucleotide and the R represents either an A or a G. This means that the MTase which methylates this motif will ignore the 4th-7th positions. </p> <p>Question</p> Question 1Answer 1 <p>An MTase recognises the motif 'GATNNNNRTAC'. Which of these sequences will it methylate?</p> <p>A: CATGTCAATAC</p> <p>B: GATGTCAATAC</p> <p>C: GATGTCACTAC</p> <p>B: GATGTCAATAC</p> <p>There are several motifs with very low 'fraction'. If an MTase is active it would be expected to methylate most of the sites in the genome. Motifs with low fraction can potentially represent a false motif. As in variant detection, with these noisy datasets false motifs can be introduced. </p> <p>Question</p> <p>Take a look at the 'meanScore' column. Are they all similar values? Do you see any relationship between 'fraction' and 'meanScore'? </p> <p>We can use the 'meanScore' column to filter out false motifs. Take a look at a few more .motif_summary.csv files and see if you can find any overlaps between the motifs found. </p> <p>We can visualise the methylation by plotting the IPD ratio against the motif position. To do this we must: </p> <ol> <li>Find the locations of the motif in the genome</li> <li>Extract the IPD ratio for each motif base in the genome</li> <li>Plot the IPD ratio against the position in the motif</li> </ol> <p>The modifications and motifs pipeline also provides a CSV file containing all the positions in the genome as rows and several columns of information for each position (including the IPD ratio). Take a look at an example: </p> <pre><code>zcat tb_pb_14.ipd.csv | head\n</code></pre> <p>This is a very large file, so we can't open it with <code>gnumeric</code>. The first few lines should look like the example below. </p> <p>Terminal output</p> <pre><code>refName,tpl,strand,base,score,tMean,tErr,modelPrediction,ipdRatio,coverage,frac,fracLow,fracUp\n\"WBB445_ARS7496|quiver\",1,0,T,2,0.644,0.189,0.696,0.926,7,,,\n\"WBB445_ARS7496|quiver\",1,1,A,0,0.576,0.204,0.882,0.653,8,,,\n\"WBB445_ARS7496|quiver\",2,0,T,6,0.766,0.264,0.597,1.283,8,,,\n\"WBB445_ARS7496|quiver\",2,1,A,5,0.927,0.237,0.804,1.153,8,,,\n\"WBB445_ARS7496|quiver\",3,0,G,7,1.231,0.589,0.689,1.788,9,,,\n\"WBB445_ARS7496|quiver\",3,1,C,1,0.425,0.169,0.554,0.768,8,,,\n\"WBB445_ARS7496|quiver\",4,0,A,1,0.919,0.260,1.275,0.720,9,,,\n\"WBB445_ARS7496|quiver\",4,1,T,1,0.428,0.192,0.696,0.615,9,,,\n\"WBB445_ARS7496|quiver\",5,0,C,6,0.936,0.277,0.722,1.296,9,,,\n</code></pre> <p>We will use the <code>analyse_motif_ipd.py</code> script to extract the IPD ratios from this file. We need to provide 1) the ipd.csv file, 2) the motif you would like to analyse and 3) the genome assembly for the sample (to find the motif locations). Let's analyse the 'CTCCAG' motif for tb_pb_14 and tb_pb_16: </p> <pre><code>python analyse_motif_ipd.py tb_pb_14.ipd.csv.gz CTCCAG tb_pb_14.assembly.fa\npython analyse_motif_ipd.py tb_pb_16.ipd.csv.gz CTCCAG tb_pb_16.assembly.fa \n</code></pre> <p>Question</p> <p>The script will output the number of times the motif was found in the genome. Why do you think there are differences? </p> <p>Now we can visualise this with the <code>plot_ipd.R</code> script. We need to provide 1) the sample names, 2) the motif we are analysing and 3) the output file. The output will be in PDF format. </p> <pre><code>Rscript plot_ipd.R tb_pb_14,tb_pb_16 CTCCAG CTCCAG.pdf\n</code></pre> <p>Using the file browser, locate the PDF file ~/data/methylation/. Double click on the CTCCAG.pdf file and it should open in a PDF viewer program. You should be able to see a figure similar to the one below. </p> <p></p> <p>You should be able to see that tb_pb_16 has elevated IPD ratios (indicating methylation) on the 5th position of the motif while tb_pb_14 does not. Check to see if this is concordant with the motif_summary CSV files for these samples. </p> <p>We will now create the same plot for the 'GTAYNNNNATC' motif for samples tb_pb_1 and tb_pb_4. </p> <pre><code>python analyse_motif_ipd.py tb_pb_2.ipd.csv.gz GTAYNNNNATC tb_pb_2.assembly.fa\npython analyse_motif_ipd.py tb_pb_16.ipd.csv.gz GTAYNNNNATC tb_pb_16.assembly.fa\nRscript plot_ipd.R tb_pb_16,tb_pb_2 GTAYNNNNATC GTAYNNNNATC.pdf\n</code></pre> <p>Question</p> Question 2Answer 2 <p>Which sample has evidence for methylation at the 'GATNNNNRTAC' motif.</p> <p>Sample tb_pb_16. There are elevated IPD ratios on the third position (A).</p> <p>Finally try to follow to same commands to visualise the methylation at 'CACGCAG' for samples tb_pb_16 and tb_pb_2.</p> <p>Let's try and combine all the individual datasets into a merged dataset. We have created a script to take the CSV files and create a matrix where the rows represent samples, the columns represent motifs and the cells represents the fraction that the motif is methylated in a particular sample. It takes an input file with the names of all the CSV files which we must first create. </p> <p>To do this we can run the following command: </p> <pre><code>ls *.motif_summary.csv &gt; files.txt\n</code></pre> <p>Take a look at the file you just created by running <code>head files.txt</code>.</p> <p>It should look something like this: </p> <p>Terminal output</p> <pre><code>tb_pb_1.motif_summary.csv\ntb_pb_10.motif_summary.csv\ntb_pb_11.motif_summary.csv\ntb_pb_12.motif_summary.csv\ntb_pb_13.motif_summary.csv\ntb_pb_14.motif_summary.csv\ntb_pb_15.motif_summary.csv\ntb_pb_16.motif_summary.csv\ntb_pb_17.motif_summary.csv\ntb_pb_2.motif_summary.csv\n</code></pre> <p>We can now pass this file to the <code>combine_motifs.py</code> script: </p> <pre><code>python combine_motifs.py files.txt unfiltered_motifs.csv\n</code></pre> <p>Take a look at the 'unfiltered_motifs.csv' file using gnumeric. There are many motifs which are present in only one sample. These likely represent noise in the data and should be filtered out. Rerun the command with a quality filter: </p> <pre><code>python combine_motifs.py files.txt filtered_motifs.csv --min_qual 60\n</code></pre> <p>We have specified the minimum QV of the motif to be 60. Take a look at the filtered_motifs.csv file using gnumeric and look at the difference. You should now have 5 motifs. This will serve as our final high-quality list of motifs.</p> <p>It is evident that some samples have methylation on certain motifs while others do not. We will now try to understand if there is a particular pattern to the methylation seen in the data. The methylation pattern can either be random or specific to a certain strain. To do this we will reconstruct the phylogeny and overlay the methylation information.</p> <p>Using the same raw data and the SMRT portal analysis suite we have generated whole genome assemblies for the samples. These were then aligned to the reference and variants were called. The variants from all the samples were merged to a single FASTA formatted file. We can use this file to create the phylogenetic tree. Try to remember the command to create the tree and run it in the terminal using 'pacbio.fasta' as the input fasta and 'pacbio.ML' as the output name. If you need the solution click on the button below. </p> <p>Question</p> Task 1Solution 1 <p>Create a phylogenetic free using the pacbio.fasta file as input.</p> <pre><code>iqtree -m GTR+G -s pacbio.fasta -bb 1000\n</code></pre> <p>Open up the tree by launching <code>figtree</code>. Open the tree by clicking on Open... and selecting the 'RAxML_bestTree.pacbio.ML' tree. Midpoint root the tree by selecting Midpoint Root from the Tree menu. Finally, we will load annotations allowing figtree to display which samples do or don't have methylation. Select Import annotations... from the File menu and select the 'filtered_motifs.tsv' file. This file was created during the merging step and is simply a tab-separated file with the rows being samples and the columns being motifs. The values in the file are either 0 (representing absence of methylation) or 1 (representing presence of methylation). </p> <p>Once the file has been loaded, we can colour the tips by selecting a motif sequence from the 'Colour by' dropdown on the 'Tip labels' panel (shown below). Samples for which methylation is absent will be coloured red. </p> <p></p> <p>Question</p> <p>Look at the different methylation patterns by colouring the tips. Is it random? </p>"},{"location":"other-omics/methylation/#methylation-and-mutations","title":"Methylation and mutations","text":"<p>Methylation in the five motifs has been linked to the following genes: </p> Motif Genes CTCCAG/CTGGAG mamA GTAYNNNNATC/GATNNNNRTAC hdsS.1, hsdM and hsdS CACGCAG mamB <p>Loss of function mutations in MTases can lead to the absence of methylation. We are going to take a look at the CTCCAG/CTGGAG motif which is methylated by the mamA MTase. This protein is encoded by the Rv3263 gene. The methylation pattern it shown on the tree below: </p> <p></p> <p>The 'filtered_motifs.csv' file and the phylogenetic tree indicates that three of the samples have no methylation on the motif (tb_pb_10, tb_pb_11 and tb_pb_14). There are a few scenarios which may be possible: </p> <ol> <li>The isolates all have the same variant which has evolved convergently (where the same mutation has appeared multiple times independently on the tree).</li> <li>The isolates all have different mutations</li> <li>Some isolates share a common mutation</li> </ol> <p>The variants found by aligning the whole genome assemblies to the reference are stored in the multi-sample VCF file 'pacbio.vcf.gz'. We will use bcftools to process this file and extract the relevant information we need . </p>"},{"location":"other-omics/methylation/#1-extracting-sample-specific-mutations","title":"1. Extracting sample-specific mutations","text":"<p>The first thing we need to do is extract variants which are only present in the three samples. We can do this using the bcftools view command. First let find out how many variants are present in the VCF file: </p> <pre><code>bcftools view pacbio.vcf.gz -H | wc -l\n</code></pre> <p>The command can be broken down into several parts: </p> <ul> <li><code>bcftools view pacbio.vcf.gz</code>: allows to view contents of the VCF file</li> <li><code>-H</code>: This flag prevents header lines present in the file to be output</li> <li><code>|</code>: The pipe passes the output from whatever command was written before it and passes it to the next.</li> <li><code>wc -l</code>: This command counts the number of lines which are passed to it</li> </ul> <p>After running the command we should see an output of 9229, i.e. 9229 variants are present across all samples. We will now add two more parameters to the command. We select only variants present in a select number of samples using the -s flag. We can also restrict our analysis to variants which are present exclusively in our samples using the -x flag. The command will be the following: </p> <pre><code>bcftools view pacbio.vcf.gz -H  -s tb_pb_10,tb_pb_14,tb_pb_11 -x | wc -l\n</code></pre> <p>The command will now count 780 variants. </p>"},{"location":"other-omics/methylation/#2-annotating-the-variants","title":"2. Annotating the variants","text":"<p>In order to narrow down the number of variants we need to narrow down out search to only variants in the Rv3263 gene. The VCF file currently only contains information on the position of the variants on the chromosome but no information about genes. We can use <code>snpEff</code> to perform the annotation: </p> <pre><code>bcftools view pacbio.vcf.gz -s tb_pb_10,tb_pb_14,tb_pb_11 -x | snpEff ann Mycobacterium_tuberculosis_h37rv  -no-upstream -no-downstream | grep Rv3263\n</code></pre> <p>We have dropped the -H as we are no longer counting lines and the header is needed by the next command. The following new parts have been added:</p> <ul> <li><code>snpEff ann</code>: This snpEff function annotates a VCF file</li> <li><code>Mycobacterium_tuberculosis_h37rv</code>: Provides the database with annotations. There are many different databases available for different organisms. The full list can be seen by running <code>snpEff databases</code></li> <li><code>-no-upstream -no-downstream</code>: This flag prevents the annotation of variants which are upstream or downstream of a gene</li> <li><code>grep Rv3263</code>: Looks for lines containing 'Rv3263' and prints them</li> </ul> <p>We can see this command prints out a number of lines. The last two lines represent two variants in VCF format:</p> <p>Terminal output</p> <pre><code>Chromosome      3643985 .       A       C       225.0   PASS    VDB=0.702241;SGB=-0.693147;MQSB=0.911099;MQ0F=0;MQ=57;DP=308;DP4=0,0,150,158;MinDP=28;AN=6;AC=4;ANN=C|missense_variant|MODERATE|Rv3263|Rv3263|transcript|CCP46082|protein_coding|1/1|c.809A&gt;C|p.Glu270Ala|809/1662|809/1662|270/553||      GT:DP:PL:AD     1/1:144:255,255,0:0,144    1/1:164:255,255,0:0,164 0/0:117:.:.\nChromosome      3644554 .       G       A       225.0   PASS    VDB=0.847222;SGB=-0.693147;MQSB=0.91807;MQ0F=0;MQ=57;DP=145;DP4=0,0,75,70;MinDP=14;AN=6;AC=2;ANN=A|missense_variant|MODERATE|Rv3263|Rv3263|transcript|CCP46082|protein_coding|1/1|c.1378G&gt;A|p.Ala460Thr|1378/1662|1378/1662|460/553||      GT:DP:PL:AD     0/0:14:.:.0/0:121:.:.      1/1:145:255,255,0:0,145\n</code></pre>"},{"location":"other-omics/methylation/#3-customising-output-format","title":"3. Customising output format","text":"<p>This format can be a little difficult to understand so the last part we will add to this command will translate this to a more readable format:</p> <pre><code>bcftools view pacbio.vcf.gz -s tb_pb_10,tb_pb_14,tb_pb_11 -x | snpEff ann Mycobacterium_tuberculosis_h37rv  -no-upstream -no-downstream | bcftools query -f '[%POS\\t%SAMPLE\\t%ANN\\n]' | grep Rv3263\n</code></pre> <p>We have moved the grep command to the end and added in the following parameters:</p> <ul> <li><code>bcftools query</code>: This command allows conversion of VCF into custom formats</li> <li><code>-f '[%POS\\t%SAMPLE\\t%ANN\\n]'</code>: This specifies the format we want (Position, Sample name and annotations)</li> </ul> <p>Terminal output</p> <pre><code>3643985 tb_pb_10        1/1     C|missense_variant|MODERATE|Rv3263|Rv3263|transcript|CCP46082|protein_coding|1/1|c.809A&gt;C|p.Glu270Ala|809/1662|809/1662|270/553||\n3643985 tb_pb_14        1/1     C|missense_variant|MODERATE|Rv3263|Rv3263|transcript|CCP46082|protein_coding|1/1|c.809A&gt;C|p.Glu270Ala|809/1662|809/1662|270/553||\n3643985 tb_pb_11        0/0     C|missense_variant|MODERATE|Rv3263|Rv3263|transcript|CCP46082|protein_coding|1/1|c.809A&gt;C|p.Glu270Ala|809/1662|809/1662|270/553||\n3644554 tb_pb_10        0/0     A|missense_variant|MODERATE|Rv3263|Rv3263|transcript|CCP46082|protein_coding|1/1|c.1378G&gt;A|p.Ala460Thr|1378/1662|1378/1662|460/553||\n3644554 tb_pb_14        0/0     A|missense_variant|MODERATE|Rv3263|Rv3263|transcript|CCP46082|protein_coding|1/1|c.1378G&gt;A|p.Ala460Thr|1378/1662|1378/1662|460/553||\n3644554 tb_pb_11        1/1     A|missense_variant|MODERATE|Rv3263|Rv3263|transcript|CCP46082|protein_coding|1/1|c.1378G&gt;A|p.Ala460Thr|1378/1662|1378/1662|460/553||\n</code></pre> <p>Tip</p> <p>The 3rd column is the genotype of the sample. Each sample will have a genotype entry for each variant. A value of 0/0 means  reference, and 1/1 means alternate. We can select only samples with an alternate genotype by using the command. Try adding <code>| awk '$3==\"1/1\"'</code> to the end of the command.</p> <p>From the output (above) we can see that tb_pb_10 and tb_pb_14 both have the same mutation (270E&gt;270A) while tb_pb_11 has a different mutation (460A&gt;460T). These mutations are good candidates to take towards functional studies to validate the loss of function effect on the MTase.</p> <p>You should now know how to analyse and interpret data from the PacBio platform and take it all the way to discovering interesting mutations using BCFtools. This is the end of the practical, but if you want more practice you can try find candidate mutations for the other motifs. Good luck! </p>"},{"location":"other-omics/microbiome/","title":"Microbiomes Practical","text":"<p>Bacterial vaginosis (BV) is a dysbiotic condition caused by excessive growth of certain bacteria replacing the regular vaginal microbiome. Common symptoms include increased discharge, burning with urination, and itching. BV increases the risk of infection by a number of sexually transmitted infections including HIV/AIDS as well as the risk of early delivery when pregnant. The changed composition of the microbiome leads to a higher pH and a hundred to thousand-fold increase in the total number of bacteria present.</p>"},{"location":"other-omics/microbiome/#getting-the-data","title":"Getting the data","text":"<p>For this practical we are considering 12 samples of vaginal swab that were taken at a polyclinic by a GP in a setting of high transmission of HIV. DNA was extracted from the swabs and amplified using primers specific for the first two hypervariable regions (V1 and V2) of the 16S rRNA gene (27F and 338R). These samples were then sequenced with MiSeq Illumina producing paired end data of 300 bp length per read. The 12 pairs of files generated are found in the data/microbiome/fastq/ directory. The patients\u2019 phenotype was determined by the doctors at the time of sample collection with the following results:</p> Sample BV pH BB_1 no 4.4 BB_2 no 3.6 BB_3 yes 5.5 BB_4 no 5.3 BB_5 yes 5.6 BB_6 yes 5.3 BB_7 yes 4.7 BB_8 no 4.4 BB_9 no 4.4 BB_10 no 3.6 BB_11 yes 4.7 BB_12 yes 5"},{"location":"other-omics/microbiome/#analysing-the-microbiome-samples-with-qiime2","title":"Analysing the microbiome samples with QIIME2","text":"<p>Out of all tookits aiming at the unification of the analysis of microbiome data, QIIME (pronounced \"chime\") and its successor QIIME2 have grown the largest user base in recent years (mostly due to the ease of use and comprehensive online documentation). QIIME2 wraps an extensive suite of third party tools (covering most of the \"standard\" microbiome pipeline from preprocessing and filtering of raw sequencing reads to statistical tests on diversity metrics and analyses on differential abundance of single taxa) into a single command line interface. In addition, it also provides a GUI as well as a python API for both less and more technically inclined users. We will stay on the middle ground by using the CLI today. One idiosyncrasy of QIIME2 is the use of so-called \"artefacts\". These are zip-archives with a special file extension (.qza for data artefacts and .qzv for visualisation artefacts) that hold bulk data in addition to unique IDs and provenance metadata, which describe all steps that lead to the creation of that particular artefact. This has the advantage that for every intermediate or final result of qiime it is perfectly clear how it was generated from start to finish. There is a small downside, though, since we have to import our data into the QIIME2 format prior to running any analyses. However, before we do that, let's have a look at the quality of our reads (qiime also provides functionality for sequencing data quality control, but it is not as detailed as the output of some dedicated tools like FastQC). </p>"},{"location":"other-omics/microbiome/#quality-control","title":"Quality control","text":"<p>After activating the conda environment for this practical with <code>conda activate microbiome</code>, go into the module directory with <code>cd data/microbiome</code> and have a look at its contents with <code>ls</code>. There should be a directory with the 16S sequencing data (in fastq), a 16S database (in db), and a CSV file with our metadata. Let's check if our reads are there with <code>ls fastq</code>. We can also have a look at the filesizes with <code>du -sh fastq/* | sort -h</code> (it can't hurt to get a feeling for these things). </p> <pre><code>mkdir fastqc_reports\n\nfastqc -o fastqc_reports -q -t 1 fastq/*\n</code></pre> <p>Info</p> <p>The -t flag tells fastqc the number of threads to use. If you have more CPUs available, adjust this number accordingly. </p> <p>This should produce a FastQC report for each fastq file and put them all into fastqc_reports (run <code>ls fastqc_reports</code> to double check). Going through 24 FastQC reports (two per sample; one for the forward and one for the reverse reads) manually would be quite tedious. Thankfully, multiqc can combine them for us! Let's create a new directory for it to write the results into and run it. </p> <pre><code>mkdir fastqc_combined\n\nmultiqc -o fastqc_combined fastqc_reports\n</code></pre> <p>With <code>ls fastqc_combined</code> you can see that an HTML file, which we can view in a browser, has been created. Open the HTML file using the command <code>firefox ~/data/microbiome/fastqc_combined/multiqc_report.html</code></p> <p>Scroll through the report and make note of the sequence counts barplots and the quality histograms. </p> <p></p> <p></p> <p>In the counts plot we can see that two samples (BB_3 and BB_8) have significantly fewer reads than the others. This looks like something went wrong during library preparation for these two samples and we should exclude them from further analysis. Also note that the reads of the other samples have decent quality up until ~200 bp length. We will need this information later. </p> <p>Question</p> <p>The difference in file sizes of the sequencing files between BB_3 / BB_8 and the other samples was not as drastic as the difference in actual read counts. Can you think of a reason why that might be? Hint: What does the file extension .gz mean? </p> <p>For this dataset, the primers have already been trimmed from the reads and the FastQC output showed us that there are no adapters that would need removal. Also, quality-based trimming is discouraged when using DADA2 (an important step in our later analyses). Therefore, no further pre-processing is needed and we can transform the data into the qiime format. </p>"},{"location":"other-omics/microbiome/#import-into-qiime","title":"Import into qiime","text":"<p>In order to do this, qiime needs a tab-separated file with the sample IDs and the absolute paths to the forward and reverse reads. There are many ways to create such a file (and if you have just a few samples you can simply type it by hand). We will use the opportunity to string a few handy command line utilities together that we have not seen so far. First, let's write the header line of the import-list to a new file: </p> <pre><code>printf \\\n    \"sample-id\\tforward-absolute-filepath\\treverse-absolute-filepath\\n\" \\\n    &gt; fastq_abs_paths\n</code></pre> <p>Info</p> <p>We use backslashs here to break this command into multiple lines. </p> <p>Then, we append the lines corresponding to our samples to the file that was just created. We can achieve this with </p> <pre><code>ls fastq | grep -oE 'BB_[0-9]+' | sort -t _ -k 2 -n | uniq | \\\n      grep -vE 'BB_[38]' | \\\n      awk -v path=$(pwd)/fastq/ 'OFS=\"\\t\" \\\n          {print $1, path $1 \"_1.fastq.gz\", path $1 \"_2.fastq.gz\"}' \\\n      &gt;&gt; fastq_abs_paths\n</code></pre> <p>Info</p> <p>With the first grep we get the sample IDs from the filenames. We then sort them numerically (-n) based on the second field (-k 2) when split at underscores (-t _). Since there are two files per sample, we only keep the uniq sample IDs before removing the low-read-counts samples (BB_3 and BB_8) with another grep (grep -v keeps all lines that do not match the regex). The remaining sample IDs are subsequently fed to awk in order to print the absolute paths which are finally appended to fastq_abs_paths. </p> <p>This should have done the trick. <code>cat fastq_abs_paths</code> let's us see what we got. The output should look like this: </p> <pre><code>sample-id       forward-absolute-filepath       reverse-absolute-filepath\nBB_1    /home/user/data/microbiome/fastq/BB_1_1.fastq.gz      /home/user/data/microbiome/fastq/BB_1_2.fastq.gz\nBB_2    /home/user/data/microbiome/fastq/BB_2_1.fastq.gz      /home/user/data/microbiome/fastq/BB_2_2.fastq.gz\nBB_4    /home/user/data/microbiome/fastq/BB_4_1.fastq.gz      /home/user/data/microbiome/fastq/BB_4_2.fastq.gz\nBB_5    /home/user/data/microbiome/fastq/BB_5_1.fastq.gz      /home/user/data/microbiome/fastq/BB_5_2.fastq.gz\nBB_6    /home/user/data/microbiome/fastq/BB_6_1.fastq.gz      /home/user/data/microbiome/fastq/BB_6_2.fastq.gz\nBB_7    /home/user/data/microbiome/fastq/BB_7_1.fastq.gz      /home/user/data/microbiome/fastq/BB_7_2.fastq.gz\nBB_9    /home/user/data/microbiome/fastq/BB_9_1.fastq.gz      /home/user/data/microbiome/fastq/BB_9_2.fastq.gz\nBB_10   /home/user/data/microbiome/fastq/BB_10_1.fastq.gz     /home/user/data/microbiome/fastq/BB_10_2.fastq.gz\nBB_11   /home/user/data/microbiome/fastq/BB_11_1.fastq.gz     /home/user/data/microbiome/fastq/BB_11_2.fastq.gz\nBB_12   /home/user/data/microbiome/fastq/BB_12_1.fastq.gz     /home/user/data/microbiome/fastq/BB_12_2.fastq.gz\n</code></pre> <p>Great! This should be sufficient to let QIIME2 know where the files that we want to import are. Now, we can import the reads with </p> <pre><code>qiime tools import \\\n    --type 'SampleData[PairedEndSequencesWithQuality]' \\\n    --input-path fastq_abs_paths \\\n    --output-path fastq_imported.qza \\\n    --input-format PairedEndFastqManifestPhred33V2\n</code></pre> <p>This hopefully finishes successfully in a few seconds. Afterwards, you can check whether a new file was created with <code>ls</code> (which is slowly becoming our best friend now \u2013 right after <code>cd</code> of course). </p> <p>qiime also requires the metadata to be in TSV (tab-separated values), whereas our file is a CSV (comma-separated). We can simply fix this with </p> <pre><code>cat meta.csv | tr ',' '\\t' &gt; meta.tsv\n</code></pre>"},{"location":"other-omics/microbiome/#denoising-with-dada2","title":"Denoising with DADA2","text":"<p>Now that we have imported the data we can unleash the power of qiime! Sequence denoising (or OTU clustering) is the centrepiece of every 16S pipeline. We will use the DADA2 which fits a sequencing error model to the data and tries to merge (\"denoise\") sequences that differ only due to sequencing errors as opposed to actual biological variation. </p> <pre><code>qiime dada2 denoise-paired \\\n    --i-demultiplexed-seqs fastq_imported.qza \\\n    --p-trunc-len-f 190 \\\n    --p-trunc-len-r 190 \\\n    --p-n-threads 1 \\\n    --verbose \\\n    --o-table table.qza \\\n    --o-representative-sequences rep_seqs.qza \\\n    --o-denoising-stats denoising_stats.qza\n</code></pre> <p>This will produce an artefact holding a list of unique sequences (rep_seqs.qza) as well as a table with the number of occurrences of each representative sequence per sample (table.qza). DADA2 fits the error model on all reads of a sequencing run simultaneously (as opposed to Deblur, which fits it separately for each sample). It is therefore considerably slower and we will have to wait a few minutes for it to finish. In the meantime you can have a look at the later sections of the practical. </p> <p>Info</p> <p>If applicable, increase the number of threads in order to speed things up. Hint: You can put time in front of any command to see how long it took. Try it with <code>time sleep 5</code> in a new terminal. Also note that we told the program to truncate forward and reverse reads after 190 bp due to the decrease in quality we saw in multiqc_report.html. As the amplicon is only expected to be ~310 bp long, this should still give us sufficient overlap. </p>"},{"location":"other-omics/microbiome/#building-a-tree","title":"Building a tree","text":"<p>qiime includes a tree-building pipeline which allows us to generate a phylogenetic tree from the denoised sequences with a single command. MAFFT is used for the alignment and multiple tree-inference methods are available (have a look at qiime's phylogeny plugin for details). We will use FastTree, which is the fastest but also least accurate option available. </p> <pre><code>qiime phylogeny align-to-tree-mafft-fasttree \\\n    --i-sequences rep_seqs.qza \\\n    --o-alignment aligned_rep_seqs.qza \\\n    --o-masked-alignment masked_aligned_rep_seqs.qza \\\n    --o-tree unrooted_tree.qza \\\n    --o-rooted-tree rooted_tree.qza\n</code></pre> <p>As you can see, this will create an alignment, mask locations that aligned badly, and then generate the tree (unrooted and rooted at midpoint). </p> <p>Info</p> <p>We have built a new tree here because the pipeline with FastTree runs quite quickly and we don't want to have to wait during the practical. However, instead of creating one from scratch, we could have also inserted our sequences into an existing phylogeny. qiime offers pre-computed trees for two popular 16S databases and an insertion algorithm in the fragment-insertion plugin. In general, it needs to be said that phylogenomics is an incredibly deep topic. Since the tree is not substantial for our analysis, we can simply use qiime's pipeline with the default parameters. However, if you ever rely on a high-quality phylogeny for a different project, you should definitely try to find the best approach for your data and have a close look at the respective literature. </p>"},{"location":"other-omics/microbiome/#estimating-diversity","title":"Estimating diversity","text":"<p>One reason for generating a phylogeny in a microbiome analysis is so that it can be used in phylogeny-based diversity metrics. Let's generate these now. Before we can run the corresponding command, though, we need to look up the lowest number of denoised reads per sample. </p> <p>Info</p> <p>Large differences in sequencing depth between samples can distort the results of diversity estimates. Therefore, it is common practice to down-sample (in ecology-speech \"rarify\") the reads of each sample to a number that is equal or smaller than the number of reads in the least deeply sequenced sample. This simply means that we randomly select N reads from each sample where N is the smallest number of reads in any sample. </p> <p>To check the number of denoised reads per sample, we can create a qiime visualisation of our counts table with </p> <pre><code>qiime feature-table summarize \\\n    --i-table table.qza \\\n    --o-visualization table.qzv\n</code></pre> <p>Visualisation files are produced by certain qiime commands and provide human-readable information like plots, tables, or summary statistics. There are several ways to view such files. The easiest one is to go to https://view.qiime2.org/ and drag &amp; drop them into your browser window. </p> <p>Note that the tables and plots generated at https://view.qiime2.org/ are all rendered in your local browser and that nothing is uploaded to be processed on an external server, which is often required when working with sensitive data. </p> <p>Try using this site to view the table.qzv visualisation artefact produced by the last command. Once the visualisation has loaded, there should be a table looking like this: </p> Frequency Minimum frequency 24,366.0 1st quartile 31,235.75 Median frequency 35,43.0 3rd quartile 41,694.0 Maximum frequency 53,797.0 Mean frequency 36,440.9 <p>So, we need to rarify to a sampling depth of 24,336 reads. Let's generate the diversity metrics now (again, adjust the number of threads according to your setup): </p> <pre><code>qiime diversity core-metrics-phylogenetic \\\n    --i-phylogeny rooted_tree.qza \\\n    --i-table table.qza \\\n    --p-sampling-depth 24366 \\\n    --p-n-jobs-or-threads 1 \\\n    --m-metadata-file meta.tsv \\\n    --output-dir core-metrics-results\n</code></pre> <p>This will generate a new directory \"core-metrics-results\" holding (based on multiple different diversity metrics) sample-wise diversity values (\"alpha diversity\"), pairwise inter-sample distance matrices (\"beta diversity\"), and visualisations of PCoA plots (ending in .qzv). These can again be inspected with https://view.qiime2.org/. For example, the PCoA plot based on Bray\u2013Curtis distance with BV-negative samples in red and BV-positive samples in blue looks like this: </p> <p></p> <p>Question</p> <p>What does the plot tell us about our samples and the impact of BV on inter-sample diversity. Look up the term \"Anna Karenina Principle\" and what it means in terms of the microbiome. Can we say that it applies to our data? </p>"},{"location":"other-omics/microbiome/#taxonomic-classification","title":"Taxonomic classification","text":"<p>We are not only interested in the ecological diversity of our samples; we also want to know which species were found. Again, there are multiple ways of achieving this. We will use a Na\u00efve Bayes classifier (pre-trained on the Greengenes 13_8 database) available from the Qiime2 website. You can find it in the db directory and use it with the following command: </p> <pre><code>qiime feature-classifier classify-sklearn \\\n    --i-classifier db/2024.09.backbone.full-length.nb.sklearn-1.4.2.qza \\\n    --i-reads rep_seqs.qza \\\n    --p-n-jobs 1 \\\n    --o-classification taxonomy.qza\n</code></pre> <p>The produced table in taxonomy.qza simply links the taxonomic classifications (from phylum to species level) to the corresponding sequences. If you want to inspect the table, run </p> <pre><code>qiime metadata tabulate \\\n    --m-input-file taxonomy.qza \\\n    --o-visualization taxonomy.qzv\n</code></pre> <p>and open the visualisation taxonomy.qzv in https://view.qiime2.org/. </p> <p>To get a more intuitive understanding of the microbial composition of our samples we can now ask qiime to plot it for us: </p> <pre><code>qiime taxa barplot \\\n    --i-table table.qza \\\n    --i-taxonomy taxonomy.qza \\\n    --m-metadata-file meta.tsv \\\n    --o-visualization taxa_barplot.qzv\n</code></pre> <p>The resulting visualisation at species level (\"Level 7\") looks like this in https://view.qiime2.org/: </p> <p></p> <p>Info</p> <p>You can adjust the with of the bars with the slider above the plot. </p> <p>Question</p> <p>We can see that some samples are dominated by Lactobacillus iners (green), whereas for others the situation looks very different. Double check with the metadata to find out if this is associated with BV-status. </p>"},{"location":"other-omics/microbiome/#testing-differences-in-alpha-diversity","title":"Testing differences in alpha diversity","text":"<p>Alpha diversity measures the general diversity of an ecosystem (i.e. a sample in our case) or a group of ecosystems (i.e. groups of samples like all samples with BV). After looking at the taxa barplot we just generated, do you think that the BV samples are statistically significantly more diverse than the non-BV samples? We can check if your estimate is correct by running a Kruskal\u2013Wallis test on the four metrics of alpha diversity that we calculated for our samples. Let's create a new directory to write the results into and run: </p> <pre><code>mkdir alpha_tests\nfor metric in faith_pd evenness shannon observed_features; do\n    qiime diversity alpha-group-significance \\\n        --i-alpha-diversity core-metrics-results/${metric}_vector.qza \\\n        --m-metadata-file meta.tsv \\\n        --o-visualization alpha_tests/${metric}_group_significance.qzv\ndone\n</code></pre> <p>This produces four more visualisations. Have a look at them. Were you right? </p> <p>Question</p> <p>It looks like the different alpha diversity metrics disagree! After reading the short definitions of the metrics provided below, can you think of a reason for this discrepancy? faith_pd: Faith's phylogenetic diversity is defined as the sum of branch-lengths in the phylogeny between all species found in a sample (regardless of abundance). evenness: Pielou's evenness index quantifies how differently abundant species making up an ecosystem are. shannon: The Shannon index or Shannon entropy quantifies how difficult it is to guess the species of a random specimen taken from the sample (the more species and the more equally abundant they are, the more difficult). observed_features: Simply the number of unique taxa found in the sample. </p>"},{"location":"other-omics/microbiome/#testing-beta-diversity","title":"Testing beta diversity","text":"<p>As opposed to alpha diversity, which quantifies the diversity of a sample (or a group of samples) overall, beta diversity gives an estimate of the magnitude of differences between individual samples or groups of samples. We can test the difference between BV and non-BV samples for all beta diversity metrics with the following command: </p> <pre><code>mkdir -p beta_tests\n\nfor metric in bray_curtis jaccard unweighted_unifrac weighted_unifrac; do\n    qiime diversity beta-group-significance \\\n        --i-distance-matrix core-metrics-results/${metric}_distance_matrix.qza \\\n        --m-metadata-file meta.tsv \\\n        --m-metadata-column BV \\\n        --o-visualization beta_tests/${metric}_significance.qzv \\\n        --p-permutations 9999\ndone\n</code></pre> <p>Again, we got a visualisation for each metric. Have a look at them in https://view.qiime2.org/. What do you find? Are all of them in agreement this time? </p> <p>This concludes today's practical. If you are interested in differentially abundant taxa between the BV and non-BV samples, have a look at qiime's' ANCOM function. </p> <p>Acknowledgements: Many thanks to Dr. Suzanna Francis for providing the data and Ernest Diez-Benavente and Julian Lisebber-Egger for designing the practical materials.</p>"},{"location":"other-omics/ml/","title":"Deep learning (Neural Network)","text":"<p>Next-generation sequencing data is being produced at an ever-increasing rate. The raw data is not meaningful by itself and needs to be processed using various bioinformatic software. This practical will focus on genomic resequencing data where the raw data is aligned to a reference genome.</p>"},{"location":"other-omics/ml/#introduction","title":"Introduction","text":"<p>Deep learning neural networks have revolutionized the field of predictive modelling, especially in bioinformatics and genomics. By utilizing multiple layers of artificial neurons to extract and process data, deep learning networks have shown impressive performance in tasks such as image classification, natural language processing, and speech recognition. The same principles can be applied to infectious disease genomic DNA data for drug resistance prediction. By leveraging the inherent complexity and high-dimensional nature of genomic data, deep learning networks can learn to identify subtle patterns and relationships that may be missed by traditional statistical methods. Additionally, deep learning networks can be optimized to handle missing data, noisy data, and varying data types, which are common challenges in genomic data analysis.</p> <p></p> <p>Knowledge</p> <p>A neural network is a type of machine learning model that consists of layers of interconnected nodes, also known as neurons. Each neuron takes in input values, multiplies them by weights, and applies an activation function to produce an output. The output from one layer becomes the input to the next layer until the final layer produces the model's prediction. During training, the model adjusts the weights to minimize the difference between its prediction and the actual output. This process is repeated multiple times until the model's predictions become accurate enough for the desired task.</p> <p>To predict drug resistance from infectious disease genomic DNA data, deep learning networks are trained on large and diverse datasets containing information on various aspects of the DNA, such as nucleotide sequences, structural variants, and gene expression levels. By analyzing these features, deep learning networks can learn to predict drug resistance with high accuracy, potentially leading to the development of more effective drugs and treatment strategies. The process of training these networks involves splitting the data into training, validation, and testing sets, and optimizing the network architecture and parameters to minimize the prediction error.</p> <p>In summary, deep learning neural networks offer a promising approach for predicting drug resistance from infectious disease genomic DNA data. By leveraging the complex and high-dimensional nature of genomic data, these networks can identify subtle patterns and relationships that are difficult to detect using traditional statistical methods. With the growing threat of drug-resistant infectious diseases.</p> <p>In this practical, we will focus on training a deep learning model to predict the resistance for isoniazid.</p> <p>Important</p> <p>Before doing anything, we need to first activate the conda environment for this practical by typing the following: <code>conda activate ml</code>. This environment contains most of the software we need for this practical. This command needs to be run each time we open up a new terminal or switch from a different environment. You can list all installed environments with <code>cond env list</code>.</p>"},{"location":"other-omics/ml/#exercise-1-running-the-neural-network-for-predicting-isoniazid-resistance","title":"Exercise 1: Running the neural network for predicting Isoniazid resistance","text":"<p>Here we train a model using katG sequence data from M. tuberculosis genome. katG is a gene on which SNPs resposible isoniazid resistance are commonly found.</p> <p>In the terminal navigate to the <code>ml_workshop/inh_model</code> directory, by typing:</p> <pre><code>cd ~/data/ml_workshop/inh_model\n</code></pre> <p>Now type the commands below to train the model with default parameters.</p> <pre><code>python inh_model.py -lr 0.001 -dr 0.2\n</code></pre> <p>Important</p> <p>It is important to specify  python  before the script to complie (run the script) using python.</p> <p>Two graphs are produced in the same folder:</p> <pre><code>INH-model_LR:0.001-DR:0.2-ACC.png\nINH-model_LR:0.001-DR:0.2-LOSS.png\n</code></pre> <p>Information</p> <p>The script is adapted for CPU running (estimated running time of 2.5 minutes). Hence compromises in model accuracy is taken. </p> <p>Waiting for model to run/train and dealing with these fragmented times is also essential in our lives here are some ground rules:</p> <ul> <li> <p>If a model takes &lt; 5 minutes to train, I like to check Instagram to give my brain a break.</p> </li> <li> <p>If a model takes 5\u201330 min to train, I\u2019ll usually spend time reading or writing documentation. Sometimes debugging too.</p> </li> <li> <p>Upwards of that, and I\u2019ll continue my day as usual and work on another project in the meanwhile.</p> </li> </ul> <p>In this case you perhaps read on as well. There are some interesting info near at the end.</p> <p>When the run finishes, you can open the current folder using command <code>open .</code> in terminal. Double click on the picture files to view them. </p> <p>Or use <code>xdg-open &lt;file name&gt;</code> in the terminal</p>"},{"location":"other-omics/ml/#acc-accuracy","title":"ACC-accuracy","text":"<p>Accuracy is a common performance metric used in deep learning to evaluate the effectiveness of a model at predicting the correct output. It measures the proportion of correct predictions made by the model out of the total number of predictions. </p> <p>Intuition</p> <p>Imagine you are playing a game of darts and aiming for a bullseye. Your accuracy is determined by the number of times you hit the bullseye compared to the total number of attempts. </p> <p>Similarly, in deep learning, accuracy measures how often the model's predictions match the true labels for a given set of inputs. </p> <p>For example, if a model predicts that an image contains a cat and the true label is also cat, then that prediction is counted as correct. </p> <p>The higher the accuracy, the better the model is at making correct predictions. However, accuracy can be influenced by factors such as class imbalance (data problems) or various different types of errors the model makes (model problems). Therefore, it is important to consider other performance metrics in addition to accuracy when evaluating the performance of a deep learning model.</p>"},{"location":"other-omics/ml/#loss-loss","title":"LOSS-loss","text":"<p>In deep learning, the goal of the model is to accurately predict the outcome of a given task, such as image recognition or natural language processing and also in this case sequence processing. </p> <p>Loss is a term used to quantify the difference between the predicted output and the actual output. </p> <p>Intuition</p> <p>Think of it like a student taking a test - the score they receive is a measure of how well they performed relative to the expected outcome. </p> <p>Similarly, the loss function in deep learning measures how well the model is performing by comparing its predictions to the actual results. The lower the loss, the better the model is at predicting the outcome. </p> <p>The goal of training a deep learning model is to minimize the loss over the course of multiple iterations, or epochs, of training. Different types of loss functions can be used depending on the task and the type of output being predicted. Ultimately, the goal is to choose a loss function that encourages the model to learn the desired features and make accurate predictions.</p>"},{"location":"other-omics/ml/#epoch","title":"Epoch","text":"<p>In deep learning, an epoch refers to a complete pass through the entire training dataset during the model training process. </p> <p>Intuition</p> <p>Think of it like a chef preparing a recipe - each time they go through the entire recipe from start to finish, that's one epoch. </p> <p>During each epoch, the model is shown a batch of input data and the corresponding output labels, and it updates its parameters based on the difference between the predicted output and the actual output. </p> <p>The number of epochs that a model is trained for is an important hyperparameter that can impact the performance of the model. Training for too few epochs may result in a model that underfits or fails to learn the underlying patterns in the data. On the other hand, training for too many epochs may result in a model that overfits or becomes too specialized to the training data and fails to generalize well to new data. Therefore, the number of epochs should be chosen carefully to balance between underfitting and overfitting and achieve the best performance for the task at hand.</p>"},{"location":"other-omics/ml/#exercise-2-try-different-values-of-dropout-and-learning-rate","title":"Exercise 2 :Try different values of dropout and learning rate","text":"<p>When working with python script that required input, you can also View the explanation for each input parameter using <code>python inh_model.py -h</code>.</p> <pre><code>usage: inh_model.py [-h] [-lr LEARNING_RATE] [-dr DROPOUT_RATE]\n\nIsoniazid prediction model using katG sequences as input.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -lr LEARNING_RATE, --learning_rate LEARNING_RATE\n                        Learning rate for the model(between 10e-6 and 1) (default: 0.001)\n  -dr DROPOUT_RATE, --dropout_rate DROPOUT_RATE\n                        Dropout rate for hte model layers (between 0 and 1) (default: 0.2)\n</code></pre>"},{"location":"other-omics/ml/#lr-learning-rate","title":"lr: learning rate","text":"<p>Learning rate is a key parameter in many machine learning algorithms. </p> <p>It is the step size that a model takes when trying to find the best set of weights to make accurate predictions. </p> <p>Intuition</p> <p>Imagine you are climbing a hill to reach the top. Your learning rate is how big of a step you take with each stride. If your steps are too small, it will take a long time to reach the top, but if your steps are too big, you might overshoot and miss the peak. </p> <p>Similarly, in machine learning, a small learning rate will cause the algorithm to take longer to converge, while a large learning rate can cause the algorithm to overshoot and make inaccurate predictions (underfitting). Therefore, choosing the right learning rate is important to achieve optimal results in machine learning models.</p>"},{"location":"other-omics/ml/#dr-dropout-rate","title":"dr: dropout rate","text":"<p>Dropout is a technique used in machine learning to prevent overfitting, which is when a model becomes too specialized to the training data and fails to generalize well to new data. </p> <p>Dropout works by randomly dropping out, or \"turning off\", some of the neurons in a neural network during training. </p> <p>Intuition</p> <p>Again the chef intuition - if they only rely on a few key ingredients, the dish may taste great in the kitchen but won't necessarily appeal to a wider audience. </p> <p>By randomly \"turning off\" some of the neurons, the model is forced to learn more robust and diverse features, leading to better generalization to new data. The dropout rate is the percentage of neurons that are randomly dropped out during each training epoch, and it is another important hyperparameter that can be tuned to optimize the performance of the model.</p> <p></p> Learning Rate Dropout Rate Consequences High High Fast learning, high regularization, risk of underfitting High Low Fast learning, low regularization, risk of overfitting Low High Slow learning, high regularization, good generalization Low Low Slow learning, low regularization, risk of overfitting <p></p> <p>A high learning rate means the model will make larger updates to its weights during training, while a low learning rate means the updates will be smaller. </p> <p>A high dropout rate means more neurons will be dropped out during training, while a low dropout rate means fewer neurons will be dropped out.</p> <p>If both the learning rate and dropout rate are high, the model will learn quickly but may not generalize well to new data due to excessive regularization. If both the learning rate and dropout rate are low, the model will learn slowly and may overfit to the training data. </p> <p>If the learning rate is low and the dropout rate is high, the model will learn slowly but regularize effectively, leading to good generalization. Finally, if the learning rate is high and the dropout rate is low, the model will learn quickly but may overfit due to insufficient regularization.</p> <p>Question</p> QuestionAnswer <p>Is the parameters used in exercise1 (learning rate, dropout rate) the best suited? Try different value combinations according to above intuitions. Observe the relationship between the two curves to adjust the hyperparameters</p> <ul> <li>Optimal: Low validation loss</li> <li>Overfitting: High training loss, low validation loss</li> <li>Underfitting: Low validation loss</li> </ul> <p>Not necessarily the optimal, but here are some guiding values to try out:</p> <ul> <li>Balanced<ul> <li>lr = 0.0005</li> <li>dr = 0.2</li> </ul> </li> <li>Underfitting<ul> <li>lr = 0.2</li> <li>dr = 0.2</li> </ul> </li> <li>Overfitting<ul> <li>lr = 0.0005</li> <li>dr = 0.0</li> </ul> </li> </ul> <p>Type the commands below to train the model with different parameters.</p> <pre><code>python inh_model.py -lr &lt;Pick a learning rate&gt; -dr &lt;Pick a dropout rate&gt;\n</code></pre> <ul> <li> <p>learning rate range: between 10e-6 and 1</p> </li> <li> <p>Dropout rate range:  between 0 and 1</p> </li> </ul>"},{"location":"other-omics/ml/#exercise-3-converting-fastq-to-onehot-encoding","title":"Exercise 3: Converting Fastq to onehot encoding","text":"<p>In machine learning, one-hot encoding is a way to represent categorical data in a numerical format that can be easily understood by algorithms.</p> <p>In the case of DNA data, each nucleotide (A, T, C, G) can be considered as a category, and one-hot encoding is used to represent each nucleotide as a unique binary value. </p> <p>This is necessary because machine learning algorithms require numerical inputs to make predictions, and simply representing DNA sequences as strings of characters would not be suitable for most machine learning tasks. </p> <p>By using one-hot encoding, we can represent each DNA sequence as a series of binary values that capture the presence or absence of each nucleotide at each position in the sequence. This allows us to use a wide range of machine learning algorithms to analyse and make predictions based on DNA data, such as predicting the likelihood of a genetic disorder or identifying regions of the genome that are associated with certain traits or diseases.</p> <p>First change the directory to the script folder using hte below command:</p> <pre><code>cd ../full_model\n</code></pre> <p>Now type the commands below to view the first few lines of the original fastq file:</p> <pre><code>less ERR6634978_1.fastq.gz | head\n</code></pre> <p>Reminder</p> <p>FASTQ is a text-based file format commonly used in bioinformatics to store and exchange sequences and their corresponding quality scores. </p> <p>It consists of four lines per sequence:   - The first line starts with \"@\" followed by a unique identifier for the sequence.  - The second line contains the actual nucleotide sequence.  - The third line starts with \"+\" followed by the same unique identifier as in the first line  - The fourth line contains the quality scores corresponding to each nucleotide in the sequence. The quality scores represent the confidence level of each nucleotide call and are represented as ASCII characters. </p> <p>The FASTQ format is widely used in sequencing technologies such as Illumina, Ion Torrent, and PacBio.</p> <p>Output</p> <pre><code>@A00386:50:HGY3TDRXY:1:2101:11496:1016 1:N:0:ATTACTCG+TAAGATTA\nGNCGTTGGCGATGCGCACGGTGTTGGAGAGCGTGCCACCCGTGACGGTGCCGTCCGAGATCGTCCGGCTGCAAGAGCAGCTGGCCCAGGTGGCAAAGGGTGAGGCTTTCCTGCTGCAGGGCGGCGACTGCGCTGAGACATTCATGGACAAC\n+\nF#FFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFF,F\n@A00386:50:HGY3TDRXY:1:2101:12382:1016 1:N:0:ATTACTCG+TAAGATTA\nGNCGGACGTGTCGAACTTGGGGCCTACGACGCCGAACATGACCTGATCCTGGAGAACGACCGCGGCTTCGTGCAGGTCGCCGGTGTCAACCAGGTCGGGGTGCTGCTCGC\n+\nF#FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF\n@A00386:50:HGY3TDRXY:1:2101:21386:1016 1:N:0:ATTACTCG+TAAGATTA\nATTCCACCGCCTCGGCGACCACGACCAGCACGATCAATGTCCGGGCACTATCCCCGGCGCTGGTGGTGACATAGATCGGGTAATACCCCGACGGCACCGATCGAGCTACGGTGATCGCGACC\n</code></pre> <p>Reminder</p> <p><code>less</code> show a scrollable content of the file, <code>head</code> allow the terminal to only show the first few lines from less output.</p> <p>One hot encoding can be created from alignment files (.bam). As a refresher let's generate that .bam file from raw fastq sequence.</p> <p><pre><code>bwa index MTB-h37rv_asm19595v2-eg18.fa\n</code></pre> <pre><code>bwa mem MTB-h37rv_asm19595v2-eg18.fa ERR6634978_1.subset.fastq.gz ERR6634978_2.subset.fastq.gz | samtools sort - -o ERR6634978.bam\n</code></pre> <pre><code>samtools index ERR6634978.bam\n</code></pre></p> <p>Now type the commands below generate onehot in coded sequences: <pre><code>python bam2oh.py  ERR6634978.bam --regions gene.csv --output ERR6634978_oh.csv\n</code></pre></p> <ul> <li>--regions: gene region</li> <li>--output: output file name</li> <li>two input files are followed</li> </ul> <p>The above take a while to run Now type the commands below to view generate onehot encoded sequences: <pre><code>less  ERR6634978_oh.csv | head\n</code></pre></p> <p>Output</p> <pre><code>A,C,G,T\n1,0,0,0\n0,1,0,0\n1,0,0,0\n0,1,0,0\n0,0,1,0\n0,0,0,1\n0,1,0,0\n0,0,1,0\n1,0,0,0\n</code></pre>"},{"location":"other-omics/ml/#exercise-4-running-full-model-that-predicts-for-all-13-drug-resistances","title":"Exercise 4: Running full model that predicts for all 13 drug resistances","text":"<p>Now try to use a fully trained model to predict all 13 different types of drug resistance.</p> <p>Check if you are in the right directory using <code>ls</code> and <code>pwd</code> containing <code>full_model.py</code>.</p> <p>Now type the below commands to train the model with different parameters, show output in terminal and saving output into <code>drug_predictions.csv</code> file:</p> <pre><code>python full_model.py -i ERR6634978_oh.csv -v -o drug_predictions.csv\n</code></pre> <ul> <li>-i: one hot encoded input file</li> <li>-o: output file name</li> <li>-v: verbose (show output in the terminal)</li> </ul> <p>You can also view the output file using <code>less drug_predictions.csv</code>. Feel free to scroll around and press <code>Q</code> to exit. </p> <p>The output is in binary format (1=positive, 0=negative). What is the drug resistance profile of the sample?</p>"},{"location":"other-omics/ml/#additional-information","title":"Additional information","text":"<p>In case you'd like a more hands on and visualised example of how a neural networks functions. Neural network Playground</p>"},{"location":"other-omics/ml/#more-on-regularisation","title":"More on regularisation","text":"<p>Regularization is a technique used in deep learning to prevent overfitting and improve the generalization performance of a model. There are several ways to regularize a deep learning model, including:</p> <ol> <li>L1 and L2 Regularization: These are the most common types of regularization used in deep learning. L1 regularization adds a penalty term to the loss function that is proportional to the absolute values of the model parameters, while L2 regularization adds a penalty term that is proportional to the squared values of the model parameters. Both types of regularization encourage the model to learn simpler, more interpretable features by shrinking the magnitude of the parameters.</li> <li>Dropout (The one we have applied): Dropout is a technique that randomly drops out (i.e., sets to zero) a proportion of the neurons in a layer during training. This helps prevent overfitting by forcing the network to learn more robust features that do not rely on the activation of specific neurons.</li> <li>Data Augmentation: Data augmentation is a technique that artificially increases the size of the training dataset by creating new examples from the existing ones. This can be done by applying transformations such as rotations, flips, and crops to the original images, or by adding noise to the input data.</li> <li>Early Stopping: Early stopping is a technique that stops the training process before the model starts to overfit. This is done by monitoring the validation error during training and stopping the training process when the validation error stops improving.</li> <li>Batch Normalization: Batch normalization is a technique that normalizes the activations of a layer by subtracting the mean and dividing by the standard deviation of the activations in a batch of data. This helps to reduce the internal covariate shift, which can improve the training speed and stability of the model.</li> <li>Max-Norm Regularization: Max-Norm regularization constrains the magnitude of the weight vector for each neuron to a fixed value. This helps to prevent large weight updates during training, which can lead to overfitting</li> <li>Label Smoothing: In label smoothing, instead of assigning a one-hot vector to the target labels, a smoothed label distribution is used. This helps prevent overfitting by introducing a small amount of noise into the training targets, which can encourage the model to learn more robust decision boundaries.</li> <li>Cutout: Cutout is a form of data augmentation that randomly masks out square regions of the input images during training. This helps prevent overfitting by forcing the model to learn more robust features and by increasing the amount of training data.</li> <li>Mixup: Mixup is a form of data augmentation that involves linearly interpolating pairs of training examples and their corresponding labels. This creates new training examples and encourages the model to learn more generalizable features.</li> <li>Shake-Shake Regularization: Shake-Shake regularization is a form of regularization for residual networks that introduces stochastic depth into the network. This helps prevent overfitting by randomly dropping out entire residual blocks during training.</li> <li>Stochastic Depth: Stochastic Depth is a variant of the Shake-Shake regularization that drops out entire residual blocks with a certain probability. This technique helps prevent overfitting by randomly removing some parts of the network during training.</li> <li>Focal Loss: Focal loss is a variant of cross-entropy loss that gives more weight to hard-to-classify examples. This can help prevent overfitting by reducing the impact of easy-to-classify examples on the training process.</li> </ol>"},{"location":"other-omics/ml/#all-parameters-that-affects-the-model","title":"All parameters that affects the model","text":"<p>The learning rate and dropout rate and two of the most important hyperparameters that affect the model's learning. -   Learning Rate: The learning rate determines the step size of the optimization algorithm during training. A high learning rate can lead to unstable training, while a low learning rate can lead to slow convergence.</p> <ul> <li> <p>Batch Size: The batch size determines the number of samples that are used to compute each update during training. A larger batch size can lead to faster training, but it can also require more memory and may result in lower generalization performance.</p> </li> <li> <p>Dropout Rate: Dropout is a regularization technique that randomly drops out some neurons during training to prevent overfitting. The dropout rate determines the fraction of neurons that are dropped out during training.</p> </li> <li> <p>Weight Initialization: The initial values of the weights in the neural network can affect the model's ability to learn and generalize to new data. Common weight initialization techniques include random initialization and Xavier initialization.</p> </li> <li> <p>Optimizer: The optimizer is the algorithm used to update the weights of the neural network during training. Common optimizers include stochastic gradient descent (SGD), Adam, and RMSprop. The choice of optimizer can affect the speed and stability of training, as well as the generalization performance of the model.</p> </li> <li> <p>Number of Hidden Layers: This parameter determines the depth of the neural network, and it can greatly affect the complexity and capacity of the model. Adding more layers can allow the model to learn more complex features, but it can also increase the risk of overfitting.</p> </li> <li> <p>Number of Neurons per Layer: This parameter determines the width of the neural network, and it can also affect the model's capacity. Increasing the number of neurons per layer can increase the model's ability to capture complex relationships in the data, but it can also increase the risk of overfitting.</p> </li> <li> <p>Activation Function: The activation function is applied to each neuron in the neural network, and it determines the output of the neuron. Common activation functions include sigmoid, ReLU, and tanh. The choice of activation function can affect the model's ability to learn and generalize to new data.</p> </li> </ul>"},{"location":"other-omics/ml/#full-model-structure","title":"Full model structure","text":"<p>In case if you are curious of how the structure of the full model looks like </p>"},{"location":"other-omics/tb-resistance/","title":"TB resistance prediction","text":"<p>We will now look at using TB-Profiler to predict drug resistance patterns. The pipeline searches for small variants and large deletions associated with drug resistance. It will also report the lineage. By default, it uses trimmomatic to trim the reads, BWA (or minimap2 for nanopore) to align to the reference genome and freebayes to call variants.</p> <p>conda is a package manager that you can use to install bioinformatics software. We have installed a faster version of conda, which is called mamba. You can install almost all bioinfromatics software with mamba and it will be your best friend as you continue on in your journey as a bioinformatician. Run the following command to create a conda \"environment\" with the latest version of tb-profiler installed. We also want to use IGV and a tool called curl later, so we need to add that to the install command.</p> <pre><code>mamba create -y -n tb-profiler -c bioconda -c conda-forge tb-profiler=5 igv freebayes=1.3.6\n</code></pre> <p>Before you can use tb-profiler, you should activate the environment using the following command (remembter that you'll have to run this every time you open up a new terminal window):</p> <pre><code>conda activate tb-profiler\n</code></pre> <p>You can now run the pipeline and get the help message by running the following command:</p>"},{"location":"other-omics/tb-resistance/#tb-profiler","title":"tb-profiler","text":"<p>You should be able to see that there are many options which you can use. The main function for analysing new data is the profile command. Try getting the help options by running tb-profiler profile -h. Can you identify the required inputs? The pipeline was designed to be very flexible in terms of the inputs it can take. You can give it FASTQ, BAM, CRAM, VCF, or assembled FASTA files and it will produce the same output file.</p> <p>Let's try running it on sample1 with the BAM file as input. First, change into the tb directory and then run the pipeline using the following commands:</p> <pre><code>cd ~/data/tb/\ntb-profiler profile --bam sample1.bam --prefix sample1 --txt\n</code></pre> <p>The --prefix argument allows you to specify a prefix for the output files. After running it you should see a folder called results which contains the output files. By default the tool produes a .json formatted file but since we specified --txt it also produced a text file. You can view this file by opening it up in a text editor or by using less on the terminal. The first section gives a general summary with the drug resistance type, the lineage and the median depth. The second section section gives a summary of the resistance patterns found in the sample along with the relevant resistance mutations. The third section gives a summary of the drug-resistance variants found in the sample together with information such as the frequency at which they occur in the raw data. The fourth section gives a summary of candidate mutations found in genes generally associated with resistance (i.e. mutations that have not been linked to resistance themselves but that occurred in resistance-associated genes).</p> <p>Question</p> <p>Have a look at the report. What lineage is this sample? Do we see any drug resistance? Does the report flag any quality control issues?</p> <p>Now try running tb-profiler on sample2 using the bam file as input. After it finishes, look at the report. Do you see any differences?</p> <p>Often you want to combine the results from many runs into a single report. This helps with comparing resistance and lineage data from different isolates. A single report can be generated with the collate function. Run the follwing command and it will generate a text file containing key metrics from both samples. </p> <pre><code>tb-profiler collate\n</code></pre> <p>You can open this in gnumeric (a free alternative to excel). Open gnumeric by running the following on the terminal:</p> <pre><code>gnumeric tbprofiler.txt\n</code></pre>"},{"location":"other-omics/tb-resistance/#bedaquiline-resistance-a-case-study","title":"Bedaquiline resistance: a case study","text":"<p>Now that you know how to run tb-profiler, we will look into using it to compare the results with phenotypic tests. For this, we have selected five samples that were publishes in this study by Fowler et al. We will be looking at resistance to bedaquiline and the variants that cause resistance. Bedaquiline is one of the new drugs that have been intruduced in the last decade. Sadly, resistance has already been observed, and it is imperative that this is limited to maintain global effectiveness of the drug. Two genes implicated in resistance against bedaquiline are the drug target atpE and a transcriptional regulator (mmpR5) which represses expression of an efflux pump that can pump bedaquiline out of the cell. Mutations in atpE prevent binding of the drug, while mutations in mmpR5 cause loss of function and as a result increased expression of the efflux pump (and resistance!). The phenotypic results are shown below. First let's download the relevant data and change into the new directory. </p> <pre><code>cd ~/data/tb/bedaquiline\n</code></pre> Sample DST ERR4829977 Resistant ERR8975807 Resistant ERR4796447 Resistant ERR8975920 Resistant ERR5917992 Resistant <p>As you can see they are all resistant according to phenotypic methods. Let's see if WGS agrees with this. Run tb-profiler for all samples using the VCF files. Remember that VCF format contains variants detected in the genome. We have created these VCF files for you so that the analysis won't take too long. Here is the command for the first sample. You can adapt it for the rest. </p> <pre><code>tb-profiler profile --vcf ERR4829977.vcf.gz --prefix ERR4829977 --txt\n</code></pre> <p>After you have generated the result files, run the collate function and open using LibreOffice Calc. </p> <pre><code>tb-profiler collate\n</code></pre> <p>Open the tbprofiler.txt file in ~/data/tb/bedaquiline using libreoffice similar to as you have done before.</p> <p>Question</p> <p>Do the results from tb-profiler agree with the phenotypic results? </p> <p>It seems like there are two samples which are sensitive to bedaquiline according to WGS. Have a look at the individual text format result files for these samples and go to the \"Other variants\" section. Resistance can often occur due to novel variants that haven't been observed before. </p> <p>Question</p> <p>Can you identify any variants that could explain the discrepant results? Hint: look for variants in the atpE and mmpR5 genes. </p>"},{"location":"other-omics/tb-resistance/#answers","title":"Answers!","text":"<p>Let's first have a look at ERR8975807. It looks like the there are no known resistant mutations, however the \"Other variants\" section reveals that there are two variants in the mmpR5 gene. Remember that loss of function in this gene can cause resistance. One of these mutations is a small indel that causes a frameshift, and as a result \u2013 loss of function. Interestingly you might notice that the mutations are not fixed in the sample, with both occuring at close to 0.5 fraction of the reads. This indicates that there are two sub-populations in the host with different resistance mutations. Have a look at the bam file for this sample in IGV and see if you agree.</p> <p>Second, lets look at ERR4796447. This sample also does not present any known variants. However, the \"Other variants\" section reveals that there is a variant in the atpE gene which codes for the drug target. Because this is a SNP causing less of a functional impact than the frameshift variant mentioned above, this will be more difficult to evaluate. However, there are some tools which can help. When assesing the functional impact of SNPs on proteins, researchers often turn to in silico methods. These predict the effect on characteristics such as protein stability and ligand binding affinity using a protein model. One such tool already exists for bedaquiline resistance called SUSPECT-BDQ. It allows you to upload a mutation and will predict whether it will confer resistance. Click the link and input the mutation you have found and have a look at the output. You'll have to convert the amino acids to one letter code (E.g. E61D). </p> <p>Question</p> <p>After studying the other mutations, do you wish to manually reclassify your samples? </p>"},{"location":"other-omics/tb-resistance/#summary","title":"Summary","text":"<p>You have seen how tb-profiler automates a lot of the steps you have performed so far. It does mapping, variant calling, annotation, and formats the results into a human-readable report. This is just one tool of many that can be used to predict resistance and, more generally, perform automated analyses. For other organisms, such as E. coli, you should use a tool which can also analyse resistance genes on plasmids. </p>"},{"location":"other-omics/transcriptomics/","title":"Transcriptomics","text":""},{"location":"other-omics/transcriptomics/#introduction","title":"Introduction","text":"<p>An application of next-generation sequencing is RNA sequencing (Mortazavi et al., 2008; Wang et al., 2009). In particular we will discuss transcriptome (messenger RNA) sequencing. Transcriptome sequencing is a very useful addition to genome sequencing projects as it helps to identify genes and thus aids in genome annotation. In this sense, it is similar to earlier transcriptome sequencing using capillary methods (EST sequencing), but provides much higher coverage of the transcriptome.</p> <p>Reads from RNA sequencing can be treated in much the same way as those from DNA sequencing. The exception is in eukaryotes when there is splicing, where a single gene can code for multiple proteins through transcription of determined exons.</p> <p>Due to the vast number of reads produced by next generation sequencing technology, the transcriptome is also sequenced very deeply. Each gene is sequenced in proportion to its abundance and the large number of reads means that even low abundance genes are sequenced to some extent. This means that expression levels of genes can be compared. One can visualise the \"pile up\" of reads in a particular region by looking at coverage plots. The higher the signals in the plot, the more expressed a transcript is. It is important to note that the sequences originate from transcriptome samples (mRNA) and therefore only contains information about the exons and untranslated regions (UTRs).</p> <p>Imagine the following transcript is present in the sample:</p> <p></p> <p>Reads belonging to the transcript are produced by the sequencing process. When the reads come out as raw data, there is no information about where they belong in the reference genome. Furthermore, all reads from several different transcripts come out together. An alignment algorithm localises them in the reference genome based on similarity matches.</p> <p></p> <p>In the plot, the coverage line represents the number of reads that align to the genome at each base position. This allows us to identify coding regions, here, the 3 exons (in yellow) that comprise the transcript above.</p> <p>In this module we will use a similar approach used to map DNA sequencing data to map RNA sequencing data from Mycobacterium tuberculosis.</p> <p>Understanding an organism's genome goes beyond cataloging the genes that are present in the genome. Insight into the biological stages in which each gene is expressed (potentially used) helps us to identify how organisms develop and respond to a particular external stimuli. The first layer of such complex patterns involves the understanding of how the genome is being used is the transcriptome. This is also the most accessible type of information because, like the genome, the transcriptome is made of nucleic acids and can be sequenced relatively easily. Arguably the proteome is of greater relevance to understand cellular biology, but it is chemically heterogenous making it much more difficult to assay.</p> <p>Over the past two decades, microarray technology has been applied extensively for addressing the question of which genes are expressed and when, enabling the performance of differential expression analysis. Despite its success, this technology is limited in that it requires prior knowledge of the gene sequences for an organism and has a limited dynamic range in detecting the level of expression, e.g. how many copies of a transcript are made. RNA sequencing technology using, for instance Illumina HiSeq machines, can sequence all the genes that are transcribed, and the results have a more linear relationship to the real number of transcripts generated.</p> <p>The aim of differential expression analysis is to determine which genes are more or less expressed in different situations. We could ask, for instance, whether a bacterium uses its genome differentially when exposed to stress, such as heat or challenged by a drug. Alternatively, we could ask which genes make human livers different from kidneys.</p> <p>In these exercises, we will try to gain some understanding of differences between M. tuberculosis lineages. The genome of M. tuberculosis was published in 1998 (Cole et al., 1998). It has 4.4 Mb and a high GC content (~65%), comprising 4,111 genes. Although the variability of the M. tuberculosis genome has been considered limited, it has been demonstrated a higher diversity than it was previously thought. Currently, M. tuberculosis is classified in 7 different lineages with a different geographical distribution, and also virulence or spreading capacity has been seen to vary between lineages. Strains from different lineages have shown differences in virulence or in acquisition of drug resistance (Parwati et al., 2010), and these differences might be caused by variable expression of determined genes.</p> <p>Therefore, the aim of this session is to become familiar with the steps carried out in transcriptomics studies, from mapping RNA-seq reads to the performance of differential expression analysis.</p>"},{"location":"other-omics/transcriptomics/#exercise-1-mapping-rna-seq-with-bwa","title":"Exercise 1: Mapping RNA-seq with BWA","text":"<p>First, we will map RNA sequence reads from a M. tuberculosis lineage 1 strain to the reference genome, in this case, the H37Rv M. tuberculosis strain.</p> <p>You can find the M. tuberculosis H37Rv reference genome (called H37Rv.fa) as well as the two files of RNA-seq reads from a lineage 1 strain (Mtb_L1_1.fastq and Mtb_L1_2.fastq) in the transcriptomics directory.</p>"},{"location":"other-omics/transcriptomics/#running-bwa","title":"Running BWA","text":"<p>To work with the command line of Linux, you will first need to open a terminal, activate the rnaseq conda environment, go to the data and transcriptomics directory:</p> <pre><code>conda activate rnaseq\n</code></pre> <pre><code>cd ~/data/transcriptomics\n</code></pre> <p>And list the files there:</p> <pre><code>ls\n</code></pre> <p>You will find there the 4 fastq files, 2 of them from the lineage 1 sample, and the reference genome fasta file; these are the input data for this practical.</p> <p>For the mapping, first an index of the reference genome must be constructed with bwa index. On the command line, you should type:</p> <pre><code>bwa index H37Rv.fa\n</code></pre> <p>This will generate 5 files which are needed for BWA. We will then align the RNA-seq reads to the reference genome with bwa mem. We will therefore use the reference genome (in fasta format) and the two fastq files that contain the RNA-seq reads, saving the output to a SAM file. To start you should type:</p> <p><pre><code>bwa mem H37Rv.fa Mtb_L1_1.fastq.gz Mtb_L1_2.fastq.gz | samtools sort - -o Mapping_Mtb/Mtb_L1.bam\nsamtools index Mapping_Mtb/Mtb_L1.bam\n</code></pre> The output will be located in the Mapping_Mtb folder, which is in the transcriptomics directory.</p> <p>Question</p> Question 1Answer 1 <p>We also need to index our bam file. Can you remember how to do this?</p> <p>We can do this with the following code:</p> <pre><code>samtools index Mapping_Mtb/Mtb_L1.bam\n</code></pre>"},{"location":"other-omics/transcriptomics/#exercise-2-visualising-alignments","title":"Exercise 2: Visualising alignments","text":"<p>Now we will examine the reads mapping in IGV. Follow the steps as before:</p> <p>Launch IGV by running the command <code>igv</code> on a New Terminal and perform the following steps:</p> <ol> <li>We first need to load the reference genome. To do this click on Genomes -&gt; Load Genome from File.... Navigate to ~/data/transcriptomics/ and select the H37Rv.fa file.</li> <li>You can also load the genes by clicking on File -&gt; Load from File..., then selecting the Mtb.gtf file. </li> <li>Finally you can load the bam file by clicking File -&gt; Load from File... and selecting the bam file you wish to load.</li> </ol>"},{"location":"other-omics/transcriptomics/#interpreting-the-mapping","title":"Interpreting the mapping","text":"<p>This exercise is similar to the one performed before in the Visualisation module. Scroll along the genome and examine the read coverage (the part of the genome mapped goes from position 2420631 to 2920631. To view that part select 'Goto', 'Navigator' and write in 'Goto Base' 2420631). Notice how different genes have different depths of coverage.</p> <p></p> <p>Question</p> Question 2Answer 2 <p>Why do some genes have little or no coverage?</p> <p>They are not being expressed in the given sample.</p> <p>Question</p> Question 3Answer 3 <p>Why do some reads map where there are no genes?</p> <p>This could be due to: * Genome annotation incompleteness * Novel transcripts and isoforms * Non-coding RNAs</p>"},{"location":"other-omics/transcriptomics/#including-more-lineages","title":"Including more lineages","text":"<p>One interesting feature of the IGV viewer is the possibility to see more than one BAM file at the same time, which enables to comapre coverage from different samples. Hence, next we want to include more lineages, in this case we will add another sample belonging to the lineage 4 of Mtb.</p> <p>To do it, we will need first to follow the previous steps in order to get the sorted and index bam files. Therefore, map it with BWA as before, the fastq files are in the directory ~/data/transcriptomics and are called Mtb_L4_1.fastq.gz and Mtb_L4_2.fastq.gz.</p> <p>Once we have the bam files, we can add it to the IGV viewer using File -&gt; Load from File... and selecting the bam file you wish to load.</p> <p></p> <p>In the BAM view of the reads, it might be difficult to distinguish the differences between the BAM files. However, in the coverage plot, one can see the differences in coverage.</p> <p>One reason to perform RNA-seq under different conditions or in different samples (in this case different lineages of Mtb), is to see genes that are differentially expressed. For example, one gene may be more highly expressed in one lineage that in the other.</p> <p>Question</p> Question 4 <p>Can you find any genes that have differential expression?</p> <p>Now go to gene Rv2161c (position 2422271).</p> <p></p> <p>Question</p> Question 5 <p>Would you think that this gene is differentially expressed?</p>"},{"location":"other-omics/transcriptomics/#exercse-3-differential-expression","title":"Exercse 3: Differential expression","text":"<p>There are tools that calculate the differential expression, like the R packages DESeq or EdgeR. They take the reads mapped to each gene, normalize the resulting quantities of mapped reads (coverage), and then estimate if any genes are differentially expressed. In general, the results are more credible and significant if biological replicates are included.</p>"},{"location":"other-omics/transcriptomics/#counting-reads-with-htseq-count","title":"Counting reads with HTSeq-count","text":"<p>In order to perform a differential expression analysis, the first step is to count the reads mapped to each of the genes so that we can make comparisons between the different samples.</p> <p>HTSeq-count is part of the HTSeq package which will quantify the number of reads mapping to gene models in different RNA-seq experiments given a file with aligned sequencing reads (the bam file obtained through BWA) and a list of genomic features (the Mtb.gtf file with the gene annotation).</p> <p>We will run HTSeq-count with the RNA-seq data from the L1 and L4 strains we were using in the previous exercises. To do it, we will go to the directory where we have the data and use the following command:</p> <pre><code>cd ~/data/transcriptomics\n\npython -m HTSeq.scripts.count -f bam -r pos -s reverse -t gene ./Mapping_Mtb/Mtb_L1.bam Mtb.gtf &gt; ./Mapping_Mtb/Mtb_L1_htseq_count.txt\n\npython -m HTSeq.scripts.count -f bam -r pos -s reverse -t gene ./Mapping_Mtb/Mtb_L4.bam Mtb.gtf &gt; ./Mapping_Mtb/Mtb_L4_htseq_count.txt\n</code></pre> <p>The parameters we have used in the HTSeq-count command are:</p> <ul> <li>-f: format of the input data</li> <li>-r: how is sorted the data</li> <li>-s: whether the data is from a strand-specific assay. The reverse option is used for pair-end reads from HiSeq in order to mantain the strain-specificity (the reads have to map to the gene in the corresponding strand to be counted)</li> <li>-t: feature type, in this case exon</li> </ul> <p>If you have any doubt about the parameters of the program, type:</p> <pre><code>python -m HTSeq.scripts.count\n</code></pre> <p>Question</p> Question 6Answer 6 <p>What would -a parameter do?</p> <p>Ensures HTSeq-count skips all reads with alignment quality lower than the given minimum value.</p> <p>We can now take a look at the results from the HTSeq-count typing the following in the command line:</p> <pre><code>cd ~/data/transcriptomics/Mapping_Mtb\n\nless Mtb_L1_htseq_count.txt\n</code></pre> <p>You should see two columns with the list of genes and the counts for each gene.</p> <p>Terminal output</p> <pre><code>Rv2158c 0\nRv2159c 22\nRv2160A 11\nRv2160c 0\nRv2161c 193\nRv2162c 753\nRv2163c 6181\nRv2164c 2169\nRv2165c 6878\nRv2166c 11357\nRv2167c 0\nRv2168c 0\nRv2169c 3017\nRv2170  235\nRv2171  1779\nRv2172c 9812\nRv2173  1525\nRv2174  1828\nRv2175c 767\nRv2176  1440\nRv2177c 169\nRv2178c 6200\nRv2179c 205\nRv2180c 319\nRv2181  4288\nRv2182c 10635\nRv2183c 1197\n:\n</code></pre> <p>To exit this view you just need to type 'q'.</p>"},{"location":"other-omics/transcriptomics/#finding-differentially-expressed-genes-with-deseq2","title":"Finding differentially expressed genes with DESeq2","text":"<p>Now we are going to perform a differential expression analysis in order to look for genes with variable expression between lineages. To do it we will use 6 sequenced samples, 3 from lineage 1 and 3 from lineage 4. Two of them will be the two analysed in the previous steps. We are going to use an R package for the anaylsis of the differential gene expression called DESeq2.</p> <p>A differential expression analysis is used to compare gene expression levels, given by the number of reads per gene (obtained by HTSeq-count) between samples (for example, between 2 lineages of Mtb). In order to accurately ascertain which genes are differentially expressed, and the amount of expression, it is necessary to use replicated data. As with all biological experiments doing it once may simply not be enough. There is no simple way to decide how many replicates to do, it is usually a compromise of statistical power and cost. By determining how much variability there is in the sample preparation and sequencing reactions we can better assess whether genes are really expressed and more accurately determine any differences. The key to this is performing biological rather than technical replicates. This means, for instance in tuberculosis, growing up three cultures of bacteria, treating them all identically, extracting RNA from each and sequencing the three samples separately. Technical replicates, whereby the same sample is sequenced three times do not account for the variability that really exists in biological systems or the experimental error between cultures of bacteria and RNA extractions. More replicates will help to improve statistical power for genes that are already detected at high levels, while deeper sequencing will improve power to detect differential expression for genes which are expressed at low levels. In this exercise we will consider the 3 L1 and the 3 L4 samples as biological replicates.</p> <p>To start, we firstly need a table with the counts. In the Mapping_Mtb directory (where we should be) we can find a folder called HTSeqCounts with 6 files called:</p> <pre><code>cd ~/data/transcriptomics/Mapping_Mtb/HTSeqCounts\n\nls\n</code></pre> <p>Terminal output</p> <pre><code>Mtb_1_L4_htseq_count.txt  Mtb_3_L1_htseq_count.txt  Mtb_5_L1_htseq_count.txt\nMtb_2_L1_htseq_count.txt  Mtb_4_L4_htseq_count.txt  Mtb_6_L4_htseq_count.txt\n</code></pre> <p>Now, to start, we need to open R in the terminal, just typing:</p> <pre><code>R\n</code></pre> <p>And then load the packages we are going to need for the analysis:</p> <pre><code>library(DESeq2)\nlibrary(gplots)\n</code></pre> <p>These 6 files are the results of the HTSeq-count of the two samples previously analysed plus 4 more Mtb samples we will use to perform the analysis.</p> <p>To prepare the data we are going to use the following scripts.</p> <p>First we are going to set the directory where we have the files:</p> <pre><code>directory &lt;- \"~/data/transcriptomics/Mapping_Mtb/HTSeqCounts/\" \n</code></pre> <p>And now we can select the files and save them in the variable \u2018sampleFiles\u2019 by selecting all the files that contain \"Mtb\" that are present in our directory:</p> <pre><code>sampleFiles &lt;- grep(\"Mtb\", list.files(directory), value = TRUE)\n</code></pre> <p>As we are comparing lineage 1 to lineage 4 samples, we are going to set up lineage as \"condition\".</p> <pre><code>sampleCondition &lt;- c(\"l4\",\"l1\",\"l1\",\"l4\",\"l1\",\"l4\")\n</code></pre> <p>Now we construct the table with the sample information and convert it in a DESeq object:</p> <pre><code>sampleTable &lt;- data.frame(\n    sampleName = sampleFiles,\n    fileName = sampleFiles,\n    condition = sampleCondition\n)\n\ndds &lt;- DESeqDataSetFromHTSeqCount(\n    sampleTable = sampleTable,\n    directory = directory,\n    design = ~ condition\n)\n</code></pre> <p>Before doing the analysis, we are going to filter the dataset keeping only the genes with at least 10 counts, so that we make sure that every gene considered for the analysis was transcribed.</p> <p>To do it, type the following:</p> <pre><code>keep &lt;- rowSums(counts(dds)) &gt;= 10\ndds &lt;- dds[keep,]\n</code></pre> <p>And we are going to set up the condition for the analysis in two levels which are 'lineage 1' and 'lineage 4':</p> <pre><code>dds$condition &lt;- factor(dds$condition, levels = c(\"l1\",\"l4\"))\n</code></pre> <p>We can then run the differential expression anaylisis by calling the function DESeq(), which will normalise the data and compare between the two groups established (l1 and l4). We can store the results in a variable called 'res':</p> <pre><code>dds &lt;- DESeq(dds)\nres &lt;- results(dds)\n</code></pre> <p>Let's take a look at the results. Type:</p> <pre><code>res\n</code></pre> <p>And you should get a table like this:</p> <p>Terminal output</p> <pre><code>log2 fold change (MLE): condition l4 vs l1\nWald test p-value: condition l4 vs l1\nDataFrame with 441 rows and 6 columns\n        baseMean log2FoldChange     lfcSE       stat       pvalue         padj\n\nRv2159c 1251.0885      6.5961892 0.5609037  11.759933 6.278432e-32 9.208368e-30\nRv2160A  766.1015      6.6738839 0.5544491  12.036965 2.271613e-33 4.997548e-31\nRv2161c 5336.3523      5.8122020 0.4210642  13.803600 2.424369e-43 1.066722e-40\nRv2162c 4784.6386      3.8915692 0.3784335  10.283364 8.375204e-25 9.212724e-23\nRv2163c 3616.8513     -0.5480522 0.3940154  -1.390941 1.642434e-01 9.243213e-01\n...           ...            ...       ...        ...          ...          ...\nRv2586c  3828.546     0.16674016 0.3612166  0.4616071    0.6443631    0.9964280\nRv2587c  5655.191     0.21607557 0.3636699  0.5941530    0.5524098    0.9964280\nRv2588c  1305.013    -0.02030647 0.4871420 -0.0416849    0.9667499    0.9983353\nRv2589   1665.930     0.25845265 0.3629410  0.7121064    0.4763989    0.9964280\nRv2590   5873.482     0.42707857 0.6169747  0.6922141    0.4888029    0.9964280\n</code></pre> <p>The first column represents the name of each gene analysed, which are represented in rows.</p> <p>Question</p> Question 7Answer 7 <p>How many genes did we analyse?</p> <p>441</p> <p>Let's take a look at the summary of the results we just obtained:</p> <pre><code>summary(res)\n</code></pre> <p>Terminal output</p> <pre><code>out of 441 with nonzero total read count\nadjusted p-value &lt; 0.1\nLFC &gt; 0 (up)     : 10, 2.3%\nLFC &lt; 0 (down)   : 7, 1.6%\noutliers [1]     : 1, 0.23%\nlow counts [2]   : 0, 0%\n(mean count &lt; 2)\n[1] see 'cooksCutoff' argument of ?results\n[2] see 'independentFiltering' argument of ?results\n</code></pre> <p>When asking whether a gene is differentially expressed we use statistical tests to assign a p-value. If a gene has a p-value of 0.05 we know that there is only a 5% chance that it is not really differentially expressed. However, if we are asking this question for every gene in the genome, then we would expect to see due to multiple comparison, p-values less than 0.05 for many genes even though they are not really differentially expressed. Due to this statistical problem we must correct the p-values so that we are not tricked into accepting a large number of erroneous results. Adjusted p-values are p-values which have been corrected for what is known as multiple hypothesis testing. </p> <p>The summary shows us the number of genes with an adjusted p-value &lt; 0.1 that are under or over expressed in one of the groups (log2FoldChange above or below 0, here represented as LFC). The adjusted p-value in the DESeq analysis is equivalent to the FDR or 'False Discovery Rate'. This value represents the proportion of discoveries that we can expect to be false.</p> <p>Question</p> Question 8Answer 8 <p>Which is the maximum percentage of \"false discoveries\" that we can expect given a cut off adjusted p-value of 0.1?</p> <p>10%</p> <p>Question</p> Question 9Answer 9 <p>How many genes are up and down-regulated with an adjusted p-value &lt; 0.1?</p> <p>10 genes are up-regulated. 7 genes are down-regulated.</p> <p>Some of the p-values in our results might be NA values, which can be due to extreme outliers. To continue with the analysis we are going to remove these missing values.</p> <pre><code>res &lt;- res[!is.na(res$padj),]\n</code></pre> <p>Let's now order the results by p value, so we see the top genes with the highest statistial significance. Take a look at the results again.</p> <pre><code>resOrdered &lt;- res[order(res$pvalue),]\nresOrdered\n</code></pre> <p>To visualise the diffences in expression we are going to plot a heatmap using the 17 genes that are above the cut off. To do it, we will first get the normalised counts in a variable called \"counts_heatmap\". We will copy the names of the first 17 genes from our \"resOrdered\" table in a vector, and then extract the normalised counts from \"counts_heatmap\" for the 17 genes we want to plot.</p> <pre><code>counts_heatmap &lt;- counts(dds, normalized = TRUE)\nidx &lt;- rownames(resOrdered)[1:17]\ncounts_heatmap &lt;- counts_heatmap[rownames(counts_heatmap)%in%idx,]\n</code></pre> <p>If we type:</p> <pre><code>counts_heatmap\n</code></pre> <p>We can see the table with the normalised counts for each sample and each of the genes of our interest.</p> <p>Terminal output</p> <pre><code>    Mtb_1_L4_htseq_count.txt Mtb_2_L1_htseq_count.txt\nRv2159c               1061.02472                16.090459\nRv2160A                933.18297                 8.045229\nRv2161c              10927.68994               141.157208\nRv2162c               8503.01996               550.732527\nRv2188c               1643.41488               495.147306\nRv2271                 804.10604              1115.361360\nRv2274A                 57.43615                68.018758\nRv2275                2069.55403              1757.516950\nRv2292c                 54.96577               263.298419\nRv2338c               1439.60920              3638.637880\nRv2346c               1742.22976              1260.175491\nRv2381c               5656.53398              1778.727101\nRv2382c               2140.57722               623.139593\nRv2493                2611.18306               308.644258\nRv2494                1635.38617               244.282423\nRv2506                 345.23447               993.220149\nRv2573                  31.49724                71.675681\n        Mtb_3_L1_htseq_count.txt Mtb_4_L4_htseq_count.txt\nRv2159c                 34.44955               3070.71886\nRv2160A                 25.05422               1716.06570\nRv2161c                122.13932              10217.41567\nRv2162c                685.85926              12248.53695\nRv2188c                595.03771               3311.94670\nRv2271                3936.64424                797.08204\nRv2274A                147.19354                 55.37080\nRv2275                3592.14872               1232.32231\nRv2292c                140.92998                 62.66773\nRv2338c               3335.34297               1310.44236\nRv2346c                908.21546               2235.00602\nRv2381c                466.63484               3045.82346\nRv2382c                654.54148               1449.94245\nRv2493                 184.77487               2647.49705\nRv2494                 266.20108               1598.45639\nRv2506                 494.82084                499.19570\nRv2573                 112.74399                 57.08773\n        Mtb_5_L1_htseq_count.txt Mtb_6_L4_htseq_count.txt\nRv2159c                 30.57068               3293.67692\nRv2160A                 16.98371               1897.27712\nRv2161c                302.31010              10307.40137\nRv2162c                577.44627               6142.23638\nRv2188c                343.07102               4986.18078\nRv2271                1837.63783                871.00738\nRv2274A                220.78828                 26.91855\nRv2275                9847.15722               1075.78064\nRv2292c                125.67948                 71.62257\nRv2338c               5896.74541               1217.10303\nRv2346c                886.54986               8732.18549\nRv2381c               1895.38245               3401.35112\nRv2382c                740.48992               1372.36538\nRv2493                 377.03844               1754.03197\nRv2494                 261.54919               1206.52789\nRv2506                1864.81177                392.72242\nRv2573                 220.78828                 49.03022\n</code></pre> <p>To plot the heatmap copy the following script:</p> <pre><code>colnames(counts_heatmap) &lt;- c(\"L4_1\",\"L1_2\",\"L1_3\",\"L4_4\",\"L1_5\",\"L4_6\")\nheatmap.2(as.matrix(counts_heatmap), scale=\"row\", col=greenred(75), Rowv=NA, dendrogram = \"col\", trace=\"none\", density.info = \"none\")\n</code></pre> <p>You should get a plot like this:</p> <p></p> <p>Question</p> Question 10Answer 10 <p>Do the samples cluster by lineage in the dendrogram?</p> <p>Yes</p> <p>As you can see in the color key, red cells in the plot represent overexpressed genes whilst green ones the underexpressed genes. Rows represent the 17 genes of interest and columns the 6 samples we are analysing.</p> <p>Question</p> Question 11Answer 11 <p>How is Rv2493 in lineage 4 samples? And Rv2159c in lineage 1?</p> <p>Rv2493 is over expressed in 2 out of 3 lineage 4 samples and Rv2159c is under expressed in all lineage 1 samples</p> <p>Take a look at the first 5 genes in the plot. As you might assume by the numbers they are located in the genome one after the other. Which potential explanations would you give to their down-regulation in one of the lineages?</p>"},{"location":"other-omics/transcriptomics/#further-exploration","title":"Further exploration","text":""},{"location":"other-omics/transcriptomics/#what-do-i-do-with-a-gene-list","title":"What do I do with a gene list?","text":"<p>As we have just seen, differential expression analysis results is a list of genes which show differences between two conditions. It can be daunting trying to determine what the results mean. On one hand you may find that due to there being no real differences or there being too much noise in your experiment, you have no significant differences. On the other hand, you may find thousands of genes are differentially expressed. What can you say about that?</p> <p>Other than looking for genes you expect to be different or unchanged, one of the first things to do is look at Gene Ontology (GO) term enrichment. There are many different algorithms for this, but you should annotate your genes with functional terms from GO using for instance Bast2GO (Conesa et al., 2005) and then use perhaps TopGO (Alexa et al., 2006) to determine whehter any particular sorts of genes occur more than expected in your differentially expressed genes.</p>"},{"location":"other-omics/transcriptomics/#references","title":"References","text":"<p>Mortazavi A, Williams BA, McCue K, Schaeffer L, Wold B. (2008). Mapping and quantifying mammalian transcriptomes by RNA-Seq. Nat Methods 5(7):621-8. doi: 10.1038/nmeth.1226</p> <p>Wang Z, Gerstein M, Snyder M. (2009). RNA-Seq: a revolutionary tool for transcriptomics. Nat Rev Genet 10(1):57-63. doi: 10.1038/nrg2484</p> <p>Cole ST, Brosch R, Parkhill J, Garnier T, Churcher C, Harris D et al. (1998). Deciphering the biology of Mycobacterium tuberculosis from the complete genome sequence. Nature 393(6685):537-44.</p> <p>Parwati I, van Crevel R, van Soolingen D. (2010) Possible underlying mechanisms for successful emergence of the Mycobacterium tuberculosis Beijing genotype strains. Lancet Infect Dis 10(2):103-111. doi: 10.1016/S1473-3099(09)70330-5</p> <p>Conesa A, Gotz S, Garcia-Gomez JM, Terol J, Talon M, Robles M. (2005) Blast2GO: a universal tool for annotation, visualization and analysis in functional genomics research. Bioinformatics 21(18):3674-6.</p> <p>Alexa A, Rahnenfuhrer J, Lengauer T. (2006). Improved scoring of functional groups from gene expression data by decorrelating GO graph structure. Bioinformatics 22(13):1600-7.</p>"},{"location":"presentations/presentations/","title":"Presentations","text":"<p>This page lists all the presentations given before each session. Click on the links to view the PDF file.</p> <ul> <li>Introduction</li> <li>Mapping</li> <li>Variant detection</li> <li>Assembly</li> <li>Transcriptomics</li> <li>Oxford Nanopore</li> <li>Microbiome</li> <li>Methylation and EQTLs</li> <li>Phylodynamics</li> <li>GWAS</li> </ul>"}]}